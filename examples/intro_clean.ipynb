{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to TapeAgents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TapeAgents** is a framework that leverages a structured, replayable log (**Tape**) of the agent session to facilitate all stages of the LLM Agent development lifecycle. In TapeAgents, the agent reasons by processing the tape and the LLM output to produce new thoughts, actions, control flow steps and append them to the tape. The environment then reacts to the agentâ€™s actions by likewise appending observation steps to the tape.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- how to create TapeAgents using the low-level API\n",
    "- run and resume TapeAgents\n",
    "- have one TapeAgent reuse another TapeAgent's tape as training data\n",
    "\n",
    "In upcoming versions of this tutorial, you will also learn: \n",
    "- how to make a team TapeAgent with subagents\n",
    "- how to build TapeAgents using available high-level APIs\n",
    "- how to build a TapeAgent that streams partial steps\n",
    "\n",
    "Other tutorials and examples will cover:\n",
    "- code execution and browser use\n",
    "- finetuning\n",
    "- the TapeAgents apps (Studio and Browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We're assuming that you already installed the project through the `make setup` or the jupyter notebook is running in the context of the project. If not, please refer to the [README](README.md) for more detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set the OPENAI_API_KEY environment variable to your API key.\n",
    "\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"<your-api-key>\"\n",
    "    # os.environ[\"OPENAI_ORGANIZATION\"] = \"\" # optional\n",
    "today = \"2024-09-17\"  # fixed date for reproducible tests\n",
    "\n",
    "\n",
    "# If you prefer to skip the OpenAI setup and not make any LLM calls, you can use ones from the cache.\n",
    "# it will work instead of the real LLM fine as long as the prompts are not changed.\n",
    "# Uncomment the following lines to use the cache:\n",
    "#\n",
    "# from tapeagents import llms\n",
    "# import os\n",
    "# llm_cache_path = \"tests/res/intro_notebook/tapedata.sqlite\"\n",
    "# if not os.path.exists(llm_cache_path):\n",
    "#     llm_cache_path = f\"../{llm_cache_path}\"\n",
    "# assert os.path.exists(llm_cache_path)\n",
    "# llms._REPLAY_SQLITE = llm_cache_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Your first TapeAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build the simplest possible \"hello world\" agent. We will then go through all the new concepts that you need to know to understand the code. This section is quite long, but with the solid foundation you acquire here other TapeAgent tutorials will be easy to process.\n",
    "\n",
    "Without further ado, here's the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.agent import Agent, Node\n",
    "from tapeagents.core import Prompt, SetNextNode\n",
    "from tapeagents.dialog_tape import AssistantStep, DialogTape, UserStep\n",
    "from tapeagents.llms import LiteLLM, LLMStream\n",
    "from tapeagents.prompting import tape_to_messages\n",
    "\n",
    "llm = LiteLLM(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class MainNode(Node):\n",
    "    name: str = \"main\"\n",
    "\n",
    "    def make_prompt(self, agent: Agent, tape: DialogTape) -> Prompt:\n",
    "        # Render the whole tape into the prompt, each step is converted to message\n",
    "        return Prompt(messages=tape_to_messages(tape))\n",
    "\n",
    "    def generate_steps(self, agent: Agent, tape: DialogTape, llm_stream: LLMStream):\n",
    "        yield AssistantStep(content=llm_stream.get_text())  # Generate new step from the LLM output stream.\n",
    "        yield SetNextNode(next_node=\"main\")  # Which node to execute next, more on that later\n",
    "\n",
    "\n",
    "agent = Agent[DialogTape].create(llm, nodes=[MainNode()])\n",
    "start_tape = DialogTape(steps=[UserStep(content=\"Tell me about Vulcan in 3 sentences\")])\n",
    "final_tape = agent.run(start_tape).get_final_tape()  # agent will start executing the first node\n",
    "print(f\"Final tape: {final_tape.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn about tapes, steps, prompts, llm streams, nodes and agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tape\n",
    "\n",
    "The fundamental concept of the TapeAgents is the `Tape`, a comprehensive semantic level log of the agent's session. A `Tape` contains a context and a sequence of `Step` objects. As you can see, a TapeAgent runs by adding steps (such as `UserStep` or `AssistantStep`) to the _tape_. This example uses the `DialogTape` tape, which is a basic tape for user-assistant conversations. Let's see what are the possible steps in a `DialogTape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Python generics to instantiate many different Tape types by\n",
    "# specifying different Context and Step types. In the output of this cell,\n",
    "# look at Union[UserStep, AssistantStep, ...]\n",
    "# for the list of possible step types in the DialogTape.\n",
    "DialogTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these steps should be familiar to you. `UserStep`, `AssistantStep`, `SystemStep` and `ToolResult` correspond to `role=user`, `role=assistant`, `role=system` and `role=tool` LLM API messages respectively. `ToolCalls` and `AssistantThought` correspond to assistant messages where the LLM requests a tool call or produces an intermediate thought that is not meant to be shown to the user. `SetNextNode` and `Pass` are TapeAgent's internal step to control which node it should run at the next iteration (more on this below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt format; LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the industry-standard \"chat.completions\" prompt format in TapeAgents: a list of user/assistant/system/tool messages plus tool schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost all classes in TapeAgents are Pydantic base models.\n",
    "# This allows easy validation, serialization and instrospection. For example,\n",
    "# here we are able to list all the fields in the Prompt model.\n",
    "Prompt.model_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLMs in TapeAgent take `Prompt` and return an `LLMStream` object. The `LLMStream` object can be used both to fast-forward to the complete response text and to stream partial outputs step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_stream = LiteLLM(model_name=\"gpt-4o-mini-2024-07-18\", stream=True)\n",
    "\n",
    "# Streaming\n",
    "prompt = Prompt(messages=[{\"role\": \"user\", \"content\": \"Write hello world in Java\"}])\n",
    "for event in llm_stream.generate(prompt):\n",
    "    print(event.chunk, end=\"\")\n",
    "\n",
    "# No streaming\n",
    "# (note: you can not use Prompt object for more than 1 LLM call in TapeAgents)\n",
    "prompt = Prompt(messages=[{\"role\": \"user\", \"content\": \"Write hello world in C\"}])\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(llm_stream.generate(prompt).get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we use the easiest way to create a prompt from the tapes: `tape_to_messages`. Under the hood, this method uses `step.llm_dict()` method of all non-control steps in the tape to create the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((user := UserStep(content=\"hi AI!\")).llm_dict())\n",
    "print((assistant := AssistantStep(content=\"hello human\")).llm_dict())\n",
    "print(tape_to_messages(DialogTape(steps=[user, assistant])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key priority in TapeAgents is making use of the data that running the agent generates. To make this possible, some TapeAgent LLMs know how to make their finetuning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import LLMOutput\n",
    "from tapeagents.llms import TrainableLLM\n",
    "\n",
    "trainable_llm = TrainableLLM(\n",
    "    base_url=\"\",  # we only use the tokenizer from the model here, no need for a base_url for inference\n",
    "    model_name=\"microsoft/Phi-3.5-MoE-instruct\",\n",
    "    tokenizer_name=\"microsoft/Phi-3.5-MoE-instruct\",\n",
    ")\n",
    "\n",
    "simple_tape = DialogTape(\n",
    "    steps=[\n",
    "        UserStep(content=\"Say bla 3 times and foo 2 times\"),\n",
    "        AssistantStep(content=\"Sure! Let me say bla bla bla foo foo\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = Prompt(messages=tape_to_messages(simple_tape[:1]))  # type: ignore\n",
    "output = agent.make_llm_output(simple_tape, index=1)\n",
    "text = trainable_llm.make_training_text(prompt=prompt, output=output)\n",
    "print(\"--- ALL TEXT ---\")\n",
    "print(text.text)\n",
    "print(\"--- PREDICTED CHARACTERS ---\")\n",
    "print(text.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node represents an uninterruptible atom of TapeAgent's computation. When TapeAgents runs a node, it uses its two main functions: `make_prompt` to create an LLM Prompt from the tape and `generate_steps` to create new steps from the LLM output. To build a node, you can subclass `Node` and override these functions. Note that `generate_steps` must be a generator, a design choice we made to make TapeAgents a streaming-friendly framework. \n",
    "\n",
    "Let's see what the node from the above example can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.llms import LLMEvent\n",
    "\n",
    "\n",
    "class MainNode(Node):\n",
    "    name: str = \"main\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        return Prompt(messages=tape_to_messages(tape))\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        yield AssistantStep(content=llm_stream.get_text())\n",
    "        yield SetNextNode(next_node=\"main\")  # Continue to the same node\n",
    "\n",
    "\n",
    "node = MainNode()\n",
    "\n",
    "# Let's run \"make_prompt\" in isolation.\n",
    "prompt = node.make_prompt(agent=None, tape=DialogTape(steps=[UserStep(content=\"Hi, AI!\")]))\n",
    "print(f\"Raw node prompt:\\n{prompt}\\n\")\n",
    "\n",
    "\n",
    "# Now, let's run \"generate_steps\" in isolation.\n",
    "# We need to construct a fake LLMStream to do that.\n",
    "def _generator():\n",
    "    yield LLMEvent(output=LLMOutput(content=\"Hello, human!\"))\n",
    "\n",
    "\n",
    "stream = LLMStream(_generator(), Prompt())\n",
    "step_stream = node.generate_steps(agent=None, tape=DialogTape(), llm_stream=stream)\n",
    "print(f\"Steps produced by node's generator:\\n{list(step_stream)}\\n\")\n",
    "\n",
    "# When the agent runs a node, it is equivalent to the following three steps:\n",
    "# Step 1: make a prompt\n",
    "start_tape = DialogTape(steps=[UserStep(content=\"Hi, AI!\")])\n",
    "prompt = node.make_prompt(agent, start_tape)\n",
    "# Step 2: construct the LLMStream from the prompt (happens inside the agent)\n",
    "stream = llm.generate(prompt)\n",
    "# Step 3: generate steps that the agent will then add to the tape\n",
    "print(\"Produced Steps:\")\n",
    "for step in node.generate_steps(agent, start_tape, stream):\n",
    "    print(step.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and its nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TapeAgent agent iteratively runs the nodes and appends the steps generated by each node to the tape. To select which next node to run, internally a TapeAgent computes the **tape view** object. The Tape remains the only **state** that the agent uses, the view only represents its content in a way that is convenient for the agent to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import StepMetadata\n",
    "from tapeagents.view import TapeViewStack\n",
    "\n",
    "# The \"top\" view in the tape view stack is the view of the current agent.\n",
    "# Initially `top.last_node` is empty and the agent will run the first node from its list\".\n",
    "tape1 = DialogTape(steps=[UserStep(content=\"Hi, AI!\")])\n",
    "last_node = TapeViewStack.compute(tape1).top.last_node\n",
    "print(f\"1: {last_node}\")\n",
    "assert last_node == \"\"\n",
    "\n",
    "\n",
    "# When the agent computes the view, it updates `top.last_node` with the node from the latest agent step\n",
    "# The agent will search the next node after the last_node in its nodes list.\n",
    "tape2 = DialogTape(\n",
    "    steps=[\n",
    "        UserStep(content=\"Hi, AI!\"),\n",
    "        AssistantStep(metadata=StepMetadata(prompt_id=\"123\", node=\"main\"), content=\"AI here, how I can help?\"),\n",
    "    ]\n",
    ")\n",
    "last_node = TapeViewStack.compute(tape2).top.last_node\n",
    "print(f\"2: {last_node}\")\n",
    "assert last_node == \"main\"\n",
    "\n",
    "# The SetNextNode step on the tape changes `top.next_node` to the value of the `next_node` field in the SetNextNode step.\n",
    "# The agent will use this value\n",
    "tape3 = DialogTape(\n",
    "    steps=[\n",
    "        UserStep(content=\"Hi, AI!\"),\n",
    "        AssistantStep(metadata=StepMetadata(prompt_id=\"123\"), content=\"AI here, how I can help?\"),\n",
    "        SetNextNode(next_node=\"act\"),\n",
    "    ]\n",
    ")\n",
    "next_node = TapeViewStack.compute(tape3).top.next_node\n",
    "print(f\"3: {next_node}\")\n",
    "assert next_node == \"act\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the agent stops after the last node has produced an `Action` step. The action steps are the steps by which the agent requests information from the environment. For example, `AssistantStep` is an `Action` as it indicates the agent awaits the user response, `ToolCalls` is an action requesting tool call results. Let's look at all possible steps in `DialogTape` tape and see which of them are actions, observations and thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import Action, Observation, Pass, Thought\n",
    "from tapeagents.dialog_tape import AssistantThought, ToolCalls, ToolResult\n",
    "\n",
    "assert all([issubclass(step_class, Action) for step_class in [AssistantStep, ToolCalls]])\n",
    "assert all([issubclass(step_class, Thought) for step_class in [AssistantThought, SetNextNode, Pass]])\n",
    "assert all([issubclass(step_class, Observation) for step_class in [UserStep, ToolResult]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to look at a simplified summary of the corner-stone `agent.run` algorithm.\n",
    "\n",
    "1. Compute the new tape view\n",
    "2. Choose the active agent (more on multi-agent TapeAgents later)\n",
    "3. Choose the active node\n",
    "4. Run the node and add steps on the tape\n",
    "5. If the last node yielded an action, then stop, else repeat.\n",
    "\n",
    "`agent.run` returns an `AgentStream` object which allows iterating through the agent's steps (or partial steps when streaming) and fast-forwardin to the complete new tape with `get_final_tape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converse with a TapeAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the conversation with the agent that previously responded to us with the Vulcan definition from the StarTrek.\n",
    "Remember, the session is stored in the tape **final_tape**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape_to_continue = final_tape + [UserStep(content=\"No, I mean Vulcan the company\")]\n",
    "continued_tape = agent.run(tape_to_continue).get_final_tape()\n",
    "print(continued_tape.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the agent is able to continue talking to you thanks for `SetNextNode(next_node=\"main\")` step that `generate_steps` produced. If you try to remove this step as an exercise, the agent will crash because there is only one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tape rendering\n",
    "\n",
    "LLM agents create a lot of data that can be overwhelming to process. In TapeAgents we render the tape with the associated prompts and outputs into a more readable HTML for you. To make this work, we store prompts and outputs in an SQLite database every time you call `agent.run()`.\n",
    "\n",
    "Here's how to use tape rendering in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from tapeagents.renderers import render_tape_with_prompts\n",
    "from tapeagents.renderers.pretty import PrettyRenderer\n",
    "\n",
    "HTML(render_tape_with_prompts(continued_tape, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Your TapeAgent with planning and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a TapeAgent that plans and acts. We will be using OpenAI function calling capabilities in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import SetNextNode\n",
    "from tapeagents.dialog_tape import AssistantThought, ToolCalls\n",
    "from tapeagents.environment import ToolEnvironment\n",
    "from tapeagents.orchestrator import main_loop\n",
    "from tapeagents.tools.stock import get_stock_data, get_stock_ticker\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "You will help the user to learn about financials of companies.\n",
    "Use as many relevant tools as possible to include more details and facts in your responses.\n",
    "Today is {today}.\n",
    "\"\"\"\n",
    "system_message = {\"role\": \"system\", \"content\": system_instruction}\n",
    "\n",
    "env = ToolEnvironment([get_stock_ticker, get_stock_data])\n",
    "\n",
    "\n",
    "class PlanNode(Node):\n",
    "    name: str = \"plan\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        guidance = \"Write a natural language plan on how to use tools help the user. Output a list of numbered items, like 1., 2., 3., etc.\"\n",
    "        guidance_message = {\"role\": \"user\", \"content\": guidance}\n",
    "        return Prompt(\n",
    "            messages=[system_message] + tape_to_messages(tape) + [guidance_message], tools=env.get_tool_schema_dicts()\n",
    "        )\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        if content := llm_stream.get_output().content:\n",
    "            yield AssistantThought(content=content)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "class ActNode(Node):\n",
    "    name: str = \"act\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        guidance = \"Follow the plan you created to earlier. When you are done, respond to the user.\"\n",
    "        guidance_message = {\"role\": \"user\", \"content\": guidance}\n",
    "        return Prompt(\n",
    "            messages=[system_message] + tape_to_messages(tape) + [guidance_message], tools=env.get_tool_schema_dicts()\n",
    "        )\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        o = llm_stream.get_output()\n",
    "        if o.content:\n",
    "            yield AssistantStep(content=o.content)\n",
    "            yield SetNextNode(next_node=\"plan\")\n",
    "        elif o.tool_calls:\n",
    "            yield ToolCalls.from_llm_output(o)\n",
    "            yield SetNextNode(next_node=\"act\")\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "agent1 = Agent.create(LiteLLM(model_name=\"gpt-4o\", parameters={\"temperature\": 0.1}), nodes=[PlanNode(), ActNode()])\n",
    "\n",
    "print(\"Run the agent!\")\n",
    "final_tape1 = None\n",
    "for event in main_loop(agent1, DialogTape() + [UserStep(content=\"Tell me about Vulcan in 3 sentences\")], env):\n",
    "    if ae := event.agent_event:\n",
    "        if ae.step:\n",
    "            print(ae.step.model_dump_json(indent=2))\n",
    "        if ae.final_tape:\n",
    "            final_tape1 = ae.final_tape\n",
    "    if event.observation:\n",
    "        print(event.observation.model_dump_json(indent=2))\n",
    "assert final_tape1\n",
    "print(\"Final tape:\")\n",
    "HTML(render_tape_with_prompts(final_tape1, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main new thing in this example is the environment. In TapeAgents framework the environment responds to the agent `Action` steps with `Observation` steps. We expect you to use the environment to encapsulate tool use, retrieval, code execution: everything that is non-deterministic, non-stationary, or computationally heavy. On the contrary, we encourage you to implements the agent's deterministic decision-making in `make_prompt` and `generate_steps` methods.\n",
    "\n",
    "Here we use a pre-defined `main_loop` orchestrator to run the agent and the environment. `main_loop` is a generator of events that you can use as you wish. You are free to implement your own orchestration paradigm with a fine-grained control over what actions get to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Agent configuration, resumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a similar agent with an open-weights LLAMA3 70B models. Conveniently, [Together AI](together.ai) offers API endpoints. You can create an account and get API key with some free quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access to Hugging Face Gated Models\n",
    "Make sure you have access to each model we are going to use (read more [here](https://huggingface.co/docs/hub/en/models-gated#access-gated-models-as-a-user)):\n",
    "- https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\n",
    "\n",
    "##### Troubleshoot\n",
    "- If you receive a 401 error, it means the authentication failed.\n",
    "- If you receive a 403 error, it means you are authenticated but not authorized to access the specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TAPEAGENTS_LLM_TOKEN\" not in os.environ:\n",
    "    os.environ[\"TAPEAGENTS_LLM_TOKEN\"] = \"<your-together-ai-api-key>\"\n",
    "\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    # We need this to acces the model's tokenizer\n",
    "    os.environ[\"HF_TOKEN\"] = \"<your-hugging-face-api-key>\"\n",
    "\n",
    "if \"SERPER_API_KEY\" not in os.environ:  # web search, api key for https://serper.dev\n",
    "    os.environ[\"SERPER_API_KEY\"] = \"<your-serper-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Setup your Agents\n",
    "\n",
    "We've found that LLAMA3 function-calling is not yet battle-ready. We will use the structured output approach to make it call tools instead. We are also making this agent trainable by adding `make_llm_output` methods to each node. `Node.make_llm_output` defines how a node can reconstruct the LLM completion message that would be required to make the steps from the given tape at the given index. You can think of `Node.make_llm_output` as the inverse of `Node.generate_steps`.\n",
    "\n",
    "When you run the code below, you might see a different behavior every time. Often the LLAMA-based agent gets stuck in a loop. We will look into how TapeAgents supports you in addressing this issue by\n",
    "- tuning the prompt and resuming the agent exactly where it got stuck \n",
    "- producing training text from a different agent's tape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tapeagents.dialog_tape import FunctionCall, ToolCall\n",
    "from tapeagents.llms import TrainableLLM\n",
    "from tapeagents.prompting import step_to_message\n",
    "\n",
    "env = ToolEnvironment([get_stock_ticker, get_stock_data])\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "You will help the user to learn about financials of companies.\n",
    "Use as many relevant tools as possible to include more details and facts in your responses.\n",
    "Today is {today}.\n",
    "\n",
    "You have access to the following tools: {env.get_tool_schema_dicts()}\"\"\"\n",
    "\n",
    "planning_guidance = \"Write a natural language plan on how to use tools help the user. Output a list of numbered items, like 1., 2., 3., etc.\"\n",
    "\n",
    "call_or_respond_guidance = \"\"\"\n",
    "Follow the plan you created earlier. When you are done, respond to the user.\n",
    "If you want to call a or several tools, output JSON like this\n",
    "{\"kind\": \"tool_call\", \"tool_name\": \"...\", \"parameters\": \"... unquoted parameters json ...\"}\n",
    "If you have called all the tools in the plan, respond to the user with the JSON of the form\n",
    "{\"kind\": \"response\", \"content\": \"... you response ... \"}.\n",
    "Output ONE JSON OBJECT ONLY PER LINE ONLY AND NOTHING ELSE.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PlanNode(Node):\n",
    "    name: str = \"plan\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": system_instruction}\n",
    "        guidance_message = {\"role\": \"user\", \"content\": agent.templates[\"planning\"]}\n",
    "        return Prompt(messages=[system_message] + tape_to_messages(tape) + [guidance_message])\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        if content := getattr(llm_stream.get_output(), \"content\", None):\n",
    "            yield AssistantThought(content=content)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    def make_llm_output(self, agent, tape: DialogTape, index: int) -> LLMOutput:\n",
    "        if not isinstance(current := tape[index], AssistantThought):\n",
    "            raise ValueError()\n",
    "        return LLMOutput(role=\"assistant\", content=current.content)\n",
    "\n",
    "\n",
    "class ActNode(Node):\n",
    "    name: str = \"act\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": system_instruction}\n",
    "        guidance_message = {\"role\": \"user\", \"content\": agent.templates[\"call_or_respond\"]}\n",
    "        messages = [system_message]\n",
    "        for step in tape:\n",
    "            if isinstance(step, (ToolCalls)):\n",
    "                messages.append({\"role\": \"assistant\", \"content\": _llm_message_content(step)})\n",
    "            elif not isinstance(step, SetNextNode):\n",
    "                messages.append(step_to_message(step))\n",
    "        messages += [guidance_message]\n",
    "        return Prompt(messages=messages)\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        o = llm_stream.get_output()\n",
    "        try:\n",
    "            assert o.content\n",
    "            tool_calls = []\n",
    "            response = None\n",
    "            for line in o.content.split(\"\\n\"):\n",
    "                data = json.loads(line)\n",
    "                if data.get(\"kind\") == \"response\":\n",
    "                    response = data[\"content\"]\n",
    "                elif data.get(\"kind\") == \"tool_call\":\n",
    "                    tool_call = ToolCall(\n",
    "                        function=FunctionCall(name=data[\"tool_name\"], arguments=json.dumps(data[\"parameters\"])),\n",
    "                        # tool call must be a unique string, it helps to make it something deterministic\n",
    "                        id=f\"tool_call_{len(tool_calls)}_node_starts_at_{len(tape)}\",\n",
    "                    )\n",
    "                    tool_calls.append(tool_call)\n",
    "                else:\n",
    "                    yield AssistantStep(content=\"Invalid LLM output: kind field must be 'response' or 'tool_call'\")\n",
    "            if response and tool_calls:\n",
    "                yield AssistantStep(content=\"Invalid LLM output: response and tool_call cannot be in the same message\")\n",
    "            if response:\n",
    "                yield AssistantStep(content=response)\n",
    "                yield SetNextNode(next_node=\"plan\")\n",
    "            if tool_calls:\n",
    "                yield ToolCalls(tool_calls=tool_calls)\n",
    "                yield SetNextNode(next_node=\"act\")\n",
    "        except Exception as e:\n",
    "            yield AssistantStep(content=\"Invalid JSON object: \" + str(e))\n",
    "\n",
    "    def make_llm_output(self, agent, tape: DialogTape, index: int) -> LLMOutput:\n",
    "        if not isinstance(step := tape[index], AssistantStep | ToolCalls):\n",
    "            raise ValueError()\n",
    "        content = _llm_message_content(step)\n",
    "        return LLMOutput(role=\"assistant\", content=content)\n",
    "\n",
    "\n",
    "def _llm_message_content(step: AssistantStep | ToolCalls):\n",
    "    \"\"\"Helper function to make both the prompt and the target completion\"\"\"\n",
    "    match step:\n",
    "        case AssistantStep():\n",
    "            return json.dumps({\"kind\": \"response\", \"content\": step.content})\n",
    "        case ToolCalls():\n",
    "            content = \"\"\n",
    "            for tool_call in step.tool_calls:\n",
    "                if content:\n",
    "                    content += \"\\n\"\n",
    "                content += json.dumps(\n",
    "                    {\n",
    "                        \"kind\": \"tool_call\",\n",
    "                        \"tool_name\": tool_call.function.name,\n",
    "                        \"parameters\": json.loads(tool_call.function.arguments),\n",
    "                    }\n",
    "                )\n",
    "            return content\n",
    "        case _:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "agent2 = Agent.create(\n",
    "    TrainableLLM(\n",
    "        base_url=\"https://api.together.xyz\",\n",
    "        model_name=\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
    "        tokenizer_name=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        parameters=dict(temperature=0.01),\n",
    "    ),\n",
    "    templates={\n",
    "        \"system\": system_instruction,\n",
    "        \"planning\": planning_guidance,\n",
    "        \"call_or_respond\": call_or_respond_guidance,\n",
    "    },\n",
    "    nodes=[PlanNode(), ActNode()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Run your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tape2 = None\n",
    "print(\"Run LLAMA agent!\")\n",
    "for event in main_loop(\n",
    "    agent2, DialogTape() + [UserStep(content=\"Tell me about Vulcan Materials in 3 sentences\")], env, max_loops=3\n",
    "):\n",
    "    if ae := event.agent_event:\n",
    "        if ae.step:\n",
    "            print(ae.step.model_dump_json(indent=2))\n",
    "        if ae.final_tape:\n",
    "            final_tape2 = ae.final_tape\n",
    "    if event.observation:\n",
    "        print(event.observation.model_dump_json(indent=2))\n",
    "assert final_tape2 is not None\n",
    "print(\"Final tape:\")\n",
    "HTML(render_tape_with_prompts(final_tape2, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the above agent works well, but quite likely you are seeing that the LLAMA-based agent is having trouble. Let's try to help it. For reproducibility, we'll use a pre-recorded failed tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"assets/failed_tape.json\") as src:\n",
    "    failed_tape = DialogTape.model_validate(json.load(src))\n",
    "agent2b = agent2.model_copy(deep=True)\n",
    "agent2b.templates[\"call_or_respond\"] += (\n",
    "    \"REMEMBER: check what tool calls you have already made. Do not do the same call again!\"\n",
    ")\n",
    "resume_from_step8 = agent2b.run(failed_tape[:8]).get_final_tape()\n",
    "HTML(render_tape_with_prompts(resume_from_step8, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this helpful hint often gets LLAMA-based agent unstuck. Note how easy it was to test it thanks to the ability of the agent to resume from step 8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tape reuse and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another way to help this agent (or one with an even smaller LLM) is to finetune the LLM. And the most important step towards finetuning is making the training data!\n",
    "\n",
    "There are two ways to make training data in TapeAgents:\n",
    "- the basic one: use the `LLMCall` structures that the agent created when it produced the tape. You can retrieve them from the SQLite storage and convert into training text.\n",
    "- the much powerful one: call `agent.reuse` to reconstructed the prompts and outputs **and** to validate that with the reconstructed LLMCalls the agent would indeed create the given tape\n",
    "\n",
    "The big advantage of the second approach is that it allows you to use the tape from a teacher agent (think slower and more expensive) to train a student agent (think faster and cheaper). Or to train an agent on its own revised tapes.\n",
    "\n",
    "Of course, restrictions apply: the tape by agent A may not be reusable by agent B directly. You might have to add/remove some steps. But at least `agent.reuse` verifies if your tape modifications led to creation of a tape that the agent B can indeed produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training data from the past LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.observe import retrieve_tape_llm_calls\n",
    "\n",
    "llm_calls = retrieve_tape_llm_calls(final_tape2)\n",
    "print(f\"Retrieved {len(llm_calls)} LLM calls from the tape.\")\n",
    "# under the hood agent2 will route this request to its llm\n",
    "example_text = agent2.make_training_text(list(llm_calls.values())[0])\n",
    "print(\"From the first retrieved LLM call, the LLM will be trained to predict this text:\")\n",
    "print(\"---\")\n",
    "print(example_text.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training data by reusing a tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `agent2` reuses the tape by `agent1`, even though they have very different prompt and output formats! \n",
    "\n",
    "You can inspect the reused tape below and see that the steps are the same as before, but the prompts and outputs are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reused_tape, _ = agent2.reuse(final_tape1)\n",
    "HTML(render_tape_with_prompts(reused_tape, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer a quick way to harness the tape reuse to make training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = agent2.make_training_data(final_tape1)\n",
    "print(training_data[0].output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be simpler than that?!\n",
    "We're only scratching the surface of what TapeAgents can do. We invite you to explore the other tutorials and examples to learn more about the TapeAgents framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-agent teams, deeper dive into view stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a colleague to our agent that will help it search the internet! \n",
    "\n",
    "First thing, we need to give this colleague some tools. In TapeAgents, the agents do not use the environment directly, their interaction with the environment is mediated by in orchestrator such as you application. But the agents should know what tools they can call. And if you are using `main_loop` as the orchestrator, it requires one environment that contains tools of all the agents. \n",
    "\n",
    "Let's go ahead and define environments:\n",
    "- the environments of the root agent\n",
    "- the one of its internet search specialist colleague\n",
    "- the environment that contains all the tools.\n",
    "\n",
    "Note that we won't use the first two environments to produce observations, we'll use them only to generate tool schemas for agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.tools.simple_browser import SimpleTextBrowser\n",
    "from tapeagents.tools.web_search import web_search_tool\n",
    "\n",
    "browser = SimpleTextBrowser()\n",
    "search_agent_env = ToolEnvironment([web_search_tool, browser.get_page, browser.get_next_page])\n",
    "\n",
    "\n",
    "# We will use the tool choice mechanism to let the main agent call its search specialist agent.\n",
    "# To this end, we create a mock tool that represents calling the search agent.\n",
    "def call_search_agent(query: str):\n",
    "    \"\"\"Use this tool to ask a fellow AI agent to search for information on the web.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "main_agent_env = ToolEnvironment([get_stock_ticker, get_stock_data, call_search_agent])\n",
    "whole_env = ToolEnvironment(\n",
    "    [get_stock_ticker, get_stock_data, web_search_tool, browser.get_page, browser.get_next_page]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the subagent, let's review the way we do multi-agent communication in TapeAgents:\n",
    "- when the root agent wants to call its subagent \"xyz\", it puts `Call(agent_name=\"xyz\")` step on the tape\n",
    "- at the next iteration, the root agent will compute `TapeViewStack` and delegate to the currently active agent. Right after `Call(agent_name=\"B\")` on top of the stack there will be a new view associated with Agent \"xyz\". The root agent will delegate to \"xyz\" to make the prompt and to generate the steps from \"xyz\"'s current node.\n",
    "- when \"xyz\" is done it will put `Respond()` step on the tape. At the next iteration the view stack won't have \"xyz\"'s view on the top any more. \n",
    "\n",
    "A reader familiar with the concept of a call stack will find the `TapeViewStack` concept very similar...\n",
    "\n",
    "Note that for the purpose of computing the view stack the root agent's name is not known. It's view will be signed as \"root\".\n",
    "\n",
    "Let's explore the way `Call` and `Respond` works with a minimal example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import Call, Respond\n",
    "\n",
    "tape = DialogTape(\n",
    "    steps=[\n",
    "        UserStep(content=\"Compute 2 + 2\"),\n",
    "        Call(agent_name=\"xyz\", content=\"what is 2 + 2, dear xyz\"),\n",
    "        AssistantThought(content=\"deep thinking by agent xyz\"),\n",
    "        Respond(content=\"I heard it is 4\"),\n",
    "    ]\n",
    ")\n",
    "# We will print a brief summary of view stack after each step\n",
    "for i in range(0, len(tape)):\n",
    "    print(f\"View stack after step {i}\")\n",
    "    view_stack = TapeViewStack.compute(tape[: i + 1])\n",
    "    for view in view_stack.stack:\n",
    "        step_summary = \", \".join([step.__class__.__name__ for step in view.steps])\n",
    "        print(f\"-- {view.agent_full_name}: {step_summary}\")\n",
    "# Note how \"root/xyz\" view appears after step 1 and disappears after step 3.\n",
    "# Also note how \"root\" does not see private thoughts of \"xyz\" (step 2), and \"xyz\"\n",
    "# does not see the initial observation of root (step 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to proceed to the implementation of the internet search agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import Respond\n",
    "from tapeagents.nodes import FixedStepsNode\n",
    "from tapeagents.prompting import view_to_messages\n",
    "\n",
    "search_system_instruction = \"Use at most 5 tool calls to search the request info on on the web.\"\n",
    "search_system_message = {\"role\": \"system\", \"content\": search_system_instruction}\n",
    "\n",
    "\n",
    "class SearchAgentMainNode(Node):\n",
    "    name: str = \"main\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        view = agent.compute_view(tape)\n",
    "        return Prompt(messages=view_to_messages(view.top, agent), tools=search_agent_env.get_tool_schema_dicts())\n",
    "\n",
    "    def generate_steps(self, agent, tape, llm_stream: LLMStream):\n",
    "        o = llm_stream.get_output()\n",
    "        if o.content:\n",
    "            # if the LLM responds, yield Respond(..) as your last step\n",
    "            yield Respond(content=o.content)\n",
    "        elif o.tool_calls:\n",
    "            # when the LLM suggests tool calls, yield them as action steps\n",
    "            yield ToolCalls.from_llm_output(o)\n",
    "            yield SetNextNode(next_node=\"main\")\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "search_agent = Agent.create(\n",
    "    name=\"search_agent\",\n",
    "    llms=LiteLLM(model_name=\"gpt-4o\", parameters={\"temperature\": 0.1}),\n",
    "    nodes=[SearchAgentMainNode()],\n",
    ")\n",
    "# To test the subagent, we'll make a mock root agent that immediately calls \"search_agent\"\n",
    "call_step = Call(agent_name=\"search_agent\", content=\"What influenced Nvidia stock price in late 2022?\")\n",
    "test_root_agent = Agent.create(subagents=[search_agent], nodes=[FixedStepsNode(steps=[call_step])])\n",
    "start_tape = DialogTape()\n",
    "final_tape = None\n",
    "for event in main_loop(test_root_agent, start_tape, search_agent_env):\n",
    "    # We need to stop the loop when \"search_agent\" responds,\n",
    "    # otherwise the code will crash because `test_root_agent` only has one node.\n",
    "    if (ae := event.agent_event) and isinstance(ae.step, Respond):\n",
    "        final_tape = ae.partial_tape\n",
    "        break\n",
    "assert final_tape\n",
    "HTML(render_tape_with_prompts(final_tape, PrettyRenderer()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add the search subagent to a financial analyst agent like the ones in earlier examples. We need to give the root agent a way to use the LLM to decide whether to call the search specialist, and if yes, what query to pass to it. We will abuse the tool calling mechanism for this purpose to make the example simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "from tapeagents.core import SetNextNode\n",
    "from tapeagents.dialog_tape import AssistantThought, ToolCalls\n",
    "from tapeagents.orchestrator import MainLoopStatus, main_loop\n",
    "from tapeagents.view import Call\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "You will help the user to learn about financials of companies. \n",
    "For general user queries, include some info about stock price changes during the last year, as well as some general information on the company.\n",
    "Today is {today}.\n",
    "\"\"\"\n",
    "system_message = {\"role\": \"system\", \"content\": system_instruction}\n",
    "\n",
    "\n",
    "class PlanNode(Node):\n",
    "    name: str = \"plan\"\n",
    "\n",
    "    def make_prompt(self, agent, tape) -> Prompt:\n",
    "        view = agent.compute_view(tape)\n",
    "        guidance = \"Write a natural language plan on how to use tools help the user. Output a list of numbered items, like 1., 2., 3., etc.\"\n",
    "        guidance_message = {\"role\": \"user\", \"content\": guidance}\n",
    "        return Prompt(\n",
    "            messages=[system_message] + view_to_messages(view.top, agent) + [guidance_message],\n",
    "            tools=main_agent_env.get_tool_schema_dicts(),\n",
    "        )\n",
    "\n",
    "    def generate_steps(self, agent, dialog, llm_stream: LLMStream):\n",
    "        if content := llm_stream.get_output().content:\n",
    "            yield AssistantThought(content=content)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "class ActNode(Node):\n",
    "    name: str = \"act\"\n",
    "\n",
    "    def make_prompt(self, agent, tape: DialogTape) -> Prompt:\n",
    "        view = agent.compute_view(tape)\n",
    "        guidance = \"Follow the plan you created to earlier. When you are done, respond to the user.\"\n",
    "        guidance_message = {\"role\": \"user\", \"content\": guidance}\n",
    "        return Prompt(\n",
    "            messages=[system_message] + view_to_messages(view.top, agent) + [guidance_message],\n",
    "            tools=main_agent_env.get_tool_schema_dicts(),\n",
    "        )\n",
    "\n",
    "    def generate_steps(self, agent, dialog, llm_stream: LLMStream):\n",
    "        o = llm_stream.get_output()\n",
    "        if o.content:\n",
    "            yield SetNextNode(next_node=\"plan\")\n",
    "            yield AssistantStep(content=o.content)\n",
    "        elif o.tool_calls:\n",
    "            yield SetNextNode(next_node=\"act\")\n",
    "            # only keep the tool calls before the call to another agent\n",
    "            agent_call = None\n",
    "            for i, tc in enumerate(o.tool_calls):\n",
    "                if tc.function.name == \"call_search_agent\":\n",
    "                    agent_call = tc\n",
    "                    o.tool_calls = o.tool_calls[:i]\n",
    "                    break\n",
    "            # either produce the ToolCalls action OR call another agent\n",
    "            if o.tool_calls:\n",
    "                yield ToolCalls.from_llm_output(o)\n",
    "            else:\n",
    "                assert agent_call and agent_call.function.name == \"call_search_agent\"\n",
    "                yield Call(agent_name=\"search_agent\", content=json.loads(agent_call.function.arguments)[\"query\"])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "multi_agent_analyst = Agent.create(\n",
    "    name=\"analyst\",\n",
    "    subagents=[search_agent.clone()],\n",
    "    llms=LiteLLM(model_name=\"gpt-4o\", parameters={\"temperature\": 0.1}),\n",
    "    nodes=[PlanNode(), ActNode()],\n",
    ")\n",
    "\n",
    "print(\"Run the agent!\")\n",
    "start_tape = DialogTape(steps=[UserStep(content=\"Tell me about Vulcan in 3 sentences\")])\n",
    "for event in main_loop(multi_agent_analyst, start_tape, whole_env):\n",
    "    # This agent runs for a while, so we will show you a fresh render every time\n",
    "    # when the environment finishes reacting with new actions\n",
    "    if new_tape := event.agent_tape or event.env_tape:\n",
    "        clear_output()\n",
    "        display(HTML(render_tape_with_prompts(new_tape, PrettyRenderer())))\n",
    "    # Uncomment this if you want to pause after every loop\n",
    "    # if event.env_tape:\n",
    "    #     input(\"Press Enter the run the next iteration of the main loop\")\n",
    "    if event.status == MainLoopStatus.EXTERNAL_INPUT_NEEDED:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this rather long tape and note how \"analyst\" calls \"search_agent\", and how the latter then responds to \"analyst\". Congratulations, you now know how to build a multi-agent TapeAgent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tapeagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
