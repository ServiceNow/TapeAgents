defaults:
  - data: test

# Model class. Must be one of the following: causal-language-modeling, causal-language-modeling-rl
model_class: causal-language-modeling
# Model name or path of model to be trained.
config_name: gpt2
# Optimizer type, for now only adamw_torch and adafactor are supported
optim: adamw_torch
# use half precision training, full bf16 without mixed precision copies at all
load_as_bf16: False
#LoRA adapter tuning config
auto_device_map: False
lora:
  enabled: False
  task_type: CAUSAL_LM # supported: CAUSAL_LM, SEQ_2_SEQ_LM
  base_model_8bit: False # format used for storing base mode weights during forward
  base_model_4bit: False
  r: 16 # Lora attention dimension.
  alpha: 16 # the alpha parameter for Lora scaling.
  dropout: 0.05 # the dropout probability for Lora layers.
  bias: "none" # Bias type for Lora. Can be 'none', 'all' or 'lora_only'
  target_modules: [] # the names of the modules to apply Lora to.
# Comma-separated list of keywords to tag the run.
tags: []
#Use W&B experiment logging
use_wandb: False
# W&B id; if given, will resume this run
wandb_id: null
# W&B entity name
wandb_entity_name: null
# W&B project name
wandb_project_name: tapeagents
# W&B resume policy
wandb_resume: if_not_interactive
# Whether to use only the basename or the full path as the run name
wandb_use_basename: false
# Overwrite existing model checkpoints. If False, resume training from existing checkpoint if exists
force_restart: False
# Rewind dataloder to the last used position if training resumed from existing checkpoint
resume_dataloader: True
# Batch size for training.
train_batch_size: 4
# Batch size for evaluation.
valid_batch_size: 4
# Value of weight decay.
weight_decay: 0.1
# Learning rate for training.
learning_rate: 0.001
# How much to clip the gradient (no clipping if null)
gradient_clipping_threshold: 1.0
# Learning rate scheduler type (indexed by completed_steps).
lr_scheduler_type: cosine
# Number of warmup (completed) steps in the learning rate schedule.
num_warmup_steps: 2
# Number of gradient accumulation steps.
gradient_accumulation_passes: 2
# Use gradient checkpointing to reduce memory footprint.
gradient_checkpointing: False
# Number of training steps passed to LR scheduler,
# also the maximum number of steps if interrupt_train_steps is -1
max_train_steps: 40
# Set interrupt_train_steps to the number of steps after which to interrupt training.
# Using interrupt_train_steps is useful for stopping training before max_train_steps
# is reached without affecting the LR scheduler.
# This is useful for testing purposes.
# Note that interrupt_train_steps is ignored if equal to -1
interrupt_train_steps: -1
# Maximum number of evaluation (completed) steps. If -1 the full dataset is evaluated.
max_eval_steps: -1
# Sequence lengths used for training.
seq_length: 1024
# Training seed.
seed: 1
# Interval to save checkpoints.
save_checkpoint_steps: 16
# Whether to keep intermediate checkpoints
keep_intermediate_checkpoints: True
# Whether to allow loading external model code
trust_remote_code: False
# Whether to empty the cache every training pass. Usually not worth it except for very large models if OOM
cuda_empty_cache: False
sft_config_name: null
n_examples: 0 # 0 means do not limit number of samples
log_each_n_steps: 10
also_save_steps: []
use_safetensors: True
save_final_training_state: True
eval_callback:
  _target_: tapeagents.finetune.eval.dummy_eval_callback
  config_name: ""