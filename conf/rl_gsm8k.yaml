defaults:
  - finetune: grpo
  - _self_

n_workers_per_gpu: 32
get_logprobs_workers_per_gpu: 4
test_every_n_iterations: 1
model_path: /mnt/llmd/base_models/Llama-3.2-1B-Instruct
max_agent_forks: 6000
attempts: 8 
force_restart: false
max_iterations: 1000
llm:
  parameters:
    max_tokens: 3072
    temperature: 0.7
test_llm:
  parameters: 
    max_tokens: ${...llm.parameters.max_tokens}
    temperature: 0.
    top_p: 1.0
    top_k: 50

finetune:
  config_name: ${..model_path}
  output_dir: ${..output_dir}/finetune
  # Each finetuning iteration will be stopped after 10 steps,
  # after which we generate new tapes and start a new iteration.
  # One step is one weight update. See the finetuning configuration
  # for the info in how many sequences are used for each weight update.
  save_checkpoint_steps: 10
  seq_length: 4096

system_prompt: ""
task_template: |-
  {task}\nPlease reason step by step, and put your final answer within \boxed{{}}.
max_prompt_length: 1024

vllm_config:
  vllm_kwargs:
    --download-dir: /mnt/llmd/base_models/ 
    --gpu-memory-utilization: 0.9
    --num-scheduler-steps: 16
    --disable-log-requests: ""
    --max-num-seqs: 1024 
    --enforce-eager: ""
    --return-tokens-as-token-ids: ""
    --pipeline-parallel-size: 1
    --tensor-parallel-size: 1
  actor_vllm_kwargs:
    --num-scheduler-steps: 16
  ref_vllm_kwargs:
    --num-scheduler-steps: 1
     # VLLM get log probs OOM https://github.com/vllm-project/vllm/issues/5907
    --enable-chunked-prefill: ""
    --max-num-batched-tokens: 1024

output_dir: outputs/rl_gsm8k_deepspeed
accelerate_cfg_path: conf/accelerate/accelerate_base.yaml
use_deepspeed: false

hydra:
  run:
    dir: ${output_dir}
  job_logging:
    root:
      level: INFO
    handlers:
      node_errors:
        class: logging.FileHandler
        filename: ${output_dir}/node_errors.log    
    loggers:
      tapeagents.nodes:
        handlers: [node_errors]
        propagate: no      
