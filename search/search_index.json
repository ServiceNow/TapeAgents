{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TapeAgents Documentation","text":"<p>TapeAgents is a framework that leverages a structured, replayable log (Tape) of the agent session to facilitate all stages of the LLM Agent development lifecycle. In TapeAgents, the agent reasons by processing the tape and the LLM output to produce new thoughts, actions, control flow steps and append them to the tape. The environment then reacts to the agent\u2019s actions by likewise appending observation steps to the tape.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Quick Start - Get started with TapeAgents.</li> <li>Reference - Detailed documentation of the TapeAgents framework.</li> </ul>"},{"location":"quickstart/","title":"TapeAgents","text":"<p>TapeAgents is a framework that leverages a structured, replayable log (Tape) of the agent session to facilitate all stages of the LLM Agent development lifecycle. In TapeAgents, the agent reasons by processing the tape and the LLM output to produce new thoughts, actions, control flow steps and append them to the tape. The environment then reacts to the agent\u2019s actions by likewise appending observation steps to the tape.</p> <p></p> <p>Key features: - Build your agent as a low-level state machine, as a high-level multi-agent team configuration, or as a mono-agent guided by multiple prompts - Debug your agent with TapeAgent studio or TapeBrowser apps - Serve your agent with response streaming - Optimize your agent's configuration using successful tapes; finetune the LLM using revised tapes.</p> <p>The Tape-centric design of TapeAgents will help you at all stages of your project: - Build with ultimate flexibility of having access to tape for making prompts and generating next steps - Change your prompts or team structure and resume  the debug session as long as the new agent can continue from the older tape - Fully control the Agent's tape and the Agent's acting when you use a TapeAgent in an app - Optimize tapes and agents using the carefully crafted metadata structure that links together tapes, steps, llm calls and agent configurations</p>"},{"location":"quickstart/#get-started","title":"Get Started","text":"<p>We highly recommend starting with the introductory Jupyter notebook. The notebook will introduce you to all the core concepts of framework. </p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install the latest release:</p> <pre><code>pip install TapeAgents\n</code></pre> <p>If you want to install the version from the sources: 1. Clone the repository:</p> <pre><code>git clone https://github.com/ServiceNow/TapeAgents.git\ncd TapeAgents\n</code></pre> <ol> <li>Create conda environment <code>tapeagents</code> and install the package in editable mode inside the environment:</li> </ol> <pre><code>make setup\n</code></pre>"},{"location":"quickstart/#examples","title":"Examples","text":"<p>The simplest agent just to show the basic structure of the agent:</p> <pre><code>from tapeagents.agent import Agent, Node\nfrom tapeagents.core import Prompt\nfrom tapeagents.dialog_tape import AssistantStep, UserStep, DialogTape\nfrom tapeagents.llms import LLMStream, LiteLLM\nfrom tapeagents.prompting import tape_to_messages\n\nllm = LiteLLM(model_name=\"gpt-4o-mini\")\n\n\nclass MainNode(Node):\n    def make_prompt(self, agent: Agent, tape: DialogTape) -&gt; Prompt:\n        # Render the whole tape into the prompt, each step is converted to message\n        return Prompt(messages=tape_to_messages(tape))\n\n    def generate_steps(self, agent: Agent, tape: DialogTape, llm_stream: LLMStream):\n        # Generate single tape step from the LLM output messages stream.\n        yield AssistantStep(content=llm_stream.get_text())\n\n\nagent = Agent[DialogTape].create(llm, nodes=[MainNode()])\nstart_tape = DialogTape(steps=[UserStep(content=\"Tell me about Montreal in 3 sentences\")])\nfinal_tape = agent.run(start_tape).get_final_tape()  # agent will start executing the first node\nprint(f\"Final tape: {final_tape.model_dump_json(indent=2)}\")\n</code></pre> <p>The examples/ directory contains examples of how to use the TapeAgents framework for building, debugging, serving and improving agents. Each example is a self-contained Python script (or module) that demonstrates how to use the framework to build an agent for a specific task.</p> <ul> <li>How to build a single agent that does planning, searches the web and uses code interpreter to answer knowledge-grounded questions, solving the tasks from the GAIA benchmark.</li> <li>How to build a team of TapeAgents with AutoGen-style low-code programming paradigm</li> <li>How to finetune a TapeAgent with a small LLM to be better at math problem solving on GSM-8k dataset.</li> </ul> <p>Other notable examples that demonstrate the main aspects of the framework: - workarena - custom agent that solves WorkArena benchmark using BrowserGym environment. - tape_improver.py - the agent that revisit and improves the tapes produced by another agent.</p>"},{"location":"quickstart/#learn-more","title":"Learn more","text":"<p>See our technical report on TapeAgents.</p>"},{"location":"quickstart/#contacts","title":"Contacts","text":"<p>Feel free to reach out to the team: - Dzmitry Bahdanau, dzmitry.bahdanau@servicenow.com - Oleh Shliazhko, oleh.shliazhko@servicenow.com - Jordan Prince Tremblay, jordanprince.t@servicenow.com - Alexandre Pich\u00e9, alexandre.piche@servicenow.com</p>"},{"location":"quickstart/#acknowledgements","title":"Acknowledgements","text":"<p>We acknowledge the inspiration we took from prior frameworks, in particular LangGraph, AutoGen, AIWaves Agents and DSPy.</p>"},{"location":"reference/","title":"Index","text":"<p>Modules:</p> <ul> <li> <code>agent</code>           \u2013            <p>Base classes for agents and nodes.</p> </li> <li> <code>batch</code>           \u2013            <p>Batch processing of tapes.</p> </li> <li> <code>core</code>           \u2013            <p>Core data structures for the tape agents framework.</p> </li> <li> <code>dialog_tape</code>           \u2013            <p>Types and classes for dialog tapes and annotators.</p> </li> <li> <code>environment</code>           \u2013            <p>Base classes for environments that execute actions and produce observations</p> </li> <li> <code>finetune</code>           \u2013            <p>Finetuning LLMs with tape data.</p> </li> <li> <code>io</code>           \u2013            <p>I/O routines for Tapes.</p> </li> <li> <code>llm_function</code>           \u2013            <p>Optimizable llm functions.</p> </li> <li> <code>llms</code>           \u2013            <p>Wrapper for interacting with external and hosted large language models (LLMs).</p> </li> <li> <code>nodes</code>           \u2013            <p>Nodes are the building blocks of a TapeAgent, representing atomic units of the agent's behavior.</p> </li> <li> <code>observe</code>           \u2013            <p>Functions to observe and store LLM calls and Tapes in a persistent storage</p> </li> <li> <code>orchestrator</code>           \u2013            <p>Module contains the main loops of the agent-environment interaction and replay functions.</p> </li> <li> <code>parallel_processing</code>           \u2013            <p>Functions for parallel processing of agent workers.</p> </li> <li> <code>prompting</code>           \u2013            <p>Utilities for converting between tape steps and LLM messages.</p> </li> <li> <code>renderers</code>           \u2013            <p>Renderers that produce HTML from the tapes used by various GUI scripts.</p> </li> <li> <code>tape_browser</code>           \u2013            <p>GUI for browsing tapes.</p> </li> <li> <code>team</code>           \u2013            <p>Multiagent building blocks for orchestrating multiple agents in a team.</p> </li> <li> <code>tools</code>           \u2013            <p>Tools for the agents to use.</p> </li> <li> <code>utils</code>           \u2013            <p>Various utility functions.</p> </li> <li> <code>view</code>           \u2013            <p>Views and view stacks for subagents context isolation.</p> </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Agent</li> <li>Batch</li> <li>Config</li> <li>Core</li> <li>Demo</li> <li>Dialog Tape</li> <li>Environment</li> <li>Finetune<ul> <li>Checkpoints</li> <li>Context</li> <li>Data</li> <li>Eval</li> <li>Finetune</li> <li>Logging</li> <li>Lora</li> <li>Optim</li> <li>RL<ul> <li>Utils</li> </ul> </li> <li>Types</li> </ul> </li> <li>IO</li> <li>LLM Function</li> <li>LLMs</li> <li>Nodes</li> <li>Observe</li> <li>Orchestrator</li> <li>Parallel Processing</li> <li>Prompting</li> <li>Renderers<ul> <li>Basic</li> <li>Camera Ready Renderer</li> <li>Pretty</li> </ul> </li> <li>Studio</li> <li>Tape Browser</li> <li>Team</li> <li>Tools<ul> <li>Calculator</li> <li>Container Executor</li> <li>Document Converters</li> <li>Gym Browser</li> <li>Python Interpreter</li> <li>Simple Browser</li> <li>Stock</li> </ul> </li> <li>Utils</li> <li>View</li> </ul>"},{"location":"reference/agent/","title":"Agent","text":"<p>Base classes for agents and nodes.</p> <p>Classes:</p> <ul> <li> <code>Agent</code>           \u2013            <p>Base class for agents within the TapeAgents framework.</p> </li> <li> <code>AgentStream</code>           \u2013            <p>A wrapper around a generator that produces AgentEvents, representing the result of an agent run.</p> </li> <li> <code>Annotator</code>           \u2013            <p>Annotator is the base class for agents that produce annotations for the tape of another agent.</p> </li> <li> <code>Node</code>           \u2013            <p>A node in the agent, atomic unit of the agent's behavior.</p> </li> <li> <code>TapeReuseFailure</code>           \u2013            <p>Exception raised when tape reuse operation fails.</p> </li> </ul>"},{"location":"reference/agent/#tapeagents.agent.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TapeType]</code></p> <p>Base class for agents within the TapeAgents framework.</p> <p>An agent is a model that can run on a tape, generating new steps based on the tape's state. The agent can have subagents, which are other agents that it manages and can delegate to. The agent can also have nodes, which are atomic units of the agent's behavior that it can choose to run based on the tape.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The unique name of the agent.</p> </li> <li> <code>llms</code>               (<code>dict[str, SerializeAsAny[LLM]]</code>)           \u2013            <p>A dictionary mapping names to LLM instances used by the agent.</p> </li> <li> <code>subagents</code>               (<code>list[Any]</code>)           \u2013            <p>A list of subagents managed by this agent. Subagents must have unique names.</p> </li> <li> <code>templates</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary of templates used for generating prompts.</p> </li> <li> <code>nodes</code>               (<code>list[SerializeAsAny[Node]]</code>)           \u2013            <p>A list of nodes that define the agent's actions and decision points.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>)           \u2013            <p>The maximum number of iterations the agent will execute before stopping.</p> </li> <li> <code>manager</code>               (<code>Agent</code>)           \u2013            <p>Retrieves the manager agent overseeing this agent.</p> </li> <li> <code>llm</code>               (<code>LLM</code>)           \u2013            <p>Default language model if only one is configured.</p> </li> <li> <code>template</code>               (<code>Template</code>)           \u2013            <p>Default template if only one is configured.</p> </li> <li> <code>full_name</code>               (<code>str</code>)           \u2013            <p>Hierarchical name of the agent, including its manager hierarchy.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If configuration inconsistencies are detected:</p> <ul> <li>If a subagent is already managed by another agent</li> <li>If any subagent is not an instance of Agent class</li> <li>If there are duplicate names among subagents</li> <li>If there are duplicate names among nodes</li> </ul> </li> </ul> <p>Methods:</p> <ul> <li> <code>clone</code>             \u2013              <p>Creates a deep copy of the current agent instance.</p> </li> <li> <code>compute_view</code>             \u2013              <p>Compute the view stack from a given tape.</p> </li> <li> <code>create</code>             \u2013              <p>Creates an instance of the class with provided LLMs and templates.</p> </li> <li> <code>delegate</code>             \u2013              <p>Delegates control to the appropriate subagent based on the current tape state.</p> </li> <li> <code>find_node</code>             \u2013              <p>Find a node by its name in the list of nodes.</p> </li> <li> <code>find_subagent</code>             \u2013              <p>Find a subagent by name in the list of subagents.</p> </li> <li> <code>generate_steps</code>             \u2013              <p>Generate steps from the agent by selecting a node and processing its output.</p> </li> <li> <code>get_node_runs</code>             \u2013              <p>Parse the tape and identify the indices where each node began its execution.</p> </li> <li> <code>get_subagent_names</code>             \u2013              <p>Returns a list of names of all subagents.</p> </li> <li> <code>is_agent_step</code>             \u2013              <p>Check if a step was produced by the agent.</p> </li> <li> <code>make_llm_output</code>             \u2013              <p>Generates an LLM output based on a tape and step index.</p> </li> <li> <code>make_prompt</code>             \u2013              <p>Makes the prompt for the next iteration of the agent.</p> </li> <li> <code>make_training_data</code>             \u2013              <p>Generates training data from a tape by converting LLM calls into training texts.</p> </li> <li> <code>make_training_text</code>             \u2013              <p>Routes the request to make training text to the agent's LLM.</p> </li> <li> <code>reuse</code>             \u2013              <p>Reuse another agent's tape as one's own.</p> </li> <li> <code>run</code>             \u2013              <p>Run the agent on the tape iteratively, delegating to subagents until a stop condition is met.</p> </li> <li> <code>run_iteration</code>             \u2013              <p>Run one iteration of the agent (assuming one call to the underlyng model).</p> </li> <li> <code>select_node</code>             \u2013              <p>Select the next node to execute based on the current state of the tape.</p> </li> <li> <code>should_stop</code>             \u2013              <p>Check if the agent should stop its turn and wait for observations.</p> </li> <li> <code>update</code>             \u2013              <p>Updates the agent's configuration while preserving instance types.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_name</code>           \u2013            <p>Returns the full hierarchical name of the agent.</p> </li> <li> <code>llm</code>           \u2013            <p>Get the default LLM instance associated with the agent.</p> </li> <li> <code>manager</code>           \u2013            <p>Gets the manager of the agent.</p> </li> <li> <code>template</code>           \u2013            <p>Returns the default template of the agent.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>class Agent(BaseModel, Generic[TapeType]):\n    \"\"\"\n    Base class for agents within the TapeAgents framework.\n\n    An agent is a model that can run on a tape, generating new steps based on the tape's state.\n    The agent can have subagents, which are other agents that it manages and can delegate to.\n    The agent can also have nodes, which are atomic units of the agent's behavior that it can choose to run based on the tape.\n\n    Attributes:\n        name (str): The unique name of the agent.\n        llms (dict[str, SerializeAsAny[LLM]]): A dictionary mapping names to LLM instances used by the agent.\n        subagents (list[Any]): A list of subagents managed by this agent. Subagents must have unique names.\n        templates (dict[str, Any]): A dictionary of templates used for generating prompts.\n        nodes (list[SerializeAsAny[Node]]): A list of nodes that define the agent's actions and decision points.\n        max_iterations (int): The maximum number of iterations the agent will execute before stopping.\n        manager (Agent): Retrieves the manager agent overseeing this agent.\n        llm (LLM): Default language model if only one is configured.\n        template (Template): Default template if only one is configured.\n        full_name (str): Hierarchical name of the agent, including its manager hierarchy.\n\n\n    Raises:\n        ValueError: If configuration inconsistencies are detected:\n\n            - If a subagent is already managed by another agent\n            - If any subagent is not an instance of Agent class\n            - If there are duplicate names among subagents\n            - If there are duplicate names among nodes\n\n    \"\"\"\n\n    name: str = \"\"\n    llms: dict[str, SerializeAsAny[LLM]] = {}\n    # can't use list[Agent] because of pydantic bug\n    # https://github.com/pydantic/pydantic/issues/9969\n    subagents: list[Any] = Field(\n        default_factory=lambda: [],\n        description=\"List of subagents, which are agents that are run by this agent. Subagents must have unique names.\",\n    )\n    templates: dict[str, Any] = {}\n    nodes: list[SerializeAsAny[Node]] = Field(\n        default_factory=lambda: [],\n        description=\"List of nodes in the agent, order of the list used to determine the priority during activation. Nodes must have unique names.\",\n    )\n    max_iterations: int = 100\n\n    _manager: Any | None = None\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        if not self.name:\n            # by default use the class name without the values for type variables\n            # e.g. \"Agent\" instea of \"Agent[Tape[...]]\"\"\n            self.name = self.__class__.__name__.split(\"[\")[0]\n        names = set()\n        for i, agent in enumerate(self.subagents):\n            if isinstance(agent, Agent):\n                if agent._manager is not None:\n                    raise ValueError(\"Agent is already a subagent of another agent. Make a copy of your agent.\")\n                agent._manager = self\n            else:\n                raise ValueError(\"Subagents must be instances of Agent\")\n            if agent.name in names:\n                raise ValueError(\n                    f'Duplicate subagent name \"{agent.name}\" in subagent {i}, pass a unique name to the subagent during creation'\n                )\n            names.add(agent.name)\n        node_names = set()\n        for i, node in enumerate(self.nodes):\n            if node.name in node_names:\n                raise ValueError(\n                    f'Duplicate node name \"{node.name}\" in node {i}, pass a unique name to the node during creation'\n                )\n            node_names.add(node.name)\n        return super().model_post_init(__context)\n\n    @property\n    def manager(self):\n        \"\"\"\n        Gets the manager of the agent.\n\n        Returns:\n            (Agent): The manager agent instance.\n\n        Raises:\n            ValueError: If the agent doesn't have a manager assigned.\n        \"\"\"\n        if self._manager is None:\n            raise ValueError(\"Agent doesn't have a manager\")\n        return self._manager\n\n    @property\n    def llm(self):\n        \"\"\"\n        Get the default LLM instance associated with the agent.\n\n        Returns:\n            (LLM): The default LLM instance if only one LLM is configured.\n\n        Raises:\n            ValueError: If multiple LLMs are configured for this agent. In this case, use the `llms`\n                       property to access specific LLM instances.\n        \"\"\"\n        if len(self.llms) &gt; 1:\n            raise ValueError(\"Agent has multiple LLMs. Use llms property to access a specific one.\")\n        return self.llms[DEFAULT]\n\n    @property\n    def template(self):\n        \"\"\"\n        Returns the default template of the agent.\n\n        This property provides access to the default template when the agent has exactly one template.\n        If multiple templates exist, it raises a ValueError indicating that specific templates\n        should be accessed through the templates property instead.\n\n        Returns:\n            (Template): The default template object.\n\n        Raises:\n            ValueError: If the agent has more than one template.\n            IndexError: If no templates exist (implicitly through list access).\n        \"\"\"\n        if len(self.templates) &gt; 1:\n            raise ValueError(\"Agent has multiple templates. Use templates property to access a specific one.\")\n        return self.templates[DEFAULT]\n\n    @property\n    def full_name(self):\n        \"\"\"Returns the full hierarchical name of the agent.\n\n        The full name is constructed by combining the manager's full name (if present)\n        with this agent's name, separated by a forward slash. If the agent has no\n        manager, returns just the agent's name.\n\n        Returns:\n            (str): The full hierarchical name path of the agent. Examples: \"agent_name\" (no manager), \"manager_name/agent_name\" (with manager)\n        \"\"\"\n        if self._manager is None:\n            return self.name\n        return f\"{self._manager.full_name}/{self.name}\"\n\n    def find_subagent(self, name: str):\n        \"\"\"\n        Find a subagent by name in the list of subagents.\n\n        Args:\n            name (str): The name of the subagent to find.\n\n        Returns:\n            (Agent): The found subagent instance.\n\n        Raises:\n            ValueError: If no subagent with the given name is found.\n        \"\"\"\n        for agent in self.subagents:\n            if agent.name == name:\n                return agent\n        raise ValueError(f\"Agent {name} not found\")\n\n    def find_node(self, name: str):\n        \"\"\"Find a node by its name in the list of nodes.\n\n        Args:\n            name (str): The name of the node to find.\n\n        Returns:\n            (Node): The node with the matching name.\n\n        Raises:\n            ValueError: If no node with the given name is found.\n        \"\"\"\n        for node in self.nodes:\n            if node.name == name:\n                return node\n        raise ValueError(f\"Node {name} not found\")\n\n    def get_subagent_names(self) -&gt; list[str]:\n        \"\"\"\n        Returns a list of names of all subagents.\n\n        Returns:\n            list[str]: A list containing the names of all subagents in the agent.\n        \"\"\"\n        return [agent.name for agent in self.subagents]\n\n    def clone(self) -&gt; Self:\n        \"\"\"\n        Creates a deep copy of the current agent instance.\n\n        This method creates an independent copy of the agent with all its attributes,\n        but detaches it from any manager.\n\n        Returns:\n            Self: A new instance of the agent with identical attributes but no manager.\n        \"\"\"\n        result = self.model_copy(deep=True)\n        result._manager = None\n        return result\n\n    @classmethod\n    def create(\n        cls, llms: dict[str, LLM] | LLM | None = None, templates: dict[str, Any] | str | None = None, **kwargs\n    ) -&gt; Self:\n        \"\"\"\n        Creates an instance of the class with provided LLMs and templates.\n\n        Args:\n            llms (Union[Dict[str, LLM], LLM, None]): Language model(s) to use. Can be:\n\n                - A dictionary mapping names to LLM instances\n                - A single LLM instance (will be mapped to default name)\n                - None (empty dict will be used)\n            templates (Union[Dict[str, Any], str, None]): Template(s) to use. Can be:\n\n                - A dictionary mapping names to template configurations\n                - A single template string (will be mapped to default name)\n                - None (no templates will be used)\n            **kwargs (dict, optional): Additional keyword arguments to pass to the class constructor\n\n        Returns:\n            Self: A new instance of the class initialized with the provided arguments\n\n        Example:\n            ```python\n            agent = Agent.create(llm)  # Single LLM\n            agent = Agent.create({\"gpt\": llm1, \"claude\": llm2})  # Multiple LLMs\n            agent = Agent.create(llm, \"template\")  # LLM with template\n            ```\n        \"\"\"\n        if isinstance(llms, LLM):\n            llms = {DEFAULT: llms}\n        if isinstance(templates, str):\n            templates = {DEFAULT: templates}\n        if templates:\n            kwargs[\"templates\"] = templates\n\n        return cls(llms=llms or {}, **kwargs)\n\n    def update(self, agent_config: dict[str, Any]) -&gt; Agent[TapeType]:\n        \"\"\"\n        Updates the agent's configuration while preserving instance types.\n\n        This method allows reconfiguration of the agent while maintaining the class types\n        of LLMs and subagents. It performs a deep update by recursively applying changes\n        to nested components.\n\n        Args:\n            agent_config (dict[str, Any]): New configuration dictionary containing LLMs,\n                subagents, templates and other agent settings.\n\n        Returns:\n            Agent[TapeType]: A new agent instance with updated configuration.\n\n        Raises:\n            ValueError: If the new configuration has different LLMs or number of subagents\n                than the current agent.\n\n        Note:\n            - Only string templates are updated, complex template objects are preserved\n            - Node configurations are preserved to avoid potential issues\n        \"\"\"\n\n        if not set(self.llms.keys()) == set(agent_config[\"llms\"].keys()):\n            raise ValueError(\"Agent has different LLMs than the new configuration.\")\n        if len(self.subagents) != len(agent_config[\"subagents\"]):\n            raise ValueError(\"Agent has different number of subagents than the new configuration.\")\n        # recurse into subagents\n        subagents = [\n            subagent.model_validate(subagent.update(subagent_obj))\n            for subagent, subagent_obj in zip(self.subagents, agent_config[\"subagents\"])\n        ]\n        # recurse into llms\n        llms = {name: llm.model_validate(agent_config[\"llms\"][name]) for name, llm in self.llms.items()}\n        # only update templates are str\n        templates = {\n            name: (value if isinstance(value, str) else self.templates[name])\n            for name, value in agent_config[\"templates\"].items()\n        }\n        config_copy = agent_config.copy()\n        config_copy[\"llms\"] = llms\n        config_copy[\"subagents\"] = subagents\n        config_copy[\"templates\"] = templates\n        # do not update nodes for now to avoid tricky bugs\n        config_copy[\"nodes\"] = self.nodes\n        return type(self).model_validate(config_copy)\n\n    def compute_view(self, tape: TapeType) -&gt; TapeViewStack:\n        \"\"\"\n        Compute the view stack from a given tape.\n\n        Args:\n            tape (TapeType): The input tape to process.\n\n        Returns:\n            TapeViewStack: A stack of views computed from the input tape.\n        \"\"\"\n        return TapeViewStack.compute(tape)\n\n    def select_node(self, tape: TapeType) -&gt; Node:\n        \"\"\"\n        Select the next node to execute based on the current state of the tape.\n\n        The selection process follows these rules:\n            1. If next_node is explicitly set in the tape view, return that node\n            2. If no nodes have been run yet (last_node is None), return the first node\n            3. Return the node that follows the last executed node in the list\n\n        Args:\n            tape (TapeType): The tape containing execution state and data\n\n        Returns:\n            Node: The next node to be executed\n\n        Raises:\n            ValueError: If unable to determine the next node to execute (e.g., reached end of list)\n        \"\"\"\n        # Select the node to run next based on the current state of the tape.\n        view = self.compute_view(tape).top\n        if view.next_node:\n            logger.debug(f\"{self.name}: Next node was set explicitly in the tape: {view.next_node}\")\n            return self.find_node(view.next_node)\n\n        if not view.last_node:\n            logger.debug(f\"{self.name}: No nodes have been run yet, select node 0: {self.nodes[0].name}\")\n            return self.nodes[0]\n\n        # Select the next node that stored after the last node found in the tape\n        logger.debug(f\"{self.name}: Last node in view: {view.last_node}\")\n        logger.debug(f\"{self.name}: Known nodes: {[node.name for node in self.nodes]}\")\n        for i, node in enumerate(self.nodes):\n            if node.name == view.last_node and i + 1 &lt; len(self.nodes):\n                logger.debug(f\"{self.name}: Select immediate next node: {self.nodes[i + 1].name}\")\n                return self.nodes[i + 1]\n        raise ValueError(\"Next node not found\")\n\n    def make_prompt(self, tape: TapeType) -&gt; Prompt:\n        \"\"\"\n        Makes the prompt for the next iteration of the agent.\n        This method generates a prompt by delegating to the selected node's make_prompt method.\n        Can return a prompt with no messages, indicating the agent should generate next steps\n        by following rules without LLM assistance. Agents that only delegate to subagents may\n        not need to implement this method.\n\n        Args:\n            tape (TapeType): The tape containing the agent's state and history\n\n        Returns:\n            Prompt: A prompt object for the next agent iteration, potentially empty\n\n        Note:\n            - Empty prompts signal rule-based generation without LLM\n            - Method may be optional for pure delegation agents\n        \"\"\"\n\n        return self.select_node(tape).make_prompt(self, tape)\n\n    def generate_steps(self, tape: TapeType, llm_stream: LLMStream) -&gt; Generator[Step | PartialStep, None, None]:\n        \"\"\"\n        Generate steps from the agent by selecting a node and processing its output.\n\n        Args:\n            tape (TapeType): The input tape containing the interaction history\n            llm_stream (LLMStream): Stream interface for the language model output\n\n        Yields:\n            Union[Step, PartialStep]: Union[Step, PartialStep]: The generated steps or partial steps.\n        \"\"\"\n        # Generate new steps and other events by feeding the prompt to the LLM\n        node = self.select_node(tape)\n        for step in node.generate_steps(self, tape, llm_stream):\n            if isinstance(step, AgentStep):\n                step.metadata.node = node.name\n            yield step\n\n    def make_llm_output(self, tape: TapeType, index: int) -&gt; LLMOutput:\n        \"\"\"\n        Generates an LLM output based on a tape and step index.\n\n        Args:\n            tape (TapeType): The input tape\n            index (int): The position in the tape up to which to process.\n\n        Returns:\n            LLMOutput: The generated language model output for the tape segment.\n\n        Note:\n            This method delegates the actual output generation to the selected node's\n            make_llm_output method after selecting the appropriate node based on the\n            tape segment up to the given index.\n        \"\"\"\n        return self.select_node(tape[:index]).make_llm_output(self, tape, index)\n\n    def delegate(self, tape: TapeType) -&gt; Agent[TapeType]:\n        \"\"\"\n        Delegates control to the appropriate subagent based on the current tape state.\n\n        This method recursively traverses the agent hierarchy to find the most specific\n        subagent that should handle the current tape state based on views computed from\n        the tape.\n\n        Args:\n            tape (TapeType): The tape containing the current state to process.\n\n        Returns:\n            Agent[TapeType]: The subagent that should handle the current tape state.\n        \"\"\"\n        views = self.compute_view(tape)\n        subagent = self\n        for view in views.stack[1:]:\n            subagent = subagent.find_subagent(view.agent_name)\n        logger.debug(f\"{self.full_name}: Delegating to subagent: {subagent.full_name}\")\n        return subagent\n\n    def is_agent_step(self, step: Step) -&gt; bool:\n        \"\"\"\n        Check if a step was produced by the agent.\n\n        Args:\n            step (Step): The step object to check.\n\n        Returns:\n            bool: True if the step is an Action or Thought (agent-produced),\n                  False otherwise.\n        \"\"\"\n        return isinstance(step, (Action, Thought))\n\n    def should_stop(self, tape: TapeType) -&gt; bool:\n        \"\"\"\n        Check if the agent should stop its turn and wait for observations.\n\n        Args:\n            tape (TapeType): The tape containing the sequence of steps (actions and observations).\n\n        Returns:\n            bool: True if the last step in the tape is an Action, indicating the agent should stop and wait for observations.\n                 False if the last step is not an Action, indicating the agent can continue.\n        \"\"\"\n        return isinstance(tape.steps[-1], Action)\n\n    def run_iteration(\n        self, tape: TapeType, llm_stream: LLMStream | None = None\n    ) -&gt; Generator[Step | PartialStep, None, None]:\n        \"\"\"\n        Run one iteration of the agent (assuming one call to the underlyng model).\n\n        During an iteration the agent generates steps from a stream of tokens that arises\n        from a single LLM call with a single prompt. An agent can do multiple iterations\n        before returning the next action (see `run` method).\n\n        This function can also take a given `llm_stream`, which can be useful when the agent\n        reuses a tape.\n\n        Args:\n            tape (TapeType): The tape to run the agent on\n            llm_stream (LLMStream): The stream of tokens from the LLM\n\n        Yields:\n            Union[Step, PartialStep]: The generated steps or partial\n\n        Raises:\n            NotImplementedError: If the agent has multiple LLMs and no LLM stream is provided\n        \"\"\"\n        if llm_stream is None:\n            prompt = self.make_prompt(tape)\n            if len(self.llms) &gt; 1:\n                raise NotImplementedError(\"TODO: implement LLM choice in the prompt\")\n            llm_stream = self.llm.generate(prompt) if prompt else LLMStream(None, prompt)\n        for step in self.generate_steps(tape, llm_stream):\n            if isinstance(step, AgentStep):\n                step.metadata.prompt_id = llm_stream.prompt.id\n            yield step\n\n    def run(self, tape: TapeType, max_iterations: int | None = None) -&gt; AgentStream[TapeType]:\n        \"\"\"\n        Run the agent on the tape iteratively, delegating to subagents until a stop condition is met.\n\n        This method executes the agent's logic by:\n        1. Delegating to appropriate subagents based on the tape state\n        2. Processing steps from subagent iterations\n        3. Updating the tape with new steps\n        4. Checking stop conditions\n        5. Tracking metadata about the execution\n\n        Args:\n            tape (TapeType): The input tape to process\n            max_iterations (int, optional): Maximum number of iterations to run.\n                If None, uses self.max_iterations. Defaults to None.\n\n        Returns:\n            AgentStream[TapeType]: A stream of AgentEvents containing:\n\n                - partial_step: Intermediate processing steps\n                - step: Completed agent steps with updated tape\n                - final_tape: Final tape with updated metadata after completion\n\n        Yields:\n            AgentEvent: Events indicating the agent's progress including partial steps,\n                completed steps with updated tape, and the final result.\n\n        Raises:\n            ValueError: If the agent generates anything other than steps or partial steps.\n        \"\"\"\n        if max_iterations is None:\n            max_iterations = self.max_iterations\n\n        def _run_implementation():\n            nonlocal tape\n            n_iterations = 0\n            input_tape_length = len(tape)\n            input_tape_id = tape.metadata.id\n            stop = False\n            while n_iterations &lt; max_iterations and not stop:\n                current_subagent = self.delegate(tape)\n                for step in current_subagent.run_iteration(tape):\n                    if isinstance(step, PartialStep):\n                        yield AgentEvent(partial_step=step)\n                    elif isinstance(step, AgentStep):\n                        step.metadata.agent = current_subagent.full_name\n                        tape = tape.append(step)\n                        yield AgentEvent(step=step, partial_tape=tape)\n                        if self.should_stop(tape):\n                            stop = True\n                    else:\n                        raise ValueError(\"Agent can only generate steps or partial steps\")\n                n_iterations += 1\n            updated_metadata = tape.metadata.model_copy(\n                update=dict(\n                    parent_id=input_tape_id,\n                    author=self.name,\n                    n_added_steps=len(tape) - input_tape_length,\n                )\n            )\n            final_tape = tape.model_copy(update=dict(metadata=updated_metadata))\n            yield AgentEvent(final_tape=final_tape)\n\n        return AgentStream(_run_implementation())\n\n    def reuse(self, tape: TapeType) -&gt; tuple[TapeType, list[LLMCall]]:\n        \"\"\"\n        Reuse another agent's tape as one's own.\n\n        Construct LLM outputs at each step where a prompt is made. Check that output\n        parsing yield the same steps as in the original tape. Rewrite metadata for all steps.\n\n        Args:\n            tape (TapeType): The tape to reuse\n\n        Returns:\n            tuple[TapeType, list[LLMCall]]: The reused tape and a list of LLM calls made during the reuse\n\n        Raises:\n            TapeReuseFailure: If the regenerated steps don't match the original tape.\n        \"\"\"\n        reused_steps = []\n        llm_calls = []\n        i = 0\n        while i &lt; len(tape):\n            past_tape = tape[:i]\n            step = tape.steps[i]\n            if self.is_agent_step(step):\n                current_agent = self.delegate(past_tape)\n                prompt = current_agent.make_prompt(past_tape)\n                if not prompt:\n                    reused_steps.append(step)\n                    i += 1\n                    continue\n                output = current_agent.make_llm_output(tape, i)\n                llm_call = LLMCall(prompt=prompt, output=output, cached=True)\n                observe_llm_call(llm_call)\n\n                # Validate that the reconstructed llm call leads to the same steps as in the given tape\n                def _generator():\n                    yield LLMEvent(output=output)\n\n                new_steps = list(current_agent.run_iteration(past_tape, LLMStream(_generator(), prompt)))\n                for j, new_step in enumerate(new_steps):\n                    assert isinstance(new_step, Step)\n                    old_step = tape.steps[i + j]\n                    if type(old_step) is not type(new_step) or not _is_step_data_equal(old_step, new_step):\n                        raise TapeReuseFailure(\n                            f\"Can't reuse tape because regenerated step {i + j} data doesn't match\"\n                            f\"\\nold step data: {old_step.llm_dict()}\\nnew step data: {new_step.llm_dict()}\",\n                            partial_tape=past_tape,\n                        )\n                llm_calls.append(llm_call)\n                reused_steps.extend(new_steps)\n                i += len(new_steps)\n            else:\n                reused_steps.append(step)\n                i += 1\n        reused_tape = tape.model_validate(dict(context=tape.context, metadata=TapeMetadata(), steps=reused_steps))\n        return reused_tape, llm_calls\n\n    def get_node_runs(self, tape: TapeType) -&gt; list[tuple[Node, int]]:\n        \"\"\"\n        Parse the tape and identify the indices where each node began its execution.\n\n        This method identifies transition points in the tape where different nodes started\n        producing output by tracking changes in prompt IDs.\n\n        Args:\n            tape (TapeType): The sequence of tape steps to analyze.\n\n        Returns:\n            list[tuple[Node, int]]: List of tuples containing (node, index) pairs where:\n\n                - node: The Node object that produced the tape fragment\n                - index: The starting index in the tape where this node began execution\n        \"\"\"\n        last_prompt_id = None\n        result = []\n        for index, step in enumerate(tape):\n            if (prompt_id := step.metadata.prompt_id) and prompt_id != last_prompt_id:\n                node = self.find_node(step.metadata.node)\n                result.append((node, index))\n            last_prompt_id = prompt_id\n        return result\n\n    def make_training_text(self, llm_call: LLMCall) -&gt; TrainingText:\n        \"\"\"\n        Routes the request to make training text to the agent's LLM.\n\n        Args:\n            llm_call (LLMCall): Object containing prompt and output from an LLM call.\n\n        Returns:\n            TrainingText: The training text generated from the prompt and output.\n\n        Note:\n            Currently only supports one LLM. Future versions will support multiple LLMs.\n        \"\"\"\n        # TODO: support more than 1 LLM\n        return self.llm.make_training_text(llm_call.prompt, llm_call.output)\n\n    def make_training_data(self, tape: TapeType) -&gt; list[TrainingText]:\n        \"\"\"\n        Generates training data from a tape by converting LLM calls into training texts.\n\n        Args:\n            tape (TapeType): A tape containing recorded LLM interactions.\n\n        Returns:\n            list[TrainingText]: A list of training text objects created from the LLM calls.\n\n        Notes:\n            This method first reuses the tape to extract LLM calls, then converts each call\n            into a training text format using make_training_text().\n        \"\"\"\n        _, llm_calls = self.reuse(tape)\n        return [self.make_training_text(llm_call) for llm_call in llm_calls]\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.full_name","title":"<code>full_name</code>  <code>property</code>","text":"<p>Returns the full hierarchical name of the agent.</p> <p>The full name is constructed by combining the manager's full name (if present) with this agent's name, separated by a forward slash. If the agent has no manager, returns just the agent's name.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The full hierarchical name path of the agent. Examples: \"agent_name\" (no manager), \"manager_name/agent_name\" (with manager)</p> </li> </ul>"},{"location":"reference/agent/#tapeagents.agent.Agent.llm","title":"<code>llm</code>  <code>property</code>","text":"<p>Get the default LLM instance associated with the agent.</p> <p>Returns:</p> <ul> <li> <code>LLM</code>           \u2013            <p>The default LLM instance if only one LLM is configured.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If multiple LLMs are configured for this agent. In this case, use the <code>llms</code>        property to access specific LLM instances.</p> </li> </ul>"},{"location":"reference/agent/#tapeagents.agent.Agent.manager","title":"<code>manager</code>  <code>property</code>","text":"<p>Gets the manager of the agent.</p> <p>Returns:</p> <ul> <li> <code>Agent</code>           \u2013            <p>The manager agent instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent doesn't have a manager assigned.</p> </li> </ul>"},{"location":"reference/agent/#tapeagents.agent.Agent.template","title":"<code>template</code>  <code>property</code>","text":"<p>Returns the default template of the agent.</p> <p>This property provides access to the default template when the agent has exactly one template. If multiple templates exist, it raises a ValueError indicating that specific templates should be accessed through the templates property instead.</p> <p>Returns:</p> <ul> <li> <code>Template</code>           \u2013            <p>The default template object.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent has more than one template.</p> </li> <li> <code>IndexError</code>             \u2013            <p>If no templates exist (implicitly through list access).</p> </li> </ul>"},{"location":"reference/agent/#tapeagents.agent.Agent.clone","title":"<code>clone()</code>","text":"<p>Creates a deep copy of the current agent instance.</p> <p>This method creates an independent copy of the agent with all its attributes, but detaches it from any manager.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>A new instance of the agent with identical attributes but no manager.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def clone(self) -&gt; Self:\n    \"\"\"\n    Creates a deep copy of the current agent instance.\n\n    This method creates an independent copy of the agent with all its attributes,\n    but detaches it from any manager.\n\n    Returns:\n        Self: A new instance of the agent with identical attributes but no manager.\n    \"\"\"\n    result = self.model_copy(deep=True)\n    result._manager = None\n    return result\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.compute_view","title":"<code>compute_view(tape)</code>","text":"<p>Compute the view stack from a given tape.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The input tape to process.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TapeViewStack</code> (              <code>TapeViewStack</code> )          \u2013            <p>A stack of views computed from the input tape.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def compute_view(self, tape: TapeType) -&gt; TapeViewStack:\n    \"\"\"\n    Compute the view stack from a given tape.\n\n    Args:\n        tape (TapeType): The input tape to process.\n\n    Returns:\n        TapeViewStack: A stack of views computed from the input tape.\n    \"\"\"\n    return TapeViewStack.compute(tape)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.create","title":"<code>create(llms=None, templates=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class with provided LLMs and templates.</p> <p>Parameters:</p> <ul> <li> <code>llms</code>               (<code>Union[Dict[str, LLM], LLM, None]</code>, default:                   <code>None</code> )           \u2013            <p>Language model(s) to use. Can be:</p> <ul> <li>A dictionary mapping names to LLM instances</li> <li>A single LLM instance (will be mapped to default name)</li> <li>None (empty dict will be used)</li> </ul> </li> <li> <code>templates</code>               (<code>Union[Dict[str, Any], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>Template(s) to use. Can be:</p> <ul> <li>A dictionary mapping names to template configurations</li> <li>A single template string (will be mapped to default name)</li> <li>None (no templates will be used)</li> </ul> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the class constructor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>A new instance of the class initialized with the provided arguments</p> </li> </ul> Example <pre><code>agent = Agent.create(llm)  # Single LLM\nagent = Agent.create({\"gpt\": llm1, \"claude\": llm2})  # Multiple LLMs\nagent = Agent.create(llm, \"template\")  # LLM with template\n</code></pre> Source code in <code>tapeagents/agent.py</code> <pre><code>@classmethod\ndef create(\n    cls, llms: dict[str, LLM] | LLM | None = None, templates: dict[str, Any] | str | None = None, **kwargs\n) -&gt; Self:\n    \"\"\"\n    Creates an instance of the class with provided LLMs and templates.\n\n    Args:\n        llms (Union[Dict[str, LLM], LLM, None]): Language model(s) to use. Can be:\n\n            - A dictionary mapping names to LLM instances\n            - A single LLM instance (will be mapped to default name)\n            - None (empty dict will be used)\n        templates (Union[Dict[str, Any], str, None]): Template(s) to use. Can be:\n\n            - A dictionary mapping names to template configurations\n            - A single template string (will be mapped to default name)\n            - None (no templates will be used)\n        **kwargs (dict, optional): Additional keyword arguments to pass to the class constructor\n\n    Returns:\n        Self: A new instance of the class initialized with the provided arguments\n\n    Example:\n        ```python\n        agent = Agent.create(llm)  # Single LLM\n        agent = Agent.create({\"gpt\": llm1, \"claude\": llm2})  # Multiple LLMs\n        agent = Agent.create(llm, \"template\")  # LLM with template\n        ```\n    \"\"\"\n    if isinstance(llms, LLM):\n        llms = {DEFAULT: llms}\n    if isinstance(templates, str):\n        templates = {DEFAULT: templates}\n    if templates:\n        kwargs[\"templates\"] = templates\n\n    return cls(llms=llms or {}, **kwargs)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.delegate","title":"<code>delegate(tape)</code>","text":"<p>Delegates control to the appropriate subagent based on the current tape state.</p> <p>This method recursively traverses the agent hierarchy to find the most specific subagent that should handle the current tape state based on views computed from the tape.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape containing the current state to process.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Agent[TapeType]</code>           \u2013            <p>Agent[TapeType]: The subagent that should handle the current tape state.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def delegate(self, tape: TapeType) -&gt; Agent[TapeType]:\n    \"\"\"\n    Delegates control to the appropriate subagent based on the current tape state.\n\n    This method recursively traverses the agent hierarchy to find the most specific\n    subagent that should handle the current tape state based on views computed from\n    the tape.\n\n    Args:\n        tape (TapeType): The tape containing the current state to process.\n\n    Returns:\n        Agent[TapeType]: The subagent that should handle the current tape state.\n    \"\"\"\n    views = self.compute_view(tape)\n    subagent = self\n    for view in views.stack[1:]:\n        subagent = subagent.find_subagent(view.agent_name)\n    logger.debug(f\"{self.full_name}: Delegating to subagent: {subagent.full_name}\")\n    return subagent\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.find_node","title":"<code>find_node(name)</code>","text":"<p>Find a node by its name in the list of nodes.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node to find.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code>           \u2013            <p>The node with the matching name.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no node with the given name is found.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def find_node(self, name: str):\n    \"\"\"Find a node by its name in the list of nodes.\n\n    Args:\n        name (str): The name of the node to find.\n\n    Returns:\n        (Node): The node with the matching name.\n\n    Raises:\n        ValueError: If no node with the given name is found.\n    \"\"\"\n    for node in self.nodes:\n        if node.name == name:\n            return node\n    raise ValueError(f\"Node {name} not found\")\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.find_subagent","title":"<code>find_subagent(name)</code>","text":"<p>Find a subagent by name in the list of subagents.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the subagent to find.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Agent</code>           \u2013            <p>The found subagent instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no subagent with the given name is found.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def find_subagent(self, name: str):\n    \"\"\"\n    Find a subagent by name in the list of subagents.\n\n    Args:\n        name (str): The name of the subagent to find.\n\n    Returns:\n        (Agent): The found subagent instance.\n\n    Raises:\n        ValueError: If no subagent with the given name is found.\n    \"\"\"\n    for agent in self.subagents:\n        if agent.name == name:\n            return agent\n    raise ValueError(f\"Agent {name} not found\")\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.generate_steps","title":"<code>generate_steps(tape, llm_stream)</code>","text":"<p>Generate steps from the agent by selecting a node and processing its output.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The input tape containing the interaction history</p> </li> <li> <code>llm_stream</code>               (<code>LLMStream</code>)           \u2013            <p>Stream interface for the language model output</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Step | PartialStep</code>           \u2013            <p>Union[Step, PartialStep]: Union[Step, PartialStep]: The generated steps or partial steps.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def generate_steps(self, tape: TapeType, llm_stream: LLMStream) -&gt; Generator[Step | PartialStep, None, None]:\n    \"\"\"\n    Generate steps from the agent by selecting a node and processing its output.\n\n    Args:\n        tape (TapeType): The input tape containing the interaction history\n        llm_stream (LLMStream): Stream interface for the language model output\n\n    Yields:\n        Union[Step, PartialStep]: Union[Step, PartialStep]: The generated steps or partial steps.\n    \"\"\"\n    # Generate new steps and other events by feeding the prompt to the LLM\n    node = self.select_node(tape)\n    for step in node.generate_steps(self, tape, llm_stream):\n        if isinstance(step, AgentStep):\n            step.metadata.node = node.name\n        yield step\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.get_node_runs","title":"<code>get_node_runs(tape)</code>","text":"<p>Parse the tape and identify the indices where each node began its execution.</p> <p>This method identifies transition points in the tape where different nodes started producing output by tracking changes in prompt IDs.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The sequence of tape steps to analyze.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[tuple[Node, int]]</code>           \u2013            <p>list[tuple[Node, int]]: List of tuples containing (node, index) pairs where:</p> <ul> <li>node: The Node object that produced the tape fragment</li> <li>index: The starting index in the tape where this node began execution</li> </ul> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def get_node_runs(self, tape: TapeType) -&gt; list[tuple[Node, int]]:\n    \"\"\"\n    Parse the tape and identify the indices where each node began its execution.\n\n    This method identifies transition points in the tape where different nodes started\n    producing output by tracking changes in prompt IDs.\n\n    Args:\n        tape (TapeType): The sequence of tape steps to analyze.\n\n    Returns:\n        list[tuple[Node, int]]: List of tuples containing (node, index) pairs where:\n\n            - node: The Node object that produced the tape fragment\n            - index: The starting index in the tape where this node began execution\n    \"\"\"\n    last_prompt_id = None\n    result = []\n    for index, step in enumerate(tape):\n        if (prompt_id := step.metadata.prompt_id) and prompt_id != last_prompt_id:\n            node = self.find_node(step.metadata.node)\n            result.append((node, index))\n        last_prompt_id = prompt_id\n    return result\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.get_subagent_names","title":"<code>get_subagent_names()</code>","text":"<p>Returns a list of names of all subagents.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list containing the names of all subagents in the agent.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def get_subagent_names(self) -&gt; list[str]:\n    \"\"\"\n    Returns a list of names of all subagents.\n\n    Returns:\n        list[str]: A list containing the names of all subagents in the agent.\n    \"\"\"\n    return [agent.name for agent in self.subagents]\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.is_agent_step","title":"<code>is_agent_step(step)</code>","text":"<p>Check if a step was produced by the agent.</p> <p>Parameters:</p> <ul> <li> <code>step</code>               (<code>Step</code>)           \u2013            <p>The step object to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the step is an Action or Thought (agent-produced),   False otherwise.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def is_agent_step(self, step: Step) -&gt; bool:\n    \"\"\"\n    Check if a step was produced by the agent.\n\n    Args:\n        step (Step): The step object to check.\n\n    Returns:\n        bool: True if the step is an Action or Thought (agent-produced),\n              False otherwise.\n    \"\"\"\n    return isinstance(step, (Action, Thought))\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.make_llm_output","title":"<code>make_llm_output(tape, index)</code>","text":"<p>Generates an LLM output based on a tape and step index.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The input tape</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The position in the tape up to which to process.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMOutput</code> (              <code>LLMOutput</code> )          \u2013            <p>The generated language model output for the tape segment.</p> </li> </ul> Note <p>This method delegates the actual output generation to the selected node's make_llm_output method after selecting the appropriate node based on the tape segment up to the given index.</p> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_llm_output(self, tape: TapeType, index: int) -&gt; LLMOutput:\n    \"\"\"\n    Generates an LLM output based on a tape and step index.\n\n    Args:\n        tape (TapeType): The input tape\n        index (int): The position in the tape up to which to process.\n\n    Returns:\n        LLMOutput: The generated language model output for the tape segment.\n\n    Note:\n        This method delegates the actual output generation to the selected node's\n        make_llm_output method after selecting the appropriate node based on the\n        tape segment up to the given index.\n    \"\"\"\n    return self.select_node(tape[:index]).make_llm_output(self, tape, index)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.make_prompt","title":"<code>make_prompt(tape)</code>","text":"<p>Makes the prompt for the next iteration of the agent. This method generates a prompt by delegating to the selected node's make_prompt method. Can return a prompt with no messages, indicating the agent should generate next steps by following rules without LLM assistance. Agents that only delegate to subagents may not need to implement this method.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape containing the agent's state and history</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Prompt</code> (              <code>Prompt</code> )          \u2013            <p>A prompt object for the next agent iteration, potentially empty</p> </li> </ul> Note <ul> <li>Empty prompts signal rule-based generation without LLM</li> <li>Method may be optional for pure delegation agents</li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_prompt(self, tape: TapeType) -&gt; Prompt:\n    \"\"\"\n    Makes the prompt for the next iteration of the agent.\n    This method generates a prompt by delegating to the selected node's make_prompt method.\n    Can return a prompt with no messages, indicating the agent should generate next steps\n    by following rules without LLM assistance. Agents that only delegate to subagents may\n    not need to implement this method.\n\n    Args:\n        tape (TapeType): The tape containing the agent's state and history\n\n    Returns:\n        Prompt: A prompt object for the next agent iteration, potentially empty\n\n    Note:\n        - Empty prompts signal rule-based generation without LLM\n        - Method may be optional for pure delegation agents\n    \"\"\"\n\n    return self.select_node(tape).make_prompt(self, tape)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.make_training_data","title":"<code>make_training_data(tape)</code>","text":"<p>Generates training data from a tape by converting LLM calls into training texts.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>A tape containing recorded LLM interactions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[TrainingText]</code>           \u2013            <p>list[TrainingText]: A list of training text objects created from the LLM calls.</p> </li> </ul> Notes <p>This method first reuses the tape to extract LLM calls, then converts each call into a training text format using make_training_text().</p> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_training_data(self, tape: TapeType) -&gt; list[TrainingText]:\n    \"\"\"\n    Generates training data from a tape by converting LLM calls into training texts.\n\n    Args:\n        tape (TapeType): A tape containing recorded LLM interactions.\n\n    Returns:\n        list[TrainingText]: A list of training text objects created from the LLM calls.\n\n    Notes:\n        This method first reuses the tape to extract LLM calls, then converts each call\n        into a training text format using make_training_text().\n    \"\"\"\n    _, llm_calls = self.reuse(tape)\n    return [self.make_training_text(llm_call) for llm_call in llm_calls]\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.make_training_text","title":"<code>make_training_text(llm_call)</code>","text":"<p>Routes the request to make training text to the agent's LLM.</p> <p>Parameters:</p> <ul> <li> <code>llm_call</code>               (<code>LLMCall</code>)           \u2013            <p>Object containing prompt and output from an LLM call.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>The training text generated from the prompt and output.</p> </li> </ul> Note <p>Currently only supports one LLM. Future versions will support multiple LLMs.</p> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_training_text(self, llm_call: LLMCall) -&gt; TrainingText:\n    \"\"\"\n    Routes the request to make training text to the agent's LLM.\n\n    Args:\n        llm_call (LLMCall): Object containing prompt and output from an LLM call.\n\n    Returns:\n        TrainingText: The training text generated from the prompt and output.\n\n    Note:\n        Currently only supports one LLM. Future versions will support multiple LLMs.\n    \"\"\"\n    # TODO: support more than 1 LLM\n    return self.llm.make_training_text(llm_call.prompt, llm_call.output)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.reuse","title":"<code>reuse(tape)</code>","text":"<p>Reuse another agent's tape as one's own.</p> <p>Construct LLM outputs at each step where a prompt is made. Check that output parsing yield the same steps as in the original tape. Rewrite metadata for all steps.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape to reuse</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[TapeType, list[LLMCall]]</code>           \u2013            <p>tuple[TapeType, list[LLMCall]]: The reused tape and a list of LLM calls made during the reuse</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TapeReuseFailure</code>             \u2013            <p>If the regenerated steps don't match the original tape.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def reuse(self, tape: TapeType) -&gt; tuple[TapeType, list[LLMCall]]:\n    \"\"\"\n    Reuse another agent's tape as one's own.\n\n    Construct LLM outputs at each step where a prompt is made. Check that output\n    parsing yield the same steps as in the original tape. Rewrite metadata for all steps.\n\n    Args:\n        tape (TapeType): The tape to reuse\n\n    Returns:\n        tuple[TapeType, list[LLMCall]]: The reused tape and a list of LLM calls made during the reuse\n\n    Raises:\n        TapeReuseFailure: If the regenerated steps don't match the original tape.\n    \"\"\"\n    reused_steps = []\n    llm_calls = []\n    i = 0\n    while i &lt; len(tape):\n        past_tape = tape[:i]\n        step = tape.steps[i]\n        if self.is_agent_step(step):\n            current_agent = self.delegate(past_tape)\n            prompt = current_agent.make_prompt(past_tape)\n            if not prompt:\n                reused_steps.append(step)\n                i += 1\n                continue\n            output = current_agent.make_llm_output(tape, i)\n            llm_call = LLMCall(prompt=prompt, output=output, cached=True)\n            observe_llm_call(llm_call)\n\n            # Validate that the reconstructed llm call leads to the same steps as in the given tape\n            def _generator():\n                yield LLMEvent(output=output)\n\n            new_steps = list(current_agent.run_iteration(past_tape, LLMStream(_generator(), prompt)))\n            for j, new_step in enumerate(new_steps):\n                assert isinstance(new_step, Step)\n                old_step = tape.steps[i + j]\n                if type(old_step) is not type(new_step) or not _is_step_data_equal(old_step, new_step):\n                    raise TapeReuseFailure(\n                        f\"Can't reuse tape because regenerated step {i + j} data doesn't match\"\n                        f\"\\nold step data: {old_step.llm_dict()}\\nnew step data: {new_step.llm_dict()}\",\n                        partial_tape=past_tape,\n                    )\n            llm_calls.append(llm_call)\n            reused_steps.extend(new_steps)\n            i += len(new_steps)\n        else:\n            reused_steps.append(step)\n            i += 1\n    reused_tape = tape.model_validate(dict(context=tape.context, metadata=TapeMetadata(), steps=reused_steps))\n    return reused_tape, llm_calls\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.run","title":"<code>run(tape, max_iterations=None)</code>","text":"<p>Run the agent on the tape iteratively, delegating to subagents until a stop condition is met.</p> <p>This method executes the agent's logic by: 1. Delegating to appropriate subagents based on the tape state 2. Processing steps from subagent iterations 3. Updating the tape with new steps 4. Checking stop conditions 5. Tracking metadata about the execution</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The input tape to process</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of iterations to run. If None, uses self.max_iterations. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AgentStream[TapeType]</code>           \u2013            <p>AgentStream[TapeType]: A stream of AgentEvents containing:</p> <ul> <li>partial_step: Intermediate processing steps</li> <li>step: Completed agent steps with updated tape</li> <li>final_tape: Final tape with updated metadata after completion</li> </ul> </li> </ul> <p>Yields:</p> <ul> <li> <code>AgentEvent</code> (              <code>AgentStream[TapeType]</code> )          \u2013            <p>Events indicating the agent's progress including partial steps, completed steps with updated tape, and the final result.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent generates anything other than steps or partial steps.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def run(self, tape: TapeType, max_iterations: int | None = None) -&gt; AgentStream[TapeType]:\n    \"\"\"\n    Run the agent on the tape iteratively, delegating to subagents until a stop condition is met.\n\n    This method executes the agent's logic by:\n    1. Delegating to appropriate subagents based on the tape state\n    2. Processing steps from subagent iterations\n    3. Updating the tape with new steps\n    4. Checking stop conditions\n    5. Tracking metadata about the execution\n\n    Args:\n        tape (TapeType): The input tape to process\n        max_iterations (int, optional): Maximum number of iterations to run.\n            If None, uses self.max_iterations. Defaults to None.\n\n    Returns:\n        AgentStream[TapeType]: A stream of AgentEvents containing:\n\n            - partial_step: Intermediate processing steps\n            - step: Completed agent steps with updated tape\n            - final_tape: Final tape with updated metadata after completion\n\n    Yields:\n        AgentEvent: Events indicating the agent's progress including partial steps,\n            completed steps with updated tape, and the final result.\n\n    Raises:\n        ValueError: If the agent generates anything other than steps or partial steps.\n    \"\"\"\n    if max_iterations is None:\n        max_iterations = self.max_iterations\n\n    def _run_implementation():\n        nonlocal tape\n        n_iterations = 0\n        input_tape_length = len(tape)\n        input_tape_id = tape.metadata.id\n        stop = False\n        while n_iterations &lt; max_iterations and not stop:\n            current_subagent = self.delegate(tape)\n            for step in current_subagent.run_iteration(tape):\n                if isinstance(step, PartialStep):\n                    yield AgentEvent(partial_step=step)\n                elif isinstance(step, AgentStep):\n                    step.metadata.agent = current_subagent.full_name\n                    tape = tape.append(step)\n                    yield AgentEvent(step=step, partial_tape=tape)\n                    if self.should_stop(tape):\n                        stop = True\n                else:\n                    raise ValueError(\"Agent can only generate steps or partial steps\")\n            n_iterations += 1\n        updated_metadata = tape.metadata.model_copy(\n            update=dict(\n                parent_id=input_tape_id,\n                author=self.name,\n                n_added_steps=len(tape) - input_tape_length,\n            )\n        )\n        final_tape = tape.model_copy(update=dict(metadata=updated_metadata))\n        yield AgentEvent(final_tape=final_tape)\n\n    return AgentStream(_run_implementation())\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.run_iteration","title":"<code>run_iteration(tape, llm_stream=None)</code>","text":"<p>Run one iteration of the agent (assuming one call to the underlyng model).</p> <p>During an iteration the agent generates steps from a stream of tokens that arises from a single LLM call with a single prompt. An agent can do multiple iterations before returning the next action (see <code>run</code> method).</p> <p>This function can also take a given <code>llm_stream</code>, which can be useful when the agent reuses a tape.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape to run the agent on</p> </li> <li> <code>llm_stream</code>               (<code>LLMStream</code>, default:                   <code>None</code> )           \u2013            <p>The stream of tokens from the LLM</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Step | PartialStep</code>           \u2013            <p>Union[Step, PartialStep]: The generated steps or partial</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the agent has multiple LLMs and no LLM stream is provided</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def run_iteration(\n    self, tape: TapeType, llm_stream: LLMStream | None = None\n) -&gt; Generator[Step | PartialStep, None, None]:\n    \"\"\"\n    Run one iteration of the agent (assuming one call to the underlyng model).\n\n    During an iteration the agent generates steps from a stream of tokens that arises\n    from a single LLM call with a single prompt. An agent can do multiple iterations\n    before returning the next action (see `run` method).\n\n    This function can also take a given `llm_stream`, which can be useful when the agent\n    reuses a tape.\n\n    Args:\n        tape (TapeType): The tape to run the agent on\n        llm_stream (LLMStream): The stream of tokens from the LLM\n\n    Yields:\n        Union[Step, PartialStep]: The generated steps or partial\n\n    Raises:\n        NotImplementedError: If the agent has multiple LLMs and no LLM stream is provided\n    \"\"\"\n    if llm_stream is None:\n        prompt = self.make_prompt(tape)\n        if len(self.llms) &gt; 1:\n            raise NotImplementedError(\"TODO: implement LLM choice in the prompt\")\n        llm_stream = self.llm.generate(prompt) if prompt else LLMStream(None, prompt)\n    for step in self.generate_steps(tape, llm_stream):\n        if isinstance(step, AgentStep):\n            step.metadata.prompt_id = llm_stream.prompt.id\n        yield step\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.select_node","title":"<code>select_node(tape)</code>","text":"<p>Select the next node to execute based on the current state of the tape.</p> The selection process follows these rules <ol> <li>If next_node is explicitly set in the tape view, return that node</li> <li>If no nodes have been run yet (last_node is None), return the first node</li> <li>Return the node that follows the last executed node in the list</li> </ol> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape containing execution state and data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Node</code> (              <code>Node</code> )          \u2013            <p>The next node to be executed</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If unable to determine the next node to execute (e.g., reached end of list)</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def select_node(self, tape: TapeType) -&gt; Node:\n    \"\"\"\n    Select the next node to execute based on the current state of the tape.\n\n    The selection process follows these rules:\n        1. If next_node is explicitly set in the tape view, return that node\n        2. If no nodes have been run yet (last_node is None), return the first node\n        3. Return the node that follows the last executed node in the list\n\n    Args:\n        tape (TapeType): The tape containing execution state and data\n\n    Returns:\n        Node: The next node to be executed\n\n    Raises:\n        ValueError: If unable to determine the next node to execute (e.g., reached end of list)\n    \"\"\"\n    # Select the node to run next based on the current state of the tape.\n    view = self.compute_view(tape).top\n    if view.next_node:\n        logger.debug(f\"{self.name}: Next node was set explicitly in the tape: {view.next_node}\")\n        return self.find_node(view.next_node)\n\n    if not view.last_node:\n        logger.debug(f\"{self.name}: No nodes have been run yet, select node 0: {self.nodes[0].name}\")\n        return self.nodes[0]\n\n    # Select the next node that stored after the last node found in the tape\n    logger.debug(f\"{self.name}: Last node in view: {view.last_node}\")\n    logger.debug(f\"{self.name}: Known nodes: {[node.name for node in self.nodes]}\")\n    for i, node in enumerate(self.nodes):\n        if node.name == view.last_node and i + 1 &lt; len(self.nodes):\n            logger.debug(f\"{self.name}: Select immediate next node: {self.nodes[i + 1].name}\")\n            return self.nodes[i + 1]\n    raise ValueError(\"Next node not found\")\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.should_stop","title":"<code>should_stop(tape)</code>","text":"<p>Check if the agent should stop its turn and wait for observations.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>TapeType</code>)           \u2013            <p>The tape containing the sequence of steps (actions and observations).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the last step in the tape is an Action, indicating the agent should stop and wait for observations.  False if the last step is not an Action, indicating the agent can continue.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def should_stop(self, tape: TapeType) -&gt; bool:\n    \"\"\"\n    Check if the agent should stop its turn and wait for observations.\n\n    Args:\n        tape (TapeType): The tape containing the sequence of steps (actions and observations).\n\n    Returns:\n        bool: True if the last step in the tape is an Action, indicating the agent should stop and wait for observations.\n             False if the last step is not an Action, indicating the agent can continue.\n    \"\"\"\n    return isinstance(tape.steps[-1], Action)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Agent.update","title":"<code>update(agent_config)</code>","text":"<p>Updates the agent's configuration while preserving instance types.</p> <p>This method allows reconfiguration of the agent while maintaining the class types of LLMs and subagents. It performs a deep update by recursively applying changes to nested components.</p> <p>Parameters:</p> <ul> <li> <code>agent_config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>New configuration dictionary containing LLMs, subagents, templates and other agent settings.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Agent[TapeType]</code>           \u2013            <p>Agent[TapeType]: A new agent instance with updated configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the new configuration has different LLMs or number of subagents than the current agent.</p> </li> </ul> Note <ul> <li>Only string templates are updated, complex template objects are preserved</li> <li>Node configurations are preserved to avoid potential issues</li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def update(self, agent_config: dict[str, Any]) -&gt; Agent[TapeType]:\n    \"\"\"\n    Updates the agent's configuration while preserving instance types.\n\n    This method allows reconfiguration of the agent while maintaining the class types\n    of LLMs and subagents. It performs a deep update by recursively applying changes\n    to nested components.\n\n    Args:\n        agent_config (dict[str, Any]): New configuration dictionary containing LLMs,\n            subagents, templates and other agent settings.\n\n    Returns:\n        Agent[TapeType]: A new agent instance with updated configuration.\n\n    Raises:\n        ValueError: If the new configuration has different LLMs or number of subagents\n            than the current agent.\n\n    Note:\n        - Only string templates are updated, complex template objects are preserved\n        - Node configurations are preserved to avoid potential issues\n    \"\"\"\n\n    if not set(self.llms.keys()) == set(agent_config[\"llms\"].keys()):\n        raise ValueError(\"Agent has different LLMs than the new configuration.\")\n    if len(self.subagents) != len(agent_config[\"subagents\"]):\n        raise ValueError(\"Agent has different number of subagents than the new configuration.\")\n    # recurse into subagents\n    subagents = [\n        subagent.model_validate(subagent.update(subagent_obj))\n        for subagent, subagent_obj in zip(self.subagents, agent_config[\"subagents\"])\n    ]\n    # recurse into llms\n    llms = {name: llm.model_validate(agent_config[\"llms\"][name]) for name, llm in self.llms.items()}\n    # only update templates are str\n    templates = {\n        name: (value if isinstance(value, str) else self.templates[name])\n        for name, value in agent_config[\"templates\"].items()\n    }\n    config_copy = agent_config.copy()\n    config_copy[\"llms\"] = llms\n    config_copy[\"subagents\"] = subagents\n    config_copy[\"templates\"] = templates\n    # do not update nodes for now to avoid tricky bugs\n    config_copy[\"nodes\"] = self.nodes\n    return type(self).model_validate(config_copy)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.AgentStream","title":"<code>AgentStream</code>","text":"<p>               Bases: <code>Generic[TapeType]</code></p> <p>A wrapper around a generator that produces AgentEvents, representing the result of an agent run.</p> <p>The generator can be iterated over to get the events, or the final tape can be extracted with get_final_tape. Support iterable protocol and generator protocol.</p> <p>Attributes:</p> <ul> <li> <code>generator</code>               (<code>Generator[AgentEvent[TapeType], None, None]</code>)           \u2013            <p>The generator that produces AgentEvents.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_final_tape</code>             \u2013              <p>Retrieve the final tape from the agent's events.</p> </li> <li> <code>get_steps</code>             \u2013              <p>Generator function that yields steps from events.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>class AgentStream(Generic[TapeType]):\n    \"\"\"\n    A wrapper around a generator that produces AgentEvents, representing the result of an agent run.\n\n    The generator can be iterated over to get the events, or the final tape can be extracted with get_final_tape.\n    Support iterable protocol and generator protocol.\n\n    Attributes:\n        generator (Generator[AgentEvent[TapeType], None, None]): The generator that produces AgentEvents.\n    \"\"\"\n\n    def __init__(self, generator: Generator[AgentEvent[TapeType], None, None]):\n        self.generator = generator\n\n    def __iter__(self):\n        return self.generator\n\n    def __next__(self) -&gt; AgentEvent[TapeType]:\n        return next(self.generator)\n\n    def get_final_tape(self) -&gt; TapeType:\n        \"\"\"\n        Retrieve the final tape from the agent's events.\n\n        Iterates through the events of the agent and returns the final tape\n        if it is found. If no final tape is produced by the agent, a ValueError\n        is raised.\n\n        Returns:\n            TapeType: The final tape produced by the agent.\n\n        Raises:\n            ValueError: If the agent did not produce a final tape.\n        \"\"\"\n        for event in self:\n            if event.final_tape:\n                return event.final_tape\n        raise ValueError(\"Agent didn't produce final tape\")\n\n    def get_steps(self) -&gt; Generator[Step, None, None]:\n        \"\"\"\n        Generator function that yields steps from events.\n\n        Yields:\n            Step: The step associated with each event that has a step.\n        \"\"\"\n        for event in self:\n            if event.step:\n                yield event.step\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.AgentStream.get_final_tape","title":"<code>get_final_tape()</code>","text":"<p>Retrieve the final tape from the agent's events.</p> <p>Iterates through the events of the agent and returns the final tape if it is found. If no final tape is produced by the agent, a ValueError is raised.</p> <p>Returns:</p> <ul> <li> <code>TapeType</code> (              <code>TapeType</code> )          \u2013            <p>The final tape produced by the agent.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent did not produce a final tape.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def get_final_tape(self) -&gt; TapeType:\n    \"\"\"\n    Retrieve the final tape from the agent's events.\n\n    Iterates through the events of the agent and returns the final tape\n    if it is found. If no final tape is produced by the agent, a ValueError\n    is raised.\n\n    Returns:\n        TapeType: The final tape produced by the agent.\n\n    Raises:\n        ValueError: If the agent did not produce a final tape.\n    \"\"\"\n    for event in self:\n        if event.final_tape:\n            return event.final_tape\n    raise ValueError(\"Agent didn't produce final tape\")\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.AgentStream.get_steps","title":"<code>get_steps()</code>","text":"<p>Generator function that yields steps from events.</p> <p>Yields:</p> <ul> <li> <code>Step</code> (              <code>Step</code> )          \u2013            <p>The step associated with each event that has a step.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def get_steps(self) -&gt; Generator[Step, None, None]:\n    \"\"\"\n    Generator function that yields steps from events.\n\n    Yields:\n        Step: The step associated with each event that has a step.\n    \"\"\"\n    for event in self:\n        if event.step:\n            yield event.step\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Annotator","title":"<code>Annotator</code>","text":"<p>               Bases: <code>Agent[AnnotatorTapeType]</code>, <code>Generic[TapeType, AnnotatorTapeType]</code></p> <p>Annotator is the base class for agents that produce annotations for the tape of another agent. It annotates the tape by converting it into its own tape and then producing an annotation step appended to the converted tape.</p> Source code in <code>tapeagents/agent.py</code> <pre><code>class Annotator(Agent[AnnotatorTapeType], Generic[TapeType, AnnotatorTapeType]):\n    \"\"\"\n    Annotator is the base class for agents that produce annotations for the tape of another agent.\n    It annotates the tape by converting it into its own tape and then producing an annotation step appended to the converted tape.\n    \"\"\"\n\n    @abstractmethod\n    def make_own_tape(self, tape: TapeType) -&gt; AnnotatorTapeType:\n        pass\n\n    def annotate(self, tape: TapeType) -&gt; AnnotatorTapeType:\n        return self.run(self.make_own_tape(tape)).get_final_tape()\n\n    def get_annotation(self, own_tape: AnnotatorTapeType) -&gt; Any:\n        return own_tape.steps[-1].annotation\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A node in the agent, atomic unit of the agent's behavior.</p> <p>The agent chooses which node to run based on the current tape. The node has a name and contains 2 main functions:</p> <ul> <li>make a prompt out of the tape</li> <li>generate steps out of the received llm output</li> </ul> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the node. Defaults to an empty string.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>generate_steps</code>             \u2013              <p>Generates steps for the given agent, tape, and LLM stream.</p> </li> <li> <code>make_llm_output</code>             \u2013              <p>Generates an LLMOutput object for a given agent and tape at a specified index.</p> </li> <li> <code>make_prompt</code>             \u2013              <p>Creates a prompt for the given agent and tape.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"\n    A node in the agent, atomic unit of the agent's behavior.\n\n    The agent chooses which node to run based on the current tape.\n    The node has a name and contains 2 main functions:\n\n    - make a prompt out of the tape\n    - generate steps out of the received llm output\n\n    Attributes:\n        name (str): The name of the node. Defaults to an empty string.\n    \"\"\"\n\n    name: str = \"\"\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        if not self.name:\n            self.name = self.__class__.__name__.split(\"[\")[0]  # class name without type variables\n\n    def make_prompt(self, agent: Any, tape: Tape) -&gt; Prompt:\n        \"\"\"\n        Creates a prompt for the given agent and tape.\n\n        Args:\n            agent (Any): The agent for which the prompt is being created.\n            tape (Tape): The tape associated with the agent.\n\n        Returns:\n            Prompt: The generated prompt.\n        \"\"\"\n        return Prompt()\n\n    def generate_steps(\n        self, agent: Any, tape: Tape, llm_stream: LLMStream\n    ) -&gt; Generator[Step | PartialStep, None, None]:\n        \"\"\"\n        Generates steps for the given agent, tape, and LLM stream.\n\n        Args:\n            agent (Any): The agent for which steps are to be generated.\n            tape (Tape): The tape object containing relevant data.\n            llm_stream (LLMStream): The LLM stream to be used for generating steps.\n\n        Yields:\n            Union[Step, PartialStep]: The generated steps or partial steps.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by the subclass.\n        \"\"\"\n        raise NotImplementedError(\"Node must implement generate_steps\")\n\n    def make_llm_output(self, agent: Any, tape: Tape, index: int) -&gt; LLMOutput:\n        \"\"\"\n        Generates an LLMOutput object for a given agent and tape at a specified index.\n\n        Args:\n            agent (Any): The agent for which the LLMOutput is being generated.\n            tape (Tape): The tape containing the steps.\n            index (int): The index of the step in the tape from which to generate the output.\n\n        Returns:\n            LLMOutput: An object containing the role and content for the LLM output.\n        \"\"\"\n        return LLMOutput(role=\"assistant\", content=tape.steps[index].content)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Node.generate_steps","title":"<code>generate_steps(agent, tape, llm_stream)</code>","text":"<p>Generates steps for the given agent, tape, and LLM stream.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent for which steps are to be generated.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing relevant data.</p> </li> <li> <code>llm_stream</code>               (<code>LLMStream</code>)           \u2013            <p>The LLM stream to be used for generating steps.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Step | PartialStep</code>           \u2013            <p>Union[Step, PartialStep]: The generated steps or partial steps.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented by the subclass.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def generate_steps(\n    self, agent: Any, tape: Tape, llm_stream: LLMStream\n) -&gt; Generator[Step | PartialStep, None, None]:\n    \"\"\"\n    Generates steps for the given agent, tape, and LLM stream.\n\n    Args:\n        agent (Any): The agent for which steps are to be generated.\n        tape (Tape): The tape object containing relevant data.\n        llm_stream (LLMStream): The LLM stream to be used for generating steps.\n\n    Yields:\n        Union[Step, PartialStep]: The generated steps or partial steps.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by the subclass.\n    \"\"\"\n    raise NotImplementedError(\"Node must implement generate_steps\")\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Node.make_llm_output","title":"<code>make_llm_output(agent, tape, index)</code>","text":"<p>Generates an LLMOutput object for a given agent and tape at a specified index.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent for which the LLMOutput is being generated.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape containing the steps.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index of the step in the tape from which to generate the output.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMOutput</code> (              <code>LLMOutput</code> )          \u2013            <p>An object containing the role and content for the LLM output.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_llm_output(self, agent: Any, tape: Tape, index: int) -&gt; LLMOutput:\n    \"\"\"\n    Generates an LLMOutput object for a given agent and tape at a specified index.\n\n    Args:\n        agent (Any): The agent for which the LLMOutput is being generated.\n        tape (Tape): The tape containing the steps.\n        index (int): The index of the step in the tape from which to generate the output.\n\n    Returns:\n        LLMOutput: An object containing the role and content for the LLM output.\n    \"\"\"\n    return LLMOutput(role=\"assistant\", content=tape.steps[index].content)\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.Node.make_prompt","title":"<code>make_prompt(agent, tape)</code>","text":"<p>Creates a prompt for the given agent and tape.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent for which the prompt is being created.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape associated with the agent.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Prompt</code> (              <code>Prompt</code> )          \u2013            <p>The generated prompt.</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>def make_prompt(self, agent: Any, tape: Tape) -&gt; Prompt:\n    \"\"\"\n    Creates a prompt for the given agent and tape.\n\n    Args:\n        agent (Any): The agent for which the prompt is being created.\n        tape (Tape): The tape associated with the agent.\n\n    Returns:\n        Prompt: The generated prompt.\n    \"\"\"\n    return Prompt()\n</code></pre>"},{"location":"reference/agent/#tapeagents.agent.TapeReuseFailure","title":"<code>TapeReuseFailure</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Exception raised when tape reuse operation fails.</p> <p>This exception is raised when an attempt to reuse a tape encounters an error, providing access to the succesfully reused part of the tape</p> <p>Parameters:</p> <ul> <li> <code>msg</code>               (<code>str</code>)           \u2013            <p>Description of why the tape reuse failed</p> </li> <li> <code>partial_tape</code>               (<code>Tape</code>)           \u2013            <p>The incomplete/partial tape that was being constructed</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>partial_tape</code>               (<code>Tape</code>)           \u2013            <p>The incomplete tape at the point of failure</p> </li> </ul> Source code in <code>tapeagents/agent.py</code> <pre><code>class TapeReuseFailure(ValueError):\n    \"\"\"Exception raised when tape reuse operation fails.\n\n    This exception is raised when an attempt to reuse a tape encounters an error,\n    providing access to the succesfully reused part of the tape\n\n    Args:\n        msg (str): Description of why the tape reuse failed\n        partial_tape (Tape): The incomplete/partial tape that was being constructed\n\n    Attributes:\n        partial_tape (Tape): The incomplete tape at the point of failure\n    \"\"\"\n\n    def __init__(self, msg: str, partial_tape: Tape):\n        self.partial_tape = partial_tape\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/batch/","title":"Batch","text":"<p>Batch processing of tapes.</p> <p>Functions:</p> <ul> <li> <code>batch_main_loop</code>             \u2013              <p>Continue tapes in parallel using an agent.</p> </li> </ul>"},{"location":"reference/batch/#tapeagents.batch.batch_main_loop","title":"<code>batch_main_loop(agent, tapes, environments, n_workers=_DEFAULT_N_WORKERS, strict=False, max_loops=-1)</code>","text":"<p>Continue tapes in parallel using an agent.</p> Source code in <code>tapeagents/batch.py</code> <pre><code>def batch_main_loop(\n    agent: Agent[TapeType],\n    tapes: list[TapeType],\n    environments: Environment | list[Environment],\n    n_workers: int = _DEFAULT_N_WORKERS,\n    strict: bool = False,\n    max_loops: int = -1,\n) -&gt; Generator[TapeType, None, None]:\n    \"\"\"Continue tapes in parallel using an agent.\"\"\"\n    if not isinstance(environments, list):\n        environments = [environments] * len(tapes)\n\n    def worker_func(input: tuple[TapeType, Environment]) -&gt; TapeType | Exception:\n        start_tape, env = input\n        try:\n            result = main_loop(agent, start_tape, env, max_loops=max_loops).get_final_tape()\n        except Exception as e:\n            if is_debug_mode() or strict:\n                return e\n            return start_tape.model_copy(\n                update=dict(metadata=TapeMetadata(parent_id=start_tape.metadata.id, error=traceback.format_exc()))\n            )\n        result.metadata.parent_id = start_tape.metadata.id\n        return result\n\n    processor = choose_processor(n_workers=n_workers)\n    for smth in processor(zip(tapes, environments), worker_func):\n        if isinstance(smth, Tape):\n            yield smth\n        else:\n            raise smth\n</code></pre>"},{"location":"reference/config/","title":"Config","text":""},{"location":"reference/core/","title":"Core","text":"<p>Core data structures for the tape agents framework.</p> <p>Classes:</p> <ul> <li> <code>Action</code>           \u2013            <p>Base class representing an agent's action in a tape.</p> </li> <li> <code>AgentEvent</code>           \u2013            <p>Event produced by the agent during the run.</p> </li> <li> <code>AgentStep</code>           \u2013            <p>Base class representing a step produced by an agent.</p> </li> <li> <code>Call</code>           \u2013            <p>Action that calls another agent.</p> </li> <li> <code>Episode</code>           \u2013            <p>Auxiliary data structure for tape with annotations attached.</p> </li> <li> <code>Error</code>           \u2013            <p>Base class representing an error in a tape.</p> </li> <li> <code>FinalStep</code>           \u2013            <p>Action that stops orchestrator loop with a reason.</p> </li> <li> <code>LLMCall</code>           \u2013            <p>LLMCall stores info about a call to a language model.</p> </li> <li> <code>LLMOutputParsingFailureAction</code>           \u2013            <p>Represents an action that is produced automatically when the LLM output parsing fails.</p> </li> <li> <code>Observation</code>           \u2013            <p>Base class representing an observation in a tape.</p> </li> <li> <code>PartialStep</code>           \u2013            <p>Wrap your step as partial step to indicate that it is not finished yet.</p> </li> <li> <code>Pass</code>           \u2013            <p>Action that does nothing.</p> </li> <li> <code>Prompt</code>           \u2013            <p>A class representing a LLM prompt with messages and tools.</p> </li> <li> <code>Respond</code>           \u2013            <p>Action that returns a response to the top-level agent after processing the call.</p> </li> <li> <code>SetNextNode</code>           \u2013            <p>Action that sets the next node to run in the current agent.</p> </li> <li> <code>Step</code>           \u2013            <p>Base class representing a step in a tape.</p> </li> <li> <code>StepMetadata</code>           \u2013            <p>StepMetadata is a model that represents metadata for a tape step.</p> </li> <li> <code>StopStep</code>           \u2013            <p>Action that stops orchestrator loop.</p> </li> <li> <code>Tape</code>           \u2013            <p>Tape class represents a sequence of steps produced by agents and environments with associated metadata.</p> </li> <li> <code>TapeMetadata</code>           \u2013            <p>TapeMetadata is a model that represents metadata information for a tape.</p> </li> <li> <code>Thought</code>           \u2013            <p>Base class representing an agent's thought in a tape.</p> </li> <li> <code>TrainingText</code>           \u2013            <p>Training text instance used to finetune a language model.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>AnnotatorTape</code>           \u2013            <p>Type alias for annotator tapes.</p> </li> <li> <code>AnnotatorTapeType</code>           \u2013            <p>Type variable for annotator tape types.</p> </li> <li> <code>ContextType</code>           \u2013            <p>Type variable for context types.</p> </li> <li> <code>LLMOutput</code>               (<code>TypeAlias</code>)           \u2013            <p>Type alias for the output of the language model.</p> </li> <li> <code>StepType</code>           \u2013            <p>Type variable for step types.</p> </li> <li> <code>TapeType</code>           \u2013            <p>Type variable for tape types.</p> </li> </ul>"},{"location":"reference/core/#tapeagents.core.AnnotatorTape","title":"<code>AnnotatorTape = Tape[TapeType, StepType]</code>  <code>module-attribute</code>","text":"<p>Type alias for annotator tapes.</p>"},{"location":"reference/core/#tapeagents.core.AnnotatorTapeType","title":"<code>AnnotatorTapeType = TypeVar('AnnotatorTapeType', bound=AnnotatorTape)</code>  <code>module-attribute</code>","text":"<p>Type variable for annotator tape types.</p>"},{"location":"reference/core/#tapeagents.core.ContextType","title":"<code>ContextType = TypeVar('ContextType')</code>  <code>module-attribute</code>","text":"<p>Type variable for context types.</p>"},{"location":"reference/core/#tapeagents.core.LLMOutput","title":"<code>LLMOutput: TypeAlias = litellm.utils.Message</code>  <code>module-attribute</code>","text":"<p>Type alias for the output of the language model.</p>"},{"location":"reference/core/#tapeagents.core.StepType","title":"<code>StepType = TypeVar('StepType', bound=Action | Observation | Thought)</code>  <code>module-attribute</code>","text":"<p>Type variable for step types.</p>"},{"location":"reference/core/#tapeagents.core.TapeType","title":"<code>TapeType = TypeVar('TapeType', bound=Tape)</code>  <code>module-attribute</code>","text":"<p>Type variable for tape types.</p>"},{"location":"reference/core/#tapeagents.core.Action","title":"<code>Action</code>","text":"<p>               Bases: <code>AgentStep</code></p> <p>Base class representing an agent's action in a tape.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Action(AgentStep):\n    \"\"\"\n    Base class representing an agent's action in a tape.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.AgentEvent","title":"<code>AgentEvent</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[TapeType]</code></p> <p>Event produced by the agent during the run.</p> <p>Can contain a step, a final tape with all new steps, or a partial step when used in streaming mode. Attributes are mutually exclusive.</p> <p>Attributes:</p> <ul> <li> <code>step</code>               (<code>SerializeAsAny[Step]</code>)           \u2013            <p>A step produced by the agent.</p> </li> <li> <code>partial_step</code>               (<code>PartialStep</code>)           \u2013            <p>A partial step produced by the agent.</p> </li> <li> <code>partial_tape</code>               (<code>TapeType</code>)           \u2013            <p>A partial tape produced by the agent.</p> </li> <li> <code>final_tape</code>               (<code>TapeType</code>)           \u2013            <p>A final tape produced by the agent.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class AgentEvent(BaseModel, Generic[TapeType]):\n    \"\"\"\n    Event produced by the agent during the run.\n\n    Can contain a step, a final tape with all new steps, or a partial step when used in streaming mode.\n    Attributes are mutually exclusive.\n\n    Attributes:\n        step (SerializeAsAny[Step], optional): A step produced by the agent.\n        partial_step (PartialStep, optional): A partial step produced by the agent.\n        partial_tape (TapeType, optional): A partial tape produced by the agent.\n        final_tape (TapeType, optional): A final tape produced by the agent.\n    \"\"\"\n\n    step: SerializeAsAny[Step] | None = None\n    partial_step: PartialStep | None = None\n    partial_tape: TapeType | None = None\n    final_tape: TapeType | None = None\n</code></pre>"},{"location":"reference/core/#tapeagents.core.AgentStep","title":"<code>AgentStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Base class representing a step produced by an agent.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class AgentStep(Step):\n    \"\"\"\n    Base class representing a step produced by an agent.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Call","title":"<code>Call</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Action that calls another agent.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['call']</code>)           \u2013            <p>A constant string indicating the type of action.</p> </li> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content passed with the call to the agent.</p> </li> <li> <code>agent_name</code>               (<code>str</code>)           \u2013            <p>The name of the agent to call.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class Call(Thought):\n    \"\"\"\n    Action that calls another agent.\n\n    Attributes:\n        kind (Literal[\"call\"]): A constant string indicating the type of action.\n        content (str): The content passed with the call to the agent.\n        agent_name (str): The name of the agent to call.\n    \"\"\"\n\n    kind: Literal[\"call\"] = \"call\"\n    content: str = \"\"\n    agent_name: str\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Episode","title":"<code>Episode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Auxiliary data structure for tape with annotations attached. Humans may want to look at a tape with one-or-many annotations attached to agent's actions. This is a data structure to store such tapes. We store the tape id for traceability.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Episode(BaseModel):\n    \"\"\"\n    Auxiliary data structure for tape with annotations attached.\n    Humans may want to look at a tape with one-or-many annotations attached to agent's actions.\n    This is a data structure to store such tapes. We store the tape id for traceability.\n    \"\"\"\n\n    tape: Tape\n    annotator_tapes: dict[int, list[Tape]]\n    obs_making_tapes: dict[int, Tape]\n\n    def group_by_step(self) -&gt; Iterator[tuple[Tape | None, Step, list[Tape]]]:\n        for i, step in enumerate(self.tape.steps):\n            yield (\n                self.obs_making_tapes.get(i, None),\n                step,\n                self.annotator_tapes.get(i, []),\n            )\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Error","title":"<code>Error</code>","text":"<p>               Bases: <code>Step</code></p> <p>Base class representing an error in a tape.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Error(Step):\n    \"\"\"\n    Base class representing an error in a tape.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.FinalStep","title":"<code>FinalStep</code>","text":"<p>               Bases: <code>StopStep</code></p> <p>Action that stops orchestrator loop with a reason.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['final_step']</code>)           \u2013            <p>A constant string indicating the type of action.</p> </li> <li> <code>reason</code>               (<code>str</code>)           \u2013            <p>The reason for stopping the orchestr</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class FinalStep(StopStep):\n    \"\"\"\n    Action that stops orchestrator loop with a reason.\n\n    Attributes:\n        kind (Literal[\"final_step\"]): A constant string indicating the type of action.\n        reason (str): The reason for stopping the orchestr\n    \"\"\"\n\n    kind: Literal[\"final_step\"] = \"final_step\"\n    reason: str = \"\"\n</code></pre>"},{"location":"reference/core/#tapeagents.core.LLMCall","title":"<code>LLMCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLMCall stores info about a call to a language model.</p> <p>Attributes:</p> <ul> <li> <code>timestamp</code>               (<code>str</code>)           \u2013            <p>The timestamp when the call was made, in ISO 8601 format.</p> </li> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The input prompt provided to the language model.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output generated by the language model.</p> </li> <li> <code>prompt_length_tokens</code>               (<code>int</code>)           \u2013            <p>The length of the prompt in tokens. Defaults to -1 if not set.</p> </li> <li> <code>output_length_tokens</code>               (<code>int</code>)           \u2013            <p>The length of the output in tokens. Defaults to -1 if not set.</p> </li> <li> <code>cached</code>               (<code>bool</code>)           \u2013            <p>Indicates whether the result was retrieved from cache.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class LLMCall(BaseModel):\n    \"\"\"\n    LLMCall stores info about a call to a language model.\n\n    Attributes:\n        timestamp (str): The timestamp when the call was made, in ISO 8601 format.\n        prompt (Prompt): The input prompt provided to the language model.\n        output (LLMOutput): The output generated by the language model.\n        prompt_length_tokens (int): The length of the prompt in tokens. Defaults to -1 if not set.\n        output_length_tokens (int): The length of the output in tokens. Defaults to -1 if not set.\n        cached (bool): Indicates whether the result was retrieved from cache.\n    \"\"\"\n\n    timestamp: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())\n    prompt: Prompt\n    output: LLMOutput\n    prompt_length_tokens: int = -1\n    output_length_tokens: int = -1\n    cached: bool\n</code></pre>"},{"location":"reference/core/#tapeagents.core.LLMOutputParsingFailureAction","title":"<code>LLMOutputParsingFailureAction</code>","text":"<p>               Bases: <code>Action</code>, <code>Error</code></p> <p>Represents an action that is produced automatically when the LLM output parsing fails.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['llm_output_parsing_failure_action']</code>)           \u2013            <p>A constant string indicating the type of action.</p> </li> <li> <code>error</code>               (<code>str</code>)           \u2013            <p>A description of the error that occurred during parsing.</p> </li> <li> <code>llm_output</code>               (<code>str</code>)           \u2013            <p>The raw output from the LLM that could not be parsed.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class LLMOutputParsingFailureAction(Action, Error):\n    \"\"\"\n    Represents an action that is produced automatically when the LLM output parsing fails.\n\n    Attributes:\n        kind (Literal[\"llm_output_parsing_failure_action\"]): A constant string indicating the type of action.\n        error (str): A description of the error that occurred during parsing.\n        llm_output (str): The raw output from the LLM that could not be parsed.\n    \"\"\"\n\n    kind: Literal[\"llm_output_parsing_failure_action\"] = \"llm_output_parsing_failure_action\"\n    error: str\n    llm_output: str\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Observation","title":"<code>Observation</code>","text":"<p>               Bases: <code>Step</code></p> <p>Base class representing an observation in a tape.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Observation(Step):\n    \"\"\"\n    Base class representing an observation in a tape.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.PartialStep","title":"<code>PartialStep</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrap your step as partial step to indicate that it is not finished yet.</p> <p>Attributes:</p> <ul> <li> <code>step</code>               (<code>Step</code>)           \u2013            <p>An instance of the Step class representing the step details.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class PartialStep(BaseModel):\n    \"\"\"\n    Wrap your step as partial step to indicate that it is not finished yet.\n\n    Attributes:\n        step (Step): An instance of the Step class representing the step details.\n    \"\"\"\n\n    step: Step\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Pass","title":"<code>Pass</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Action that does nothing.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Pass(Thought):\n    \"\"\"\n    Action that does nothing.\n    \"\"\"\n\n    kind: Literal[\"pass\"] = \"pass\"\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Prompt","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class representing a LLM prompt with messages and tools.</p> <p>Attributes:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for the prompt, generated by default.</p> </li> <li> <code>tools</code>               (<code>list[dict]</code>)           \u2013            <p>A list of tools associated with the prompt, default is None.</p> </li> <li> <code>messages</code>               (<code>list[dict]</code>)           \u2013            <p>A list of messages in the prompt, default is an empty list.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>from_user_message</code>             \u2013              <p>Creates a Prompt instance from a user message.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class Prompt(BaseModel):\n    \"\"\"\n    A class representing a LLM prompt with messages and tools.\n\n    Attributes:\n        id (str): A unique identifier for the prompt, generated by default.\n        tools (list[dict], optional): A list of tools associated with the prompt, default is None.\n        messages (list[dict]): A list of messages in the prompt, default is an empty list.\n    \"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    tools: list[dict] | None = None\n    messages: list[dict] = Field(default_factory=list)\n\n    @staticmethod\n    def from_user_message(content: str) -&gt; Prompt:\n        \"\"\"\n        Creates a Prompt instance from a user message.\n\n        Args:\n            content (str): The content of the user message.\n\n        Returns:\n            Prompt: A Prompt instance with the user message.\n        \"\"\"\n        return Prompt(messages=[{\"role\": \"user\", \"content\": content}])\n\n    def __bool__(self) -&gt; bool:\n        return bool(self.messages)\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Prompt.from_user_message","title":"<code>from_user_message(content)</code>  <code>staticmethod</code>","text":"<p>Creates a Prompt instance from a user message.</p> <p>Parameters:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the user message.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Prompt</code> (              <code>Prompt</code> )          \u2013            <p>A Prompt instance with the user message.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>@staticmethod\ndef from_user_message(content: str) -&gt; Prompt:\n    \"\"\"\n    Creates a Prompt instance from a user message.\n\n    Args:\n        content (str): The content of the user message.\n\n    Returns:\n        Prompt: A Prompt instance with the user message.\n    \"\"\"\n    return Prompt(messages=[{\"role\": \"user\", \"content\": content}])\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Respond","title":"<code>Respond</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Action that returns a response to the top-level agent after processing the call.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['respond']</code>)           \u2013            <p>A constant string indicating the type of action.</p> </li> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the response.</p> </li> <li> <code>copy_output</code>               (<code>bool</code>)           \u2013            <p>Indicates whether the last step before this one should be copied to the top-level agent.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class Respond(Thought):\n    \"\"\"\n    Action that returns a response to the top-level agent after processing the call.\n\n    Attributes:\n        kind (Literal[\"respond\"]): A constant string indicating the type of action.\n        content (str): The content of the response.\n        copy_output (bool): Indicates whether the last step before this one should be copied to the top-level agent.\n    \"\"\"\n\n    content: str = \"\"\n    kind: Literal[\"respond\"] = \"respond\"\n    copy_output: bool = False\n</code></pre>"},{"location":"reference/core/#tapeagents.core.SetNextNode","title":"<code>SetNextNode</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Action that sets the next node to run in the current agent.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['set_next_node']</code>)           \u2013            <p>A constant string indicating the type of action.</p> </li> <li> <code>next_node</code>               (<code>str</code>)           \u2013            <p>The name of the next node to run.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class SetNextNode(Thought):\n    \"\"\"\n    Action that sets the next node to run in the current agent.\n\n    Attributes:\n        kind (Literal[\"set_next_node\"]): A constant string indicating the type of action.\n        next_node (str): The name of the next node to run.\n    \"\"\"\n\n    kind: Literal[\"set_next_node\"] = \"set_next_node\"\n    next_node: str\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class representing a step in a tape.</p> <p>Attributes:</p> <ul> <li> <code>metadata</code>               (<code>StepMetadata</code>)           \u2013            <p>Metadata associated with the step.</p> </li> <li> <code>kind</code>               (<code>Literal['define_me']</code>)           \u2013            <p>A placeholder value indicating the kind of step.                          This should be overwritten in subclasses.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_kind</code>             \u2013              <p>Returns the default value of the 'kind' field.</p> </li> <li> <code>llm_dict</code>             \u2013              <p>Dumps the step data as dictionary, excluding the metadata.</p> </li> <li> <code>llm_view</code>             \u2013              <p>Returns a JSON string representation of the step data, excluding the metadata.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class Step(BaseModel):\n    \"\"\"\n    Base class representing a step in a tape.\n\n    Attributes:\n        metadata (StepMetadata): Metadata associated with the step.\n        kind (Literal[\"define_me\"]): A placeholder value indicating the kind of step.\n                                     This should be overwritten in subclasses.\n    \"\"\"\n\n    metadata: StepMetadata = Field(default_factory=StepMetadata)\n    kind: Literal[\"define_me\"] = \"define_me\"  # This is a placeholder value, it should be overwritten in subclasses\n\n    def llm_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the step data as dictionary, excluding the metadata.\"\"\"\n        return self.model_dump(exclude_none=True, exclude={\"metadata\"})\n\n    def llm_view(self, indent: int | None = 2) -&gt; str:\n        \"\"\"Returns a JSON string representation of the step data, excluding the metadata.\"\"\"\n        return json.dumps(self.llm_dict(), indent=indent, ensure_ascii=False)\n\n    @classmethod\n    def get_kind(cls) -&gt; str:\n        \"\"\"Returns the default value of the 'kind' field.\"\"\"\n        return cls.model_fields[\"kind\"].default\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Step.get_kind","title":"<code>get_kind()</code>  <code>classmethod</code>","text":"<p>Returns the default value of the 'kind' field.</p> Source code in <code>tapeagents/core.py</code> <pre><code>@classmethod\ndef get_kind(cls) -&gt; str:\n    \"\"\"Returns the default value of the 'kind' field.\"\"\"\n    return cls.model_fields[\"kind\"].default\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Step.llm_dict","title":"<code>llm_dict()</code>","text":"<p>Dumps the step data as dictionary, excluding the metadata.</p> Source code in <code>tapeagents/core.py</code> <pre><code>def llm_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the step data as dictionary, excluding the metadata.\"\"\"\n    return self.model_dump(exclude_none=True, exclude={\"metadata\"})\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Step.llm_view","title":"<code>llm_view(indent=2)</code>","text":"<p>Returns a JSON string representation of the step data, excluding the metadata.</p> Source code in <code>tapeagents/core.py</code> <pre><code>def llm_view(self, indent: int | None = 2) -&gt; str:\n    \"\"\"Returns a JSON string representation of the step data, excluding the metadata.\"\"\"\n    return json.dumps(self.llm_dict(), indent=indent, ensure_ascii=False)\n</code></pre>"},{"location":"reference/core/#tapeagents.core.StepMetadata","title":"<code>StepMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>StepMetadata is a model that represents metadata for a tape step.</p> <p>Attributes:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for the step, generated using uuid4.</p> </li> <li> <code>prompt_id</code>               (<code>str</code>)           \u2013            <p>An identifier for the llm prompt used to produce the step.</p> </li> <li> <code>node</code>               (<code>str</code>)           \u2013            <p>The node that produced the step.</p> </li> <li> <code>agent</code>               (<code>str</code>)           \u2013            <p>The agent that produced the step.</p> </li> <li> <code>other</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary to store additional metadata related to the step.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class StepMetadata(BaseModel):\n    \"\"\"\n    StepMetadata is a model that represents metadata for a tape step.\n\n    Attributes:\n        id (str): A unique identifier for the step, generated using uuid4.\n        prompt_id (str): An identifier for the llm prompt used to produce the step.\n        node (str): The node that produced the step.\n        agent (str): The agent that produced the step.\n        other (dict[str, Any]): A dictionary to store additional metadata related to the step.\n    \"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    prompt_id: str = \"\"\n    node: str = \"\"\n    agent: str = \"\"\n    other: dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/core/#tapeagents.core.StopStep","title":"<code>StopStep</code>","text":"<p>               Bases: <code>Action</code></p> <p>Action that stops orchestrator loop.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class StopStep(Action):\n    \"\"\"\n    Action that stops orchestrator loop.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Tape","title":"<code>Tape</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ContextType, StepType]</code></p> <p>Tape class represents a sequence of steps produced by agents and environments with associated metadata.</p> <p>Supports iteration, slicing, concatenation, and appending of steps.</p> <p>Attributes:</p> <ul> <li> <code>metadata</code>               (<code>TapeMetadata</code>)           \u2013            <p>Metadata associated with the tape.</p> </li> <li> <code>context</code>               (<code>ContextType</code>)           \u2013            <p>Context information for the tape.</p> </li> <li> <code>steps</code>               (<code>List[StepType]</code>)           \u2013            <p>List of steps in the tape.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__add__</code>             \u2013              <p>Concatenate two tapes or append list of steps to the tape</p> </li> <li> <code>append</code>             \u2013              <p>Adds a step to the tape and creates new metadata (new tape id, etc...).</p> </li> <li> <code>with_new_id</code>             \u2013              <p>Creates a copy of the tape with new metadata.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class Tape(BaseModel, Generic[ContextType, StepType]):\n    \"\"\"\n    Tape class represents a sequence of steps produced by agents and environments with associated metadata.\n\n    Supports iteration, slicing, concatenation, and appending of steps.\n\n    Attributes:\n        metadata (TapeMetadata): Metadata associated with the tape.\n        context (ContextType, optional): Context information for the tape.\n        steps (List[StepType]): List of steps in the tape.\n    \"\"\"\n\n    metadata: TapeMetadata = Field(default_factory=TapeMetadata)\n    context: ContextType | None = None\n    steps: List[StepType] = Field(default_factory=list)\n\n    def __iter__(self) -&gt; Iterator[StepType]:  # type: ignore\n        return iter(self.steps)\n\n    def __len__(self) -&gt; int:\n        return len(self.steps)\n\n    def __getitem__(self, key: int | slice) -&gt; StepType | Self:\n        if isinstance(key, slice):  # cut and erase metadata\n            return self.model_copy(update=dict(steps=self.steps[key.start : key.stop], metadata=TapeMetadata()))\n        return self.steps[key]\n\n    def __add__(self, tape: Self | Iterable[Step]) -&gt; Self:\n        \"\"\"\n        Concatenate two tapes or append list of steps to the tape\n        \"\"\"\n        new_steps = tape.steps if isinstance(tape, Tape) else list(tape)\n        return self.model_copy(\n            update=dict(\n                steps=self.steps + new_steps,\n                metadata=TapeMetadata(n_added_steps=len(new_steps)),\n            )\n        )\n\n    def append(self, step: StepType) -&gt; Self:\n        \"\"\"\n        Adds a step to the tape and creates new metadata (new tape id, etc...).\n        \"\"\"\n        return self.model_copy(update=dict(steps=self.steps + [step], metadata=TapeMetadata()))\n\n    def with_new_id(self) -&gt; Self:\n        \"\"\"\n        Creates a copy of the tape with new metadata.\n        \"\"\"\n        return self.model_copy(update=dict(metadata=TapeMetadata()))\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Tape.__add__","title":"<code>__add__(tape)</code>","text":"<p>Concatenate two tapes or append list of steps to the tape</p> Source code in <code>tapeagents/core.py</code> <pre><code>def __add__(self, tape: Self | Iterable[Step]) -&gt; Self:\n    \"\"\"\n    Concatenate two tapes or append list of steps to the tape\n    \"\"\"\n    new_steps = tape.steps if isinstance(tape, Tape) else list(tape)\n    return self.model_copy(\n        update=dict(\n            steps=self.steps + new_steps,\n            metadata=TapeMetadata(n_added_steps=len(new_steps)),\n        )\n    )\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Tape.append","title":"<code>append(step)</code>","text":"<p>Adds a step to the tape and creates new metadata (new tape id, etc...).</p> Source code in <code>tapeagents/core.py</code> <pre><code>def append(self, step: StepType) -&gt; Self:\n    \"\"\"\n    Adds a step to the tape and creates new metadata (new tape id, etc...).\n    \"\"\"\n    return self.model_copy(update=dict(steps=self.steps + [step], metadata=TapeMetadata()))\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Tape.with_new_id","title":"<code>with_new_id()</code>","text":"<p>Creates a copy of the tape with new metadata.</p> Source code in <code>tapeagents/core.py</code> <pre><code>def with_new_id(self) -&gt; Self:\n    \"\"\"\n    Creates a copy of the tape with new metadata.\n    \"\"\"\n    return self.model_copy(update=dict(metadata=TapeMetadata()))\n</code></pre>"},{"location":"reference/core/#tapeagents.core.TapeMetadata","title":"<code>TapeMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TapeMetadata is a model that represents metadata information for a tape.</p> <p>Attributes:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for the tape, generated by default using uuid4.</p> </li> <li> <code>parent_id</code>               (<code>str</code>)           \u2013            <p>An optional identifier for the parent tape.</p> </li> <li> <code>author</code>               (<code>str</code>)           \u2013            <p>An optional name of the author of the tape.</p> </li> <li> <code>author_tape_id</code>               (<code>str</code>)           \u2013            <p>An optional identifier for the author's tape.</p> </li> <li> <code>n_added_steps</code>               (<code>int</code>)           \u2013            <p>The number of steps added to the tape in the last agent run, default is 0.</p> </li> <li> <code>error</code>               (<code>Any</code>)           \u2013            <p>An optional field to store any errors occured during the last agent run.</p> </li> <li> <code>result</code>               (<code>Any</code>)           \u2013            <p>Optional field to store the result associated with the tape, default is an empty dictionary.</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class TapeMetadata(BaseModel):\n    \"\"\"\n    TapeMetadata is a model that represents metadata information for a tape.\n\n    Attributes:\n        id (str): A unique identifier for the tape, generated by default using uuid4.\n        parent_id (str, optional): An optional identifier for the parent tape.\n        author (str, optional): An optional name of the author of the tape.\n        author_tape_id (str, optional): An optional identifier for the author's tape.\n        n_added_steps (int, optional): The number of steps added to the tape in the last agent run, default is 0.\n        error (Any, optional): An optional field to store any errors occured during the last agent run.\n        result (Any, optional): Optional field to store the result associated with the tape, default is an empty dictionary.\n    \"\"\"\n\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    parent_id: str | None = None\n    author: str | None = None\n    author_tape_id: str | None = None\n    n_added_steps: int = 0\n    error: Any | None = None\n    result: Any = {}\n</code></pre>"},{"location":"reference/core/#tapeagents.core.Thought","title":"<code>Thought</code>","text":"<p>               Bases: <code>AgentStep</code></p> <p>Base class representing an agent's thought in a tape.</p> Source code in <code>tapeagents/core.py</code> <pre><code>class Thought(AgentStep):\n    \"\"\"\n    Base class representing an agent's thought in a tape.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/core/#tapeagents.core.TrainingText","title":"<code>TrainingText</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Training text instance used to finetune a language model.</p> <p>Attributes:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The full text of the training instance.</p> </li> <li> <code>n_predicted</code>               (<code>int</code>)           \u2013            <p>The number of predicted tokens in the text.</p> </li> <li> <code>reward</code>               (<code>float</code>)           \u2013            <p>The reward associated with the training instance. Defaults to 0.0.</p> </li> <li> <code>logprobs</code>               (<code>List[float]</code>)           \u2013            <p>A list of log probabilities of the completion tokens from the assistant model.</p> </li> <li> <code>ref_logprobs</code>               (<code>List[float]</code>)           \u2013            <p>A list of reference log probabilities of the completion tokens from the reference model.</p> </li> <li> <code>group_id</code>               (<code>str</code>)           \u2013            <p>ID of the group. It is used by the RL finetuning script to normalize rewards.</p> </li> <li> <code>prompt_text</code>               (<code>str</code>)           \u2013            <p>Portion of the text that serves as the prompt (i.e., the text excluding the predicted tokens).</p> </li> <li> <code>output_text</code>               (<code>str</code>)           \u2013            <p>Portion of the text that represents the predicted output (i.e., the last n_predicted tokens).</p> </li> </ul> Source code in <code>tapeagents/core.py</code> <pre><code>class TrainingText(BaseModel):\n    \"\"\"\n    Training text instance used to finetune a language model.\n\n    Attributes:\n        text (str): The full text of the training instance.\n        n_predicted (int): The number of predicted tokens in the text.\n        reward (float): The reward associated with the training instance. Defaults to 0.0.\n        logprobs (List[float]): A list of log probabilities of the completion tokens from the assistant model.\n        ref_logprobs (List[float]): A list of reference log probabilities of the completion tokens from the reference model.\n        group_id (str, optional): ID of the group. It is used by the RL finetuning script to normalize rewards.\n        prompt_text (str): Portion of the text that serves as the prompt (i.e., the text excluding the predicted tokens).\n        output_text (str): Portion of the text that represents the predicted output (i.e., the last n_predicted tokens).\n    \"\"\"\n\n    text: str\n    n_predicted: int\n    reward: float = 0.0\n    logprobs: List[float] = Field(default_factory=list)\n    ref_logprobs: List[float] = Field(default_factory=list)\n    group_id: str | None = None\n\n    @property\n    def prompt_text(self) -&gt; str:\n        return self.text[: -self.n_predicted]\n\n    @property\n    def output_text(self) -&gt; str:\n        return self.text[-self.n_predicted :]\n</code></pre>"},{"location":"reference/demo/","title":"Demo","text":""},{"location":"reference/dialog_tape/","title":"Dialog Tape","text":"<p>Types and classes for dialog tapes and annotators.</p> <p>Classes:</p> <ul> <li> <code>AnnotationAction</code>           \u2013            <p>AnnotationAction is a subclass of Action that represents an action produced by an annotator.</p> </li> <li> <code>AnnotatorFreeFormThought</code>           \u2013            <p>AnnotatorFreeFormThought is a subclass of Thought that represents a free-form thought provided by an annotator.</p> </li> <li> <code>AssistantStep</code>           \u2013            <p>Represents a step taken by an assistant in a dialog.</p> </li> <li> <code>AssistantThought</code>           \u2013            <p>Represents a thought generated by an assistant.</p> </li> <li> <code>DialogAnnotator</code>           \u2013            <p>DialogAnnotator is a class that extends the Annotator agent with specific types DialogTape and DialogAnnotatorTape.</p> </li> <li> <code>DialogContext</code>           \u2013            <p>Context for dialog agents, containing tools and other information.</p> </li> <li> <code>FunctionCall</code>           \u2013            <p>A class representing a function call.</p> </li> <li> <code>FunctionSpec</code>           \u2013            <p>A class representing the specification of a function.</p> </li> <li> <code>SystemStep</code>           \u2013            <p>Step rendered into system message of the prompt.</p> </li> <li> <code>ToolCall</code>           \u2013            <p>ToolCall is a model representing a tool call with a specific function, id, and type.</p> </li> <li> <code>ToolCalls</code>           \u2013            <p>Action that wraps one-or-many tool calls.</p> </li> <li> <code>ToolResult</code>           \u2013            <p>ToolResult is a subclass of Observation that represents the result of a tool call.</p> </li> <li> <code>ToolSpec</code>           \u2013            <p>ToolSpec is a model that represents a tool specification with a type and a function.</p> </li> <li> <code>UserStep</code>           \u2013            <p>Represents a step taken by a user in a dialog.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>DialogAnnotatorTape</code>               (<code>TypeAlias</code>)           \u2013            <p>Type alias for dialog annotator tapes.</p> </li> <li> <code>DialogEvent</code>               (<code>TypeAlias</code>)           \u2013            <p>Type alias for dialog events.</p> </li> <li> <code>DialogStep</code>               (<code>TypeAlias</code>)           \u2013            <p>Type alias for dialog steps.</p> </li> <li> <code>DialogTape</code>           \u2013            <p>Type alias for dialog tapes.</p> </li> </ul>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogAnnotatorTape","title":"<code>DialogAnnotatorTape: TypeAlias = Tape[DialogTape, AnnotatorFreeFormThought | AnnotationAction]</code>  <code>module-attribute</code>","text":"<p>Type alias for dialog annotator tapes.</p>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogEvent","title":"<code>DialogEvent: TypeAlias = AgentEvent[DialogTape]</code>  <code>module-attribute</code>","text":"<p>Type alias for dialog events.</p>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogStep","title":"<code>DialogStep: TypeAlias = UserStep | ToolResult | SystemStep | AssistantThought | SetNextNode | Pass | Call | Respond | FinalStep | AssistantStep | ToolCalls</code>  <code>module-attribute</code>","text":"<p>Type alias for dialog steps.</p>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogTape","title":"<code>DialogTape = Tape[DialogContext | None, DialogStep]</code>  <code>module-attribute</code>","text":"<p>Type alias for dialog tapes.</p>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.AnnotationAction","title":"<code>AnnotationAction</code>","text":"<p>               Bases: <code>Action</code></p> <p>AnnotationAction is a subclass of Action that represents an action produced by an annotator.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['annotation_action']</code>)           \u2013            <p>A string literal indicating the type of action.</p> </li> <li> <code>annotation</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing annotation data.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class AnnotationAction(Action):\n    \"\"\"\n    AnnotationAction is a subclass of Action that represents an action produced by an annotator.\n\n    Attributes:\n        kind (Literal[\"annotation_action\"]): A string literal indicating the type of action.\n        annotation (dict): A dictionary containing annotation data.\n    \"\"\"\n\n    kind: Literal[\"annotation_action\"] = \"annotation_action\"\n    annotation: dict\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.AnnotatorFreeFormThought","title":"<code>AnnotatorFreeFormThought</code>","text":"<p>               Bases: <code>Thought</code></p> <p>AnnotatorFreeFormThought is a subclass of Thought that represents a free-form thought provided by an annotator.</p> <p>Attributes:</p> <ul> <li> <code>kind</code>               (<code>Literal['annotator_free_form_thought']</code>)           \u2013            <p>A constant string that identifies the type of thought.</p> </li> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the free-form thought provided by the annotator.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class AnnotatorFreeFormThought(Thought):\n    \"\"\"\n    AnnotatorFreeFormThought is a subclass of Thought that represents a free-form thought provided by an annotator.\n\n    Attributes:\n        kind (Literal[\"annotator_free_form_thought\"]): A constant string that identifies the type of thought.\n        content (str): The content of the free-form thought provided by the annotator.\n    \"\"\"\n\n    kind: Literal[\"annotator_free_form_thought\"] = \"annotator_free_form_thought\"\n    content: str\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.AssistantStep","title":"<code>AssistantStep</code>","text":"<p>               Bases: <code>Action</code></p> <p>Represents a step taken by an assistant in a dialog.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the assistant's response.</p> </li> <li> <code>kind</code>               (<code>Literal['assistant']</code>)           \u2013            <p>The type of step, which is always \"assistant\".</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class AssistantStep(Action):\n    \"\"\"\n    Represents a step taken by an assistant in a dialog.\n\n    Attributes:\n        content (str): The content of the assistant's response.\n        kind (Literal[\"assistant\"]): The type of step, which is always \"assistant\".\n    \"\"\"\n\n    content: str\n    kind: Literal[\"assistant\"] = \"assistant\"\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.AssistantThought","title":"<code>AssistantThought</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Represents a thought generated by an assistant.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>Any</code>)           \u2013            <p>The content of the assistant's thought.</p> </li> <li> <code>kind</code>               (<code>Literal['assistant_thought']</code>)           \u2013            <p>A literal string indicating the type of thought.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class AssistantThought(Thought):\n    \"\"\"\n    Represents a thought generated by an assistant.\n\n    Attributes:\n        content (Any): The content of the assistant's thought.\n        kind (Literal[\"assistant_thought\"]): A literal string indicating the type of thought.\n    \"\"\"\n\n    content: Any\n    kind: Literal[\"assistant_thought\"] = \"assistant_thought\"\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogAnnotator","title":"<code>DialogAnnotator</code>","text":"<p>               Bases: <code>Annotator[DialogTape, DialogAnnotatorTape]</code></p> <p>DialogAnnotator is a class that extends the Annotator agent with specific types DialogTape and DialogAnnotatorTape.</p> <p>Methods:</p> <ul> <li> <code>make_own_tape</code>             \u2013              <p>DialogTape) -&gt; DialogAnnotatorTape: Creates and returns a DialogAnnotatorTape instance using the provided DialogTape instance.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>make_own_tape</code>             \u2013              <p>Creates a DialogAnnotatorTape instance using given DialogTape as context.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class DialogAnnotator(Annotator[DialogTape, DialogAnnotatorTape]):\n    \"\"\"\n    DialogAnnotator is a class that extends the Annotator agent with specific types DialogTape and DialogAnnotatorTape.\n\n    Methods:\n        make_own_tape(tape: DialogTape) -&gt; DialogAnnotatorTape:\n            Creates and returns a DialogAnnotatorTape instance using the provided DialogTape instance.\n    \"\"\"\n\n    def make_own_tape(self, tape: DialogTape) -&gt; DialogAnnotatorTape:\n        \"\"\"\n        Creates a DialogAnnotatorTape instance using given DialogTape as context.\n\n        Args:\n            tape (DialogTape): The DialogTape instance to be converted.\n\n        Returns:\n            DialogAnnotatorTape: A new instance of DialogAnnotatorTape with the provided context.\n        \"\"\"\n        return DialogAnnotatorTape(context=tape)\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogAnnotator.make_own_tape","title":"<code>make_own_tape(tape)</code>","text":"<p>Creates a DialogAnnotatorTape instance using given DialogTape as context.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>DialogTape</code>)           \u2013            <p>The DialogTape instance to be converted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DialogAnnotatorTape</code> (              <code>DialogAnnotatorTape</code> )          \u2013            <p>A new instance of DialogAnnotatorTape with the provided context.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>def make_own_tape(self, tape: DialogTape) -&gt; DialogAnnotatorTape:\n    \"\"\"\n    Creates a DialogAnnotatorTape instance using given DialogTape as context.\n\n    Args:\n        tape (DialogTape): The DialogTape instance to be converted.\n\n    Returns:\n        DialogAnnotatorTape: A new instance of DialogAnnotatorTape with the provided context.\n    \"\"\"\n    return DialogAnnotatorTape(context=tape)\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.DialogContext","title":"<code>DialogContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Context for dialog agents, containing tools and other information.</p> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class DialogContext(BaseModel):\n    \"\"\"\n    Context for dialog agents, containing tools and other information.\n    \"\"\"\n\n    # TODO: define type signature for tools including JSONSchema and etc\n    tools: list[ToolSpec]\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.FunctionCall","title":"<code>FunctionCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class representing a function call.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the function being called.</p> </li> <li> <code>arguments</code>               (<code>Any</code>)           \u2013            <p>The arguments to be passed to the function.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class FunctionCall(BaseModel):\n    \"\"\"\n    A class representing a function call.\n\n    Attributes:\n        name (str): The name of the function being called.\n        arguments (Any): The arguments to be passed to the function.\n    \"\"\"\n\n    name: str\n    arguments: Any\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.FunctionSpec","title":"<code>FunctionSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class representing the specification of a function.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the function.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>A brief description of the function.</p> </li> <li> <code>parameters</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the parameters of the function.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class FunctionSpec(BaseModel):\n    \"\"\"\n    A class representing the specification of a function.\n\n    Attributes:\n        name (str): The name of the function.\n        description (str): A brief description of the function.\n        parameters (dict): A dictionary containing the parameters of the function.\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: dict\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.SystemStep","title":"<code>SystemStep</code>","text":"<p>               Bases: <code>Observation</code></p> <p>Step rendered into system message of the prompt.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the system step.</p> </li> <li> <code>kind</code>               (<code>Literal['system']</code>)           \u2013            <p>A literal indicating the type of step, which is always \"system\".</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class SystemStep(Observation):\n    \"\"\"\n    Step rendered into system message of the prompt.\n\n    Attributes:\n        content (str): The content of the system step.\n        kind (Literal[\"system\"]): A literal indicating the type of step, which is always \"system\".\n    \"\"\"\n\n    content: str\n    kind: Literal[\"system\"] = \"system\"\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>ToolCall is a model representing a tool call with a specific function, id, and type.</p> <p>Attributes:</p> <ul> <li> <code>function</code>               (<code>FunctionCall</code>)           \u2013            <p>The function call associated with the tool.</p> </li> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The identifier for the tool call. Defaults to an empty string.</p> </li> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>The type of the tool call. Defaults to \"function\".</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"\n    ToolCall is a model representing a tool call with a specific function, id, and type.\n\n    Attributes:\n        function (FunctionCall): The function call associated with the tool.\n        id (str): The identifier for the tool call. Defaults to an empty string.\n        type (str): The type of the tool call. Defaults to \"function\".\n    \"\"\"\n\n    function: FunctionCall\n    id: str = \"\"\n    type: str = \"function\"\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolCalls","title":"<code>ToolCalls</code>","text":"<p>               Bases: <code>Action</code></p> <p>Action that wraps one-or-many tool calls.</p> <p>We structure this class similar to OpenAI tool calls, but we let function arguments be Any, not just str (see <code>FunctionCall</code> class)</p> <p>Attributes:</p> <ul> <li> <code>tool_calls</code>               (<code>list[ToolCall]</code>)           \u2013            <p>The list of tool calls to be made.</p> </li> <li> <code>kind</code>               (<code>Literal['assistant']</code>)           \u2013            <p>The type of step, which is always \"assistant\".</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>from_dicts</code>             \u2013              <p>Create a ToolCalls instance from a list of dictionaries.</p> </li> <li> <code>from_llm_output</code>             \u2013              <p>Converts an LLMOutput object to a ToolCalls object.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class ToolCalls(Action):\n    \"\"\"Action that wraps one-or-many tool calls.\n\n    We structure this class similar to OpenAI tool calls, but we let function arguments be Any, not just str\n    (see `FunctionCall` class)\n\n    Attributes:\n        tool_calls (list[ToolCall]): The list of tool calls to be made.\n        kind (Literal[\"assistant\"]): The type of step, which is always \"assistant\".\n    \"\"\"\n\n    tool_calls: list[ToolCall]\n    kind: Literal[\"assistant\"] = \"assistant\"\n\n    @staticmethod\n    def from_dicts(dicts: list):\n        \"\"\"\n        Create a ToolCalls instance from a list of dictionaries.\n\n        Args:\n            dicts (list): A list of dictionaries where each dictionary represents a tool call.\n\n        Returns:\n            (ToolCalls): An instance of ToolCalls created from the provided list of dictionaries.\n        \"\"\"\n        return ToolCalls.model_validate({\"tool_calls\": dicts})\n\n    @staticmethod\n    def from_llm_output(llm_output: LLMOutput) -&gt; ToolCalls:\n        \"\"\"\n        Converts an LLMOutput object to a ToolCalls object.\n\n        Args:\n            llm_output (LLMOutput): The output from the language model, which contains tool calls.\n\n        Returns:\n            ToolCalls: An object containing a list of ToolCall objects.\n\n        Raises:\n            ValueError: If the llm_output does not contain any tool calls.\n        \"\"\"\n        if not llm_output.tool_calls:\n            raise ValueError()\n        tool_calls = [\n            ToolCall(\n                function=FunctionCall(name=tc.function.name, arguments=tc.function.arguments),\n                id=tc.id,\n            )\n            for tc in llm_output.tool_calls\n        ]\n        return ToolCalls(tool_calls=tool_calls)\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolCalls.from_dicts","title":"<code>from_dicts(dicts)</code>  <code>staticmethod</code>","text":"<p>Create a ToolCalls instance from a list of dictionaries.</p> <p>Parameters:</p> <ul> <li> <code>dicts</code>               (<code>list</code>)           \u2013            <p>A list of dictionaries where each dictionary represents a tool call.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ToolCalls</code>           \u2013            <p>An instance of ToolCalls created from the provided list of dictionaries.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>@staticmethod\ndef from_dicts(dicts: list):\n    \"\"\"\n    Create a ToolCalls instance from a list of dictionaries.\n\n    Args:\n        dicts (list): A list of dictionaries where each dictionary represents a tool call.\n\n    Returns:\n        (ToolCalls): An instance of ToolCalls created from the provided list of dictionaries.\n    \"\"\"\n    return ToolCalls.model_validate({\"tool_calls\": dicts})\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolCalls.from_llm_output","title":"<code>from_llm_output(llm_output)</code>  <code>staticmethod</code>","text":"<p>Converts an LLMOutput object to a ToolCalls object.</p> <p>Parameters:</p> <ul> <li> <code>llm_output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output from the language model, which contains tool calls.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ToolCalls</code> (              <code>ToolCalls</code> )          \u2013            <p>An object containing a list of ToolCall objects.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the llm_output does not contain any tool calls.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>@staticmethod\ndef from_llm_output(llm_output: LLMOutput) -&gt; ToolCalls:\n    \"\"\"\n    Converts an LLMOutput object to a ToolCalls object.\n\n    Args:\n        llm_output (LLMOutput): The output from the language model, which contains tool calls.\n\n    Returns:\n        ToolCalls: An object containing a list of ToolCall objects.\n\n    Raises:\n        ValueError: If the llm_output does not contain any tool calls.\n    \"\"\"\n    if not llm_output.tool_calls:\n        raise ValueError()\n    tool_calls = [\n        ToolCall(\n            function=FunctionCall(name=tc.function.name, arguments=tc.function.arguments),\n            id=tc.id,\n        )\n        for tc in llm_output.tool_calls\n    ]\n    return ToolCalls(tool_calls=tool_calls)\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolResult","title":"<code>ToolResult</code>","text":"<p>               Bases: <code>Observation</code></p> <p>ToolResult is a subclass of Observation that represents the result of a tool call.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>Any</code>)           \u2013            <p>The content of the tool result.</p> </li> <li> <code>tool_call_id</code>               (<code>str</code>)           \u2013            <p>The unique identifier for the tool call. Defaults to an empty string.</p> </li> <li> <code>kind</code>               (<code>Literal['tool']</code>)           \u2013            <p>The kind of result, which is always \"tool\". Defaults to \"tool\".</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class ToolResult(Observation):\n    \"\"\"\n    ToolResult is a subclass of Observation that represents the result of a tool call.\n\n    Attributes:\n        content (Any): The content of the tool result.\n        tool_call_id (str): The unique identifier for the tool call. Defaults to an empty string.\n        kind (Literal[\"tool\"]): The kind of result, which is always \"tool\". Defaults to \"tool\".\n    \"\"\"\n\n    content: Any\n    tool_call_id: str = \"\"\n    kind: Literal[\"tool\"] = \"tool\"\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolSpec","title":"<code>ToolSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>ToolSpec is a model that represents a tool specification with a type and a function.</p> <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>Literal['function']</code>)           \u2013            <p>The type of the tool, which is always \"function\".</p> </li> <li> <code>function</code>               (<code>FunctionSpec</code>)           \u2013            <p>The specification of the function.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>from_function</code>             \u2013              <p>Creates an instance of the class by validating the model from a given function.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class ToolSpec(BaseModel):\n    \"\"\"\n    ToolSpec is a model that represents a tool specification with a type and a function.\n\n    Attributes:\n        type (Literal[\"function\"]): The type of the tool, which is always \"function\".\n        function (FunctionSpec): The specification of the function.\n    \"\"\"\n\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionSpec\n\n    @classmethod\n    def from_function(cls, function: Callable):\n        \"\"\"\n        Creates an instance of the class by validating the model from a given function.\n\n        Args:\n            function (Callable): The function to be converted and validated.\n\n        Returns:\n            (ToolSpec): An instance of the class with the validated model.\n        \"\"\"\n        return cls.model_validate(convert_to_openai_tool(function))\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.ToolSpec.from_function","title":"<code>from_function(function)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the class by validating the model from a given function.</p> <p>Parameters:</p> <ul> <li> <code>function</code>               (<code>Callable</code>)           \u2013            <p>The function to be converted and validated.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ToolSpec</code>           \u2013            <p>An instance of the class with the validated model.</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>@classmethod\ndef from_function(cls, function: Callable):\n    \"\"\"\n    Creates an instance of the class by validating the model from a given function.\n\n    Args:\n        function (Callable): The function to be converted and validated.\n\n    Returns:\n        (ToolSpec): An instance of the class with the validated model.\n    \"\"\"\n    return cls.model_validate(convert_to_openai_tool(function))\n</code></pre>"},{"location":"reference/dialog_tape/#tapeagents.dialog_tape.UserStep","title":"<code>UserStep</code>","text":"<p>               Bases: <code>Observation</code></p> <p>Represents a step taken by a user in a dialog.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the user's step.</p> </li> <li> <code>kind</code>               (<code>Literal['user']</code>)           \u2013            <p>The type of step, which is always \"user\".</p> </li> </ul> Source code in <code>tapeagents/dialog_tape.py</code> <pre><code>class UserStep(Observation):\n    \"\"\"\n    Represents a step taken by a user in a dialog.\n\n    Attributes:\n        content (str): The content of the user's step.\n        kind (Literal[\"user\"]): The type of step, which is always \"user\".\n    \"\"\"\n\n    content: str\n    kind: Literal[\"user\"] = \"user\"\n</code></pre>"},{"location":"reference/environment/","title":"Environment","text":"<p>Base classes for environments that execute actions and produce observations</p> <p>Classes:</p> <ul> <li> <code>CodeExecutionEnvironment</code>           \u2013            <p>Environment for the team agents</p> </li> <li> <code>ExternalObservationNeeded</code>           \u2013            <p>Environments raise this when they can't make the required observation for an action</p> </li> <li> <code>NoActionsToReactTo</code>           \u2013            <p>Environments raise this when there are no actions to react to</p> </li> </ul>"},{"location":"reference/environment/#tapeagents.environment.CodeExecutionEnvironment","title":"<code>CodeExecutionEnvironment</code>","text":"<p>               Bases: <code>Environment</code></p> <p>Environment for the team agents The only action that the environment can perform is to execute the code blocks</p> Source code in <code>tapeagents/environment.py</code> <pre><code>class CodeExecutionEnvironment(Environment):\n    \"\"\"\n    Environment for the team agents\n    The only action that the environment can perform is to execute the code blocks\n    \"\"\"\n\n    def __init__(self, container_executor: ContainerExecutor):\n        self.container_executor = container_executor\n\n    def react(self, tape: Tape) -&gt; Tape:\n        match step := tape.steps[-1]:\n            case ExecuteCode():\n                result = self.container_executor.execute_code_blocks(step.code)\n                return tape.append(CodeExecutionResult(result=result))\n            case _:\n                return tape\n</code></pre>"},{"location":"reference/environment/#tapeagents.environment.ExternalObservationNeeded","title":"<code>ExternalObservationNeeded</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Environments raise this when they can't make the required observation for an action</p> Source code in <code>tapeagents/environment.py</code> <pre><code>class ExternalObservationNeeded(Exception):\n    \"\"\"Environments raise this when they can't make the required observation for an action\"\"\"\n\n    def __init__(self, action: Action, *args, **wargs):\n        super().__init__(*args, **wargs)\n        self.action = action\n\n    def __str__(self) -&gt; str:\n        return f\"Environment needs external observation for action {self.action}\"\n</code></pre>"},{"location":"reference/environment/#tapeagents.environment.NoActionsToReactTo","title":"<code>NoActionsToReactTo</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Environments raise this when there are no actions to react to</p> Source code in <code>tapeagents/environment.py</code> <pre><code>class NoActionsToReactTo(Exception):\n    \"\"\"Environments raise this when there are no actions to react to\"\"\"\n\n    def __init__(self, *args, **wargs):\n        super().__init__(*args, **wargs)\n</code></pre>"},{"location":"reference/io/","title":"IO","text":"<p>I/O routines for Tapes.</p> <p>Classes:</p> <ul> <li> <code>TapeSaver</code>           \u2013            <p>A class for saving Tape objects using YAML format.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>load_tapes</code>             \u2013              <p>Load tapes from dir with YAML or JSON files.</p> </li> <li> <code>save_json_tape</code>             \u2013              <p>Save a Tape object to a JSON file.</p> </li> <li> <code>stream_yaml_tapes</code>             \u2013              <p>Stream YAML tapes to a file.</p> </li> </ul>"},{"location":"reference/io/#tapeagents.io.TapeSaver","title":"<code>TapeSaver</code>","text":"<p>A class for saving Tape objects using YAML format.</p> <p>This class provides functionality to save Tape objects using a YAML dumper. It handles the serialization of Tape objects into YAML format.</p> Example <pre><code>dumper = yaml.SafeDumper(output_file)\nsaver = TapeSaver(dumper)\nsaver.save(tape)\n</code></pre> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize TapeIOYML with a YAML dumper.</p> </li> <li> <code>save</code>             \u2013              <p>Saves the tape data using the configured dumper.</p> </li> </ul> Source code in <code>tapeagents/io.py</code> <pre><code>class TapeSaver:\n    \"\"\"A class for saving Tape objects using YAML format.\n\n    This class provides functionality to save Tape objects using a YAML dumper.\n    It handles the serialization of Tape objects into YAML format.\n\n    Example:\n        ```python\n        dumper = yaml.SafeDumper(output_file)\n        saver = TapeSaver(dumper)\n        saver.save(tape)\n        ```\n    \"\"\"\n\n    def __init__(self, yaml_dumper: yaml.SafeDumper):\n        \"\"\"Initialize TapeIOYML with a YAML dumper.\n\n        Args:\n            yaml_dumper (yaml.SafeDumper): The YAML dumper instance to use for serialization.\n        \"\"\"\n        self._dumper = yaml_dumper\n\n    def save(self, tape: Tape):\n        \"\"\"\n        Saves the tape data using the configured dumper.\n\n        Args:\n            tape (Tape): The tape object containing the data to be saved.\n        \"\"\"\n        self._dumper.represent(tape.model_dump(by_alias=True))\n</code></pre>"},{"location":"reference/io/#tapeagents.io.TapeSaver.__init__","title":"<code>__init__(yaml_dumper)</code>","text":"<p>Initialize TapeIOYML with a YAML dumper.</p> <p>Parameters:</p> <ul> <li> <code>yaml_dumper</code>               (<code>SafeDumper</code>)           \u2013            <p>The YAML dumper instance to use for serialization.</p> </li> </ul> Source code in <code>tapeagents/io.py</code> <pre><code>def __init__(self, yaml_dumper: yaml.SafeDumper):\n    \"\"\"Initialize TapeIOYML with a YAML dumper.\n\n    Args:\n        yaml_dumper (yaml.SafeDumper): The YAML dumper instance to use for serialization.\n    \"\"\"\n    self._dumper = yaml_dumper\n</code></pre>"},{"location":"reference/io/#tapeagents.io.TapeSaver.save","title":"<code>save(tape)</code>","text":"<p>Saves the tape data using the configured dumper.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the data to be saved.</p> </li> </ul> Source code in <code>tapeagents/io.py</code> <pre><code>def save(self, tape: Tape):\n    \"\"\"\n    Saves the tape data using the configured dumper.\n\n    Args:\n        tape (Tape): The tape object containing the data to be saved.\n    \"\"\"\n    self._dumper.represent(tape.model_dump(by_alias=True))\n</code></pre>"},{"location":"reference/io/#tapeagents.io.load_tapes","title":"<code>load_tapes(tape_class, path, file_extension='.yaml')</code>","text":"<p>Load tapes from dir with YAML or JSON files.</p> <p>This function loads tapes from a file or directory and converts them into tape objects using the specified tape class or type adapter.</p> <p>Parameters:</p> <ul> <li> <code>tape_class</code>               (<code>Union[Type, TypeAdapter]</code>)           \u2013            <p>The class or type adapter used to validate and create tape objects.</p> </li> <li> <code>path</code>               (<code>Union[Path, str]</code>)           \u2013            <p>Path to a file or directory containing tape configurations.</p> </li> <li> <code>file_extension</code>               (<code>str</code>, default:                   <code>'.yaml'</code> )           \u2013            <p>File extension to filter by when loading from directory. Must be either '.yaml' or '.json'. Defaults to '.yaml'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Tape]</code>           \u2013            <p>list[Tape]: A list of validated tape objects.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the specified path does not exist.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If an unsupported file extension is provided.</p> </li> </ul> Example <pre><code>tapes = load_tapes(TapeClass, \"configs/tapes.yaml\")\ntapes = load_tapes(tape_adapter, \"configs/tapes\", \".json\")\n</code></pre> Source code in <code>tapeagents/io.py</code> <pre><code>def load_tapes(tape_class: Type | TypeAdapter, path: Path | str, file_extension: str = \".yaml\") -&gt; list[Tape]:\n    \"\"\"Load tapes from dir with YAML or JSON files.\n\n    This function loads tapes from a file or directory and converts them into tape objects\n    using the specified tape class or type adapter.\n\n    Args:\n        tape_class (Union[Type, TypeAdapter]): The class or type adapter used to validate and create tape objects.\n        path (Union[Path, str]): Path to a file or directory containing tape configurations.\n        file_extension (str, optional): File extension to filter by when loading from directory.\n            Must be either '.yaml' or '.json'. Defaults to '.yaml'.\n\n    Returns:\n        list[Tape]: A list of validated tape objects.\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n        ValueError: If an unsupported file extension is provided.\n\n    Example:\n        ```python\n        tapes = load_tapes(TapeClass, \"configs/tapes.yaml\")\n        tapes = load_tapes(tape_adapter, \"configs/tapes\", \".json\")\n        ```\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n    if file_extension not in (\".yaml\", \".json\"):\n        raise ValueError(f\"Unsupported file extension: {file_extension}\")\n    if os.path.isdir(path):\n        logger.info(f\"Loading tapes from dir {path}\")\n        paths = sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(file_extension)])\n    else:\n        paths = [path]\n        file_extension = os.path.splitext(path)[-1]\n    loader = tape_class.model_validate if isinstance(tape_class, Type) else tape_class.validate_python\n    tapes = []\n    for path in paths:\n        with open(path) as f:\n            if file_extension == \".yaml\":\n                data = list(yaml.safe_load_all(f))\n            else:\n                data = json.load(f)\n        if not isinstance(data, list):\n            data = [data]\n        for tape in data:\n            tapes.append(loader(tape))\n    return tapes\n</code></pre>"},{"location":"reference/io/#tapeagents.io.save_json_tape","title":"<code>save_json_tape(tape, tapes_dir, name='')</code>","text":"<p>Save a Tape object to a JSON file.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The Tape object to be saved</p> </li> <li> <code>tapes_dir</code>               (<code>str</code>)           \u2013            <p>Directory path where the JSON file will be saved</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Name of the output JSON file. If empty, tapes_dir is used as the full path. If provided without .json extension, it will be added automatically. Defaults to \"\".</p> </li> </ul> Example <pre><code>tape = Tape(...)\nsave_json_tape(tape, \"/path/to/dir\", \"my_tape\")\n# Saves to /path/to/dir/my_tape.json\n</code></pre> Source code in <code>tapeagents/io.py</code> <pre><code>def save_json_tape(tape: Tape, tapes_dir: str, name: str = \"\"):\n    \"\"\"Save a Tape object to a JSON file.\n\n    Args:\n        tape (Tape): The Tape object to be saved\n        tapes_dir (str): Directory path where the JSON file will be saved\n        name (str, optional): Name of the output JSON file. If empty, tapes_dir is used as the full path.\n            If provided without .json extension, it will be added automatically. Defaults to \"\".\n\n    Example:\n        ```python\n        tape = Tape(...)\n        save_json_tape(tape, \"/path/to/dir\", \"my_tape\")\n        # Saves to /path/to/dir/my_tape.json\n        ```\n\n    \"\"\"\n    fname = name if name.endswith(\".json\") else f\"{name}.json\"\n    fpath = os.path.join(tapes_dir, fname) if name else tapes_dir\n    with open(fpath, \"w\") as f:\n        f.write(tape.model_dump_json(indent=4))\n</code></pre>"},{"location":"reference/io/#tapeagents.io.stream_yaml_tapes","title":"<code>stream_yaml_tapes(filename, mode='w')</code>","text":"<p>Stream YAML tapes to a file.</p> <p>This function creates a context manager that allows streaming YAML documents to a file. It handles file creation, directory creation if necessary, and proper resource cleanup.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Union[Path, str]</code>)           \u2013            <p>Path to the output YAML file. Can be either a string or Path object.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'w'</code> )           \u2013            <p>File opening mode. Defaults to \"w\" (write mode).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>TapeSaver</code>           \u2013            <p>Generator[TapeSaver, None, None]: A TapeSaver instance that can be used to write YAML documents.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If there are issues with file/directory creation or permissions.</p> </li> <li> <code>YAMLError</code>             \u2013            <p>If there are YAML serialization errors.</p> </li> </ul> Source code in <code>tapeagents/io.py</code> <pre><code>@contextmanager\ndef stream_yaml_tapes(filename: Path | str, mode: str = \"w\") -&gt; Generator[TapeSaver, None, None]:\n    \"\"\"Stream YAML tapes to a file.\n\n    This function creates a context manager that allows streaming YAML documents to a file.\n    It handles file creation, directory creation if necessary, and proper resource cleanup.\n\n    Args:\n        filename (Union[Path, str]): Path to the output YAML file. Can be either a string or Path object.\n        mode (str, optional): File opening mode. Defaults to \"w\" (write mode).\n\n    Yields:\n        Generator[TapeSaver, None, None]: A TapeSaver instance that can be used to write YAML documents.\n\n    Raises:\n        OSError: If there are issues with file/directory creation or permissions.\n        yaml.YAMLError: If there are YAML serialization errors.\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n    logger.info(f\"Writing to {filename} in mode {mode}\")\n\n    # Create directory path if it does not exist\n    filename.parent.mkdir(parents=True, exist_ok=True)\n\n    # Open file for writing and create dumper instance\n    _file = open(filename, mode)\n    _dumper = yaml.SafeDumper(\n        stream=_file,\n        default_flow_style=False,\n        explicit_start=True,\n        sort_keys=False,\n    )\n    _dumper.open()\n\n    # Yield the dumper to the caller\n    yield TapeSaver(_dumper)\n\n    # Close the dumper and file\n    _dumper.close()\n    _file.close()\n</code></pre>"},{"location":"reference/llm_function/","title":"LLM Function","text":"<p>Optimizable llm functions.</p> <p>Classes:</p> <ul> <li> <code>AssistantOutput</code>           \u2013            <p>Describes an output of an LLM-based function.</p> </li> <li> <code>Input</code>           \u2013            <p>Describes an input to an LLM-based function.</p> </li> <li> <code>KindRef</code>           \u2013            <p>Refer to the input by the step kind. Refers the last step with the given kind.</p> </li> <li> <code>Output</code>           \u2013            <p>Describes an output of an LLM-based function.</p> </li> <li> <code>Variable</code>           \u2013            <p>Base class for all LLM function inputs and outputs</p> </li> </ul>"},{"location":"reference/llm_function/#tapeagents.llm_function.AssistantOutput","title":"<code>AssistantOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>Describes an output of an LLM-based function.</p> Source code in <code>tapeagents/llm_function.py</code> <pre><code>class AssistantOutput(Output):\n    \"\"\"Describes an output of an LLM-based function.\"\"\"\n\n    def parse(self, text: str) -&gt; Step:\n        return AssistantStep(content=text)\n</code></pre>"},{"location":"reference/llm_function/#tapeagents.llm_function.Input","title":"<code>Input</code>","text":"<p>               Bases: <code>Variable</code></p> <p>Describes an input to an LLM-based function.</p> Source code in <code>tapeagents/llm_function.py</code> <pre><code>class Input(Variable):\n    \"\"\"Describes an input to an LLM-based function.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/llm_function/#tapeagents.llm_function.KindRef","title":"<code>KindRef</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Refer to the input by the step kind. Refers the last step with the given kind.</p> Source code in <code>tapeagents/llm_function.py</code> <pre><code>class KindRef(BaseModel):\n    \"\"\"Refer to the input by the step kind. Refers the last step with the given kind.\"\"\"\n\n    kind: str\n</code></pre>"},{"location":"reference/llm_function/#tapeagents.llm_function.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>Variable</code></p> <p>Describes an output of an LLM-based function.</p> Source code in <code>tapeagents/llm_function.py</code> <pre><code>class Output(Variable):\n    \"\"\"Describes an output of an LLM-based function.\"\"\"\n\n    def parse(self, text: str) -&gt; Step:\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llm_function/#tapeagents.llm_function.Variable","title":"<code>Variable</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all LLM function inputs and outputs</p> Source code in <code>tapeagents/llm_function.py</code> <pre><code>class Variable(BaseModel):\n    \"\"\"Base class for all LLM function inputs and outputs\"\"\"\n\n    name: str\n    prefix: str = \"\"\n    desc: str = \"\"\n    separator: str = \" \"\n\n    def get_prefix(self):\n        return self.prefix or f\"{self.name.title()}:\"\n\n    def get_desc(self):\n        return self.desc or f\"${{{self.name}}}\"\n\n    def render(self, value: Step):\n        return value.content  # type: ignore\n</code></pre>"},{"location":"reference/llms/","title":"LLMs","text":"<p>Wrapper for interacting with external and hosted large language models (LLMs).</p> <p>Classes:</p> <ul> <li> <code>CachedLLM</code>           \u2013            <p>A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.</p> </li> <li> <code>LLM</code>           \u2013            <p>An abstract base class representing a Language Learning Model (LLM).</p> </li> <li> <code>LLMEvent</code>           \u2013            <p>An event class representing either a chunk of LLM output or the final LLM output.</p> </li> <li> <code>LLMStream</code>           \u2013            <p>A wrapper class for LLM generators that provides convenient iteration and output extraction.</p> </li> <li> <code>LiteLLM</code>           \u2013            <p>A LiteLLM implementation of the LLM interface.</p> </li> <li> <code>MockLLM</code>           \u2013            <p>A mock LLM implementation for testing purposes.</p> </li> <li> <code>ReplayLLM</code>           \u2013            <p>Specialized LLM class that replays previously recorded LLM interactions.</p> </li> <li> <code>TrainableLLM</code>           \u2013            <p>Class for interacting with trainable language models through OpenAI-compatible API endpoints.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>closest_prompt</code>             \u2013              <p>Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.</p> </li> <li> <code>trainable_llm_make_training_text</code>             \u2013              <p>Generates training text for LLM fine-tuning by combining prompt and output using tokenizer's chat template.</p> </li> </ul>"},{"location":"reference/llms/#tapeagents.llms.CachedLLM","title":"<code>CachedLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.</p> <p>This class implements caching functionality for LLM responses to avoid redundant API calls and enable replay of previous interactions. It supports both file-based caching and SQLite-based replay functionality for testing purposes.</p> <p>Attributes:</p> <ul> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Flag to enable/disable caching functionality. Defaults to False.</p> </li> <li> <code>stream</code>               (<code>bool</code>)           \u2013            <p>Flag to enable/disable streaming responses. Defaults to False.</p> </li> <li> <code>_cache</code>               (<code>dict</code>)           \u2013            <p>Internal cache storage mapping prompt keys to LLM responses.</p> </li> </ul> <p>The cache can be initialized in two modes: 1. SQLite replay mode: Used for testing, enforces cache hits only 2. File-based cache mode: Stores responses in a jsonl file for persistence</p> <p>Cache keys are generated based on the prompt content, excluding the prompt ID. During testing (replay mode), exact text matching is used instead of hashing.</p> <p>Methods:</p> <ul> <li> <code>generate</code>             \u2013              <p>Generate a response stream from the language model based on the given prompt.</p> </li> <li> <code>reindex_log</code>             \u2013              <p>Reindex the log data into cache.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class CachedLLM(LLM):\n    \"\"\"A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.\n\n    This class implements caching functionality for LLM responses to avoid redundant API calls\n    and enable replay of previous interactions. It supports both file-based caching and SQLite-based\n    replay functionality for testing purposes.\n\n    Attributes:\n        use_cache (bool): Flag to enable/disable caching functionality. Defaults to False.\n        stream (bool): Flag to enable/disable streaming responses. Defaults to False.\n        _cache (dict): Internal cache storage mapping prompt keys to LLM responses.\n\n    The cache can be initialized in two modes:\n    1. SQLite replay mode: Used for testing, enforces cache hits only\n    2. File-based cache mode: Stores responses in a jsonl file for persistence\n\n    Cache keys are generated based on the prompt content, excluding the prompt ID.\n    During testing (replay mode), exact text matching is used instead of hashing.\n    \"\"\"\n\n    use_cache: bool = False\n    stream: bool = False\n    _cache: dict = {}\n\n    def model_post_init(self, __content):\n        if _REPLAY_SQLITE:\n            self.use_cache = True\n            self._cache = {}\n            llm_calls = retrieve_all_llm_calls(_REPLAY_SQLITE)\n            for llm_call in llm_calls:\n                key = self.get_prompt_key(llm_call.prompt)\n                self._cache[key] = [LLMEvent(output=llm_call.output)]\n            logger.info(f\"Enforced LLM cache from {_REPLAY_SQLITE}, {len(self._cache)} entries\")\n            return\n        elif not self.use_cache:\n            return\n        logger.info(\"Use LLM Cache\")\n        param_hash = self._key(json.dumps({k: v for k, v in self.parameters.items() if k != \"token\"}))\n        name = self.model_name.replace(\"/\", \"__\")\n        self._cache_file = f\"llm_cache_{name}_{param_hash}.jsonl\"\n        if os.path.exists(self._cache_file):\n            with open(self._cache_file) as f:\n                for line in f:\n                    key, event_dict = json.loads(line)\n                    if key not in self._cache:\n                        self._cache[key] = []\n                    self._cache[key].append(event_dict)\n            logger.info(f\"Loaded cache with {len(self._cache)} keys\")\n        else:\n            logger.info(\"Cache file not found\")\n\n    def reindex_log(self):\n        \"\"\"\n        Reindex the log data into cache.\n\n        This method iterates through the log entries, validates each prompt and output,\n        and adds them to the cache using the prompt key as index. Each entry is converted\n        to an LLMEvent model before caching.\n\n        Side Effects:\n            - Updates the internal cache with log data\n            - Logs the total number of reindexed entries at INFO level\n        \"\"\"\n        cnt = 0\n        for log_data in self._log:\n            key = self.get_prompt_key(Prompt.model_validate(log_data[\"prompt\"]))\n            self._add_to_cache(key, LLMEvent(output=LLMOutput.model_validate(log_data[\"output\"])).model_dump())\n            cnt += 1\n        logger.info(f\"Reindexed {cnt} log entries\")\n\n    def _add_to_cache(self, key: str, event_dict: dict):\n        if not self.use_cache:\n            return\n        if key not in self._cache:\n            self._cache[key] = []\n        self._cache[key].append(event_dict)\n        with open(self._cache_file, \"a\") as f:\n            f.write(json.dumps((key, event_dict), ensure_ascii=False) + \"\\n\")\n\n    def get_prompt_key(self, prompt: Prompt) -&gt; str:\n        prompt_text = json.dumps(prompt.model_dump(exclude={\"id\"}), ensure_ascii=False, sort_keys=True)\n        return self._key(prompt_text)\n\n    def _key(self, text: str) -&gt; str:\n        if _REPLAY_SQLITE:\n            # use exact text as a key during testing\n            return text\n        return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n\n    def generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n        \"\"\"Generate a response stream from the language model based on the given prompt.\n\n        This method handles both cached and new responses, implementing a caching mechanism\n        for LLM responses to avoid redundant API calls.\n\n        Args:\n            prompt (Prompt): The prompt object containing messages to send to the LLM.\n            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.\n\n        Returns:\n            LLMStream: A stream of LLM events containing the model's response.\n\n        Raises:\n            ValueError: If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).\n\n        Notes:\n            - If caching is enabled and the prompt exists in cache, returns cached response\n            - If generating new response, tokens are counted and added to total token count\n            - All generated events are cached for future use if caching is enabled\n            - Output is logged through the logging system\n        \"\"\"\n\n        def _implementation():\n            key = self.get_prompt_key(prompt)\n            if self.use_cache and key in self._cache:\n                logger.debug(colored(f\"llm cache hit, {len(self._cache[key])} events\", \"green\"))\n                for event_dict in self._cache[key]:\n                    event = LLMEvent.model_validate(event_dict)\n                    if event.output is not None:\n                        self.log_output(prompt, event.output, cached=True)\n                    yield event\n            else:\n                if _REPLAY_SQLITE:\n                    closest, score = closest_prompt(key, list(self._cache.keys()))\n                    logger.error(\n                        f\"llm cache miss, closest in cache has score {score:.3f}\\nDIFF:\\n{diff_strings(key, closest)}\"\n                    )\n                    raise ValueError(f\"llm cache miss not allowed, prompt: {key}\")\n                toks = self.count_tokens(prompt.messages)\n                self.token_count += toks\n                logger.debug(f\"{toks} prompt tokens, total: {self.token_count}\")\n                for event in self._generate(prompt, **kwargs):\n                    self._add_to_cache(key, event.model_dump())\n                    # note: the underlying LLM will log the output\n                    yield event\n\n        return LLMStream(_implementation(), prompt)\n\n    @abstractmethod\n    def _generate(self, prompt: Prompt, **kwargs) -&gt; Generator[LLMEvent, None, None]:\n        pass\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.CachedLLM.generate","title":"<code>generate(prompt, **kwargs)</code>","text":"<p>Generate a response stream from the language model based on the given prompt.</p> <p>This method handles both cached and new responses, implementing a caching mechanism for LLM responses to avoid redundant API calls.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt object containing messages to send to the LLM.</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments to pass to the underlying LLM implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMStream</code> (              <code>LLMStream</code> )          \u2013            <p>A stream of LLM events containing the model's response.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).</p> </li> </ul> Notes <ul> <li>If caching is enabled and the prompt exists in cache, returns cached response</li> <li>If generating new response, tokens are counted and added to total token count</li> <li>All generated events are cached for future use if caching is enabled</li> <li>Output is logged through the logging system</li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n    \"\"\"Generate a response stream from the language model based on the given prompt.\n\n    This method handles both cached and new responses, implementing a caching mechanism\n    for LLM responses to avoid redundant API calls.\n\n    Args:\n        prompt (Prompt): The prompt object containing messages to send to the LLM.\n        **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.\n\n    Returns:\n        LLMStream: A stream of LLM events containing the model's response.\n\n    Raises:\n        ValueError: If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).\n\n    Notes:\n        - If caching is enabled and the prompt exists in cache, returns cached response\n        - If generating new response, tokens are counted and added to total token count\n        - All generated events are cached for future use if caching is enabled\n        - Output is logged through the logging system\n    \"\"\"\n\n    def _implementation():\n        key = self.get_prompt_key(prompt)\n        if self.use_cache and key in self._cache:\n            logger.debug(colored(f\"llm cache hit, {len(self._cache[key])} events\", \"green\"))\n            for event_dict in self._cache[key]:\n                event = LLMEvent.model_validate(event_dict)\n                if event.output is not None:\n                    self.log_output(prompt, event.output, cached=True)\n                yield event\n        else:\n            if _REPLAY_SQLITE:\n                closest, score = closest_prompt(key, list(self._cache.keys()))\n                logger.error(\n                    f\"llm cache miss, closest in cache has score {score:.3f}\\nDIFF:\\n{diff_strings(key, closest)}\"\n                )\n                raise ValueError(f\"llm cache miss not allowed, prompt: {key}\")\n            toks = self.count_tokens(prompt.messages)\n            self.token_count += toks\n            logger.debug(f\"{toks} prompt tokens, total: {self.token_count}\")\n            for event in self._generate(prompt, **kwargs):\n                self._add_to_cache(key, event.model_dump())\n                # note: the underlying LLM will log the output\n                yield event\n\n    return LLMStream(_implementation(), prompt)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.CachedLLM.reindex_log","title":"<code>reindex_log()</code>","text":"<p>Reindex the log data into cache.</p> <p>This method iterates through the log entries, validates each prompt and output, and adds them to the cache using the prompt key as index. Each entry is converted to an LLMEvent model before caching.</p> Side Effects <ul> <li>Updates the internal cache with log data</li> <li>Logs the total number of reindexed entries at INFO level</li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def reindex_log(self):\n    \"\"\"\n    Reindex the log data into cache.\n\n    This method iterates through the log entries, validates each prompt and output,\n    and adds them to the cache using the prompt key as index. Each entry is converted\n    to an LLMEvent model before caching.\n\n    Side Effects:\n        - Updates the internal cache with log data\n        - Logs the total number of reindexed entries at INFO level\n    \"\"\"\n    cnt = 0\n    for log_data in self._log:\n        key = self.get_prompt_key(Prompt.model_validate(log_data[\"prompt\"]))\n        self._add_to_cache(key, LLMEvent(output=LLMOutput.model_validate(log_data[\"output\"])).model_dump())\n        cnt += 1\n    logger.info(f\"Reindexed {cnt} log entries\")\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLM","title":"<code>LLM</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>An abstract base class representing a Language Learning Model (LLM).</p> <p>This class defines the interface for interacting with different LLM implementations. It handles basic LLM functionality like token counting, generation, and logging.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name of the LLM model</p> </li> <li> <code>parameters</code>               (<code>dict</code>)           \u2013            <p>Model-specific parameters for generation</p> </li> <li> <code>context_size</code>               (<code>int</code>)           \u2013            <p>Maximum context size in tokens (default: 32000)</p> </li> <li> <code>tokenizer_name</code>               (<code>str</code>)           \u2013            <p>Name of the tokenizer used</p> </li> <li> <code>tokenizer</code>               (<code>Any</code>)           \u2013            <p>Tokenizer instance</p> </li> <li> <code>token_count</code>               (<code>int</code>)           \u2013            <p>Running count of tokens processed</p> </li> <li> <code>_log</code>               (<code>list</code>)           \u2013            <p>Internal log of LLM calls</p> </li> </ul> Note <p>This is an abstract class and requires implementation of the abstract methods in derived classes.</p> <p>Methods:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count tokens in messages or text</p> </li> <li> <code>generate</code>             \u2013              <p>Generate text from a given prompt</p> </li> <li> <code>log_output</code>             \u2013              <p>Logs the output of an LLM (Language Model) call along with its metadata.</p> </li> <li> <code>make_training_text</code>             \u2013              <p>Create training text from prompt and output.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class LLM(BaseModel, ABC):\n    \"\"\"\n    An abstract base class representing a Language Learning Model (LLM).\n\n    This class defines the interface for interacting with different LLM implementations.\n    It handles basic LLM functionality like token counting, generation, and logging.\n\n    Attributes:\n        model_name (str): Name of the LLM model\n        parameters (dict): Model-specific parameters for generation\n        context_size (int): Maximum context size in tokens (default: 32000)\n        tokenizer_name (str): Name of the tokenizer used\n        tokenizer (Any): Tokenizer instance\n        token_count (int): Running count of tokens processed\n        _log (list): Internal log of LLM calls\n\n    Note:\n        This is an abstract class and requires implementation of the abstract methods\n        in derived classes.\n    \"\"\"\n\n    model_name: str\n    parameters: dict = {}\n    context_size: int = 32000\n    tokenizer_name: str = \"\"\n    tokenizer: Any = None\n\n    token_count: int = 0\n    _log: list = []\n\n    @abstractmethod\n    def generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n        \"\"\"\n        Generate text from a given prompt\n\n        Args:\n            prompt (Prompt): The prompt object containing messages to send to the LLM.\n            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.\n\n        Returns:\n            LLMStream: A stream of LLM events containing the model's response.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def count_tokens(self, messages: list[dict] | str) -&gt; int:\n        \"\"\"\n        Count tokens in messages or text\n\n        Args:\n            messages (Union[List[Dict], str]): List of messages or text to count tokens in\n\n        Returns:\n            int: Number of tokens in the messages or text\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n        \"\"\"\n        Create training text from prompt and output.\n\n        Args:\n            prompt (Prompt): The prompt object containing messages used to generate the output.\n            output (LLMOutput): The output generated by the LLM.\n\n        Returns:\n            TrainingText: The training text object containing the prompt and output.\n        \"\"\"\n        pass\n\n    def log_output(self, prompt: Prompt, message: LLMOutput, cached: bool = False):\n        \"\"\"\n        Logs the output of an LLM (Language Model) call along with its metadata.\n\n        Args:\n            prompt (Prompt): The prompt object containing the input messages for the LLM.\n            message (LLMOutput): The output message generated by the LLM.\n            cached (bool, optional): Indicates whether the output was retrieved from cache. Defaults to False.\n        \"\"\"\n        llm_call = LLMCall(\n            timestamp=datetime.datetime.now().isoformat(),\n            prompt=prompt,\n            output=message,\n            prompt_length_tokens=self.count_tokens(prompt.messages),\n            output_length_tokens=self.count_tokens(message.content) if message.content else 0,\n            cached=cached,\n        )\n        self._log.append(llm_call.model_dump())\n        observe_llm_call(llm_call)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLM.count_tokens","title":"<code>count_tokens(messages)</code>  <code>abstractmethod</code>","text":"<p>Count tokens in messages or text</p> <p>Parameters:</p> <ul> <li> <code>messages</code>               (<code>Union[List[Dict], str]</code>)           \u2013            <p>List of messages or text to count tokens in</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Number of tokens in the messages or text</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>@abstractmethod\ndef count_tokens(self, messages: list[dict] | str) -&gt; int:\n    \"\"\"\n    Count tokens in messages or text\n\n    Args:\n        messages (Union[List[Dict], str]): List of messages or text to count tokens in\n\n    Returns:\n        int: Number of tokens in the messages or text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLM.generate","title":"<code>generate(prompt, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate text from a given prompt</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt object containing messages to send to the LLM.</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments to pass to the underlying LLM implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMStream</code> (              <code>LLMStream</code> )          \u2013            <p>A stream of LLM events containing the model's response.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>@abstractmethod\ndef generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n    \"\"\"\n    Generate text from a given prompt\n\n    Args:\n        prompt (Prompt): The prompt object containing messages to send to the LLM.\n        **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.\n\n    Returns:\n        LLMStream: A stream of LLM events containing the model's response.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLM.log_output","title":"<code>log_output(prompt, message, cached=False)</code>","text":"<p>Logs the output of an LLM (Language Model) call along with its metadata.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt object containing the input messages for the LLM.</p> </li> <li> <code>message</code>               (<code>LLMOutput</code>)           \u2013            <p>The output message generated by the LLM.</p> </li> <li> <code>cached</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Indicates whether the output was retrieved from cache. Defaults to False.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def log_output(self, prompt: Prompt, message: LLMOutput, cached: bool = False):\n    \"\"\"\n    Logs the output of an LLM (Language Model) call along with its metadata.\n\n    Args:\n        prompt (Prompt): The prompt object containing the input messages for the LLM.\n        message (LLMOutput): The output message generated by the LLM.\n        cached (bool, optional): Indicates whether the output was retrieved from cache. Defaults to False.\n    \"\"\"\n    llm_call = LLMCall(\n        timestamp=datetime.datetime.now().isoformat(),\n        prompt=prompt,\n        output=message,\n        prompt_length_tokens=self.count_tokens(prompt.messages),\n        output_length_tokens=self.count_tokens(message.content) if message.content else 0,\n        cached=cached,\n    )\n    self._log.append(llm_call.model_dump())\n    observe_llm_call(llm_call)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLM.make_training_text","title":"<code>make_training_text(prompt, output)</code>  <code>abstractmethod</code>","text":"<p>Create training text from prompt and output.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt object containing messages used to generate the output.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output generated by the LLM.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>The training text object containing the prompt and output.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>@abstractmethod\ndef make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n    \"\"\"\n    Create training text from prompt and output.\n\n    Args:\n        prompt (Prompt): The prompt object containing messages used to generate the output.\n        output (LLMOutput): The output generated by the LLM.\n\n    Returns:\n        TrainingText: The training text object containing the prompt and output.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLMEvent","title":"<code>LLMEvent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>An event class representing either a chunk of LLM output or the final LLM output.</p> <p>This class encapsulates events that occur during LLM processing, handling both intermediate chunks of output and the final complete output.</p> <p>Attributes:</p> <ul> <li> <code>chunk</code>               (<code>str</code>)           \u2013            <p>A partial text output from the LLM stream. None if this event represents a complete output.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The complete output from the LLM. None if this event represents a partial chunk.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class LLMEvent(BaseModel):\n    \"\"\"An event class representing either a chunk of LLM output or the final LLM output.\n\n    This class encapsulates events that occur during LLM processing, handling both\n    intermediate chunks of output and the final complete output.\n\n    Attributes:\n        chunk (str, optional): A partial text output from the LLM stream. None if this\n            event represents a complete output.\n        output (LLMOutput, optional): The complete output from the LLM. None if this\n            event represents a partial chunk.\n    \"\"\"\n\n    chunk: str | None = None\n    output: LLMOutput | None = None\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLMStream","title":"<code>LLMStream</code>","text":"<p>A wrapper class for LLM generators that provides convenient iteration and output extraction.</p> <p>This class wraps a generator that yields LLMEvents and provides methods to:</p> <ul> <li>Iterate through events</li> <li>Extract complete LLM output</li> <li>Get the assistant's response text</li> </ul> <p>Attributes:</p> <ul> <li> <code>generator</code>           \u2013            <p>Generator yielding LLMEvents or None if empty</p> </li> <li> <code>prompt</code>           \u2013            <p>The prompt used to generate the LLM response:</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>When trying to iterate null stream, when no output is produced,        or when output is not an assistant message with content</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_output</code>             \u2013              <p>Returns first LLMOutput found in events</p> </li> <li> <code>get_text</code>             \u2013              <p>Returns content of first assistant message found</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class LLMStream:\n    \"\"\"A wrapper class for LLM generators that provides convenient iteration and output extraction.\n\n    This class wraps a generator that yields LLMEvents and provides methods to:\n\n    - Iterate through events\n    - Extract complete LLM output\n    - Get the assistant's response text\n\n    Attributes:\n        generator: Generator yielding LLMEvents or None if empty\n        prompt: The prompt used to generate the LLM response:\n\n    Raises:\n        ValueError: When trying to iterate null stream, when no output is produced,\n                   or when output is not an assistant message with content\n    \"\"\"\n\n    def __init__(self, generator: Generator[LLMEvent, None, None] | None, prompt: Prompt):\n        self.generator = generator\n        self.prompt = prompt\n\n    def __bool__(self):\n        return self.generator is not None\n\n    def __iter__(self):\n        if self.generator is None:\n            raise ValueError(\"can't iterate a null stream\")\n        return self\n\n    def __next__(self) -&gt; LLMEvent:\n        if self.generator is None:\n            raise StopIteration\n        return next(self.generator)\n\n    def get_output(self) -&gt; LLMOutput:\n        \"\"\"Returns first LLMOutput found in events\"\"\"\n        for event in self:\n            if event.output:\n                return event.output\n        raise ValueError(\"LLM did not produce an output\")\n\n    def get_text(self) -&gt; str:\n        \"\"\"Returns content of first assistant message found\"\"\"\n        o = self.get_output()\n        if not o.role == \"assistant\" or o.content is None:\n            raise ValueError(\"LLM did not produce an assistant message\")\n        return o.content\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLMStream.get_output","title":"<code>get_output()</code>","text":"<p>Returns first LLMOutput found in events</p> Source code in <code>tapeagents/llms.py</code> <pre><code>def get_output(self) -&gt; LLMOutput:\n    \"\"\"Returns first LLMOutput found in events\"\"\"\n    for event in self:\n        if event.output:\n            return event.output\n    raise ValueError(\"LLM did not produce an output\")\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LLMStream.get_text","title":"<code>get_text()</code>","text":"<p>Returns content of first assistant message found</p> Source code in <code>tapeagents/llms.py</code> <pre><code>def get_text(self) -&gt; str:\n    \"\"\"Returns content of first assistant message found\"\"\"\n    o = self.get_output()\n    if not o.role == \"assistant\" or o.content is None:\n        raise ValueError(\"LLM did not produce an assistant message\")\n    return o.content\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LiteLLM","title":"<code>LiteLLM</code>","text":"<p>               Bases: <code>CachedLLM</code></p> <p>A LiteLLM implementation of the LLM interface.</p> <p>This class provides integration with the LiteLLM library for making LLM API calls. It supports both streaming and non-streaming responses, token counting, and handles API timeouts with retries. Streaming responses are handled by yielding chunks of text as they arrive. Non-streaming responses return complete messages.</p> Note <p>Function calling during streaming is not yet implemented and will raise NotImplementedError.</p> <p>Methods:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count the number of tokens in a message or string.</p> </li> <li> <code>make_training_text</code>             \u2013              <p>Generates the training text for the model.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class LiteLLM(CachedLLM):\n    \"\"\"A LiteLLM implementation of the LLM interface.\n\n    This class provides integration with the LiteLLM library for making LLM API calls.\n    It supports both streaming and non-streaming responses, token counting, and handles API timeouts with retries.\n    Streaming responses are handled by yielding chunks of text as they arrive.\n    Non-streaming responses return complete messages.\n\n    Note:\n        Function calling during streaming is not yet implemented and will raise NotImplementedError.\n    \"\"\"\n\n    def count_tokens(self, messages: list[dict] | str) -&gt; int:\n        \"\"\"\n        Count the number of tokens in a message or string.\n\n        Args:\n            messages (Union[List[Dict], str]): List of messages or text to count tokens in.\n\n        Returns:\n            int: The number of tokens in the messages or text.\n        \"\"\"\n        if isinstance(messages, str):\n            return litellm.token_counter(model=self.model_name, text=messages)\n        else:\n            return litellm.token_counter(model=self.model_name, messages=messages)\n\n    def _generate(self, prompt: Prompt, **kwargs) -&gt; Generator[LLMEvent, None, None]:\n        while True:\n            try:\n                response = litellm.completion(\n                    model=self.model_name,\n                    messages=prompt.messages,\n                    tools=prompt.tools,\n                    stream=self.stream,\n                    **self.parameters,\n                )\n                break\n            except openai.APITimeoutError:\n                logger.error(\"API Timeout, retrying in 1 sec\")\n                time.sleep(1.0)\n        if self.stream:\n            buffer = []\n            for part in response:\n                assert isinstance(part, litellm.ModelResponse)\n                if isinstance(part.choices[0], litellm.utils.StreamingChoices):\n                    content_delta = part.choices[0].delta.content\n                    if content_delta:\n                        buffer.append(content_delta)\n                        yield LLMEvent(chunk=content_delta)\n                    tool_delta = part.choices[0].delta.tool_calls\n                    if tool_delta:\n                        raise NotImplementedError(\"TODO: streaming with function calls not implemented yet\")\n                else:\n                    raise ValueError(f\"Unexpected response {part.model_dump()}\")\n            output = LLMOutput(content=\"\".join(buffer))\n        else:\n            assert isinstance(response, litellm.ModelResponse)\n            assert isinstance(response.choices[0], litellm.utils.Choices)\n            output = response.choices[0].message\n        self.log_output(prompt, output)\n        yield LLMEvent(output=output)\n\n    def make_training_text(self, *args, **kwargs) -&gt; TrainingText:\n        \"\"\"\n        Generates the training text for the model.\n\n        This method should be implemented by subclasses to provide the specific\n        logic for creating the training text.\n\n        Args:\n            *args (list): Variable length argument list.\n            **kwargs (dict, optional): Arbitrary keyword arguments.\n\n        Returns:\n            TrainingText: The generated training text.\n\n        Raises:\n            NotImplementedError: If the method is not implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LiteLLM.count_tokens","title":"<code>count_tokens(messages)</code>","text":"<p>Count the number of tokens in a message or string.</p> <p>Parameters:</p> <ul> <li> <code>messages</code>               (<code>Union[List[Dict], str]</code>)           \u2013            <p>List of messages or text to count tokens in.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of tokens in the messages or text.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def count_tokens(self, messages: list[dict] | str) -&gt; int:\n    \"\"\"\n    Count the number of tokens in a message or string.\n\n    Args:\n        messages (Union[List[Dict], str]): List of messages or text to count tokens in.\n\n    Returns:\n        int: The number of tokens in the messages or text.\n    \"\"\"\n    if isinstance(messages, str):\n        return litellm.token_counter(model=self.model_name, text=messages)\n    else:\n        return litellm.token_counter(model=self.model_name, messages=messages)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.LiteLLM.make_training_text","title":"<code>make_training_text(*args, **kwargs)</code>","text":"<p>Generates the training text for the model.</p> <p>This method should be implemented by subclasses to provide the specific logic for creating the training text.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>list</code>, default:                   <code>()</code> )           \u2013            <p>Variable length argument list.</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Arbitrary keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>The generated training text.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented by a subclass.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def make_training_text(self, *args, **kwargs) -&gt; TrainingText:\n    \"\"\"\n    Generates the training text for the model.\n\n    This method should be implemented by subclasses to provide the specific\n    logic for creating the training text.\n\n    Args:\n        *args (list): Variable length argument list.\n        **kwargs (dict, optional): Arbitrary keyword arguments.\n\n    Returns:\n        TrainingText: The generated training text.\n\n    Raises:\n        NotImplementedError: If the method is not implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.MockLLM","title":"<code>MockLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>A mock LLM implementation for testing purposes.</p> <p>This class simulates an LLM by returning predefined responses in a cyclic manner. It tracks the prompts it receives and maintains a call counter.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Name of the mock model, defaults to \"mock\"</p> </li> <li> <code>call_number</code>               (<code>int</code>)           \u2013            <p>Counter for number of calls made to generate, defaults to 0</p> </li> <li> <code>mock_outputs</code>               (<code>list[str]</code>)           \u2013            <p>List of predefined responses to cycle through</p> </li> <li> <code>prompts</code>               (<code>list[Prompt]</code>)           \u2013            <p>List of received prompts</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class MockLLM(LLM):\n    \"\"\"A mock LLM implementation for testing purposes.\n\n    This class simulates an LLM by returning predefined responses in a cyclic manner.\n    It tracks the prompts it receives and maintains a call counter.\n\n    Attributes:\n        model_name (str): Name of the mock model, defaults to \"mock\"\n        call_number (int): Counter for number of calls made to generate, defaults to 0\n        mock_outputs (list[str]): List of predefined responses to cycle through\n        prompts (list[Prompt]): List of received prompts\n    \"\"\"\n\n    model_name: str = \"mock\"\n    call_number: int = 0\n    mock_outputs: list[str] = [\n        \"Agent: I'm good, thank you\",\n        \"Agent: Sure, I worked at ServiceNow for 10 years\",\n        \"Agent: I have 10 zillion parameters\",\n    ]\n    prompts: list[Prompt] = []\n\n    def generate(self, prompt: Prompt) -&gt; LLMStream:\n        def _implementation():\n            self.prompts.append(prompt)\n            output = self.mock_outputs[self.call_number % len(self.mock_outputs)]\n            time.sleep(0.01)\n            yield LLMEvent(output=LLMOutput(content=output))\n            self.call_number += 1\n\n        return LLMStream(_implementation(), prompt=prompt)\n\n    def count_tokens(self, messages: list[dict] | str) -&gt; int:\n        return 42\n\n    def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n        return TrainingText(text=\"mock trace\", n_predicted=10)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.ReplayLLM","title":"<code>ReplayLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Specialized LLM class that replays previously recorded LLM interactions.</p> <p>Loads and replays model interactions from a SQLite database, allowing for deterministic replay of previous LLM conversations without making new API calls.</p> <p>The class is useful for:</p> <ul> <li>Testing and debugging LLM interactions</li> <li>Reproducing specific model behaviors</li> <li>Avoiding repeated API calls during development</li> <li>Creating deterministic test scenarios</li> </ul> <p>Attributes:</p> <ul> <li> <code>outputs</code>               (<code>dict[str, str]</code>)           \u2013            <p>Dictionary mapping prompt strings to their recorded outputs</p> </li> <li> <code>llm_calls</code>               (<code>list[LLMCall]</code>)           \u2013            <p>List of recorded LLM call objects</p> </li> <li> <code>count_tokens_fn</code>               (<code>Callable</code>)           \u2013            <p>Function to count tokens in prompts/messages</p> </li> <li> <code>make_training_text_fn</code>               (<code>Callable</code>)           \u2013            <p>Function to create training text from prompt/output pairs</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FatalError</code>             \u2013            <p>When a prompt is not found in the recorded outputs</p> </li> <li> <code>AssertionError</code>             \u2013            <p>When the specified SQLite database file doesn't exist</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Counts the number of tokens in the given messages.</p> </li> <li> <code>from_llm</code>             \u2013              <p>Create a ReplayLLM instance from an existing LLM and a SQLite database file.</p> </li> <li> <code>generate</code>             \u2013              <p>Generates an LLMStream based on the provided prompt.</p> </li> <li> <code>make_training_text</code>             \u2013              <p>Generates training text based on the provided prompt and output.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class ReplayLLM(LLM):\n    \"\"\"\n    Specialized LLM class that replays previously recorded LLM interactions.\n\n    Loads and replays model interactions from a SQLite database, allowing for\n    deterministic replay of previous LLM conversations without making new API calls.\n\n    The class is useful for:\n\n    - Testing and debugging LLM interactions\n    - Reproducing specific model behaviors\n    - Avoiding repeated API calls during development\n    - Creating deterministic test scenarios\n\n    Attributes:\n        outputs (dict[str, str]): Dictionary mapping prompt strings to their recorded outputs\n        llm_calls (list[LLMCall]): List of recorded LLM call objects\n        count_tokens_fn (Callable): Function to count tokens in prompts/messages\n        make_training_text_fn (Callable): Function to create training text from prompt/output pairs\n\n    Raises:\n        FatalError: When a prompt is not found in the recorded outputs\n        AssertionError: When the specified SQLite database file doesn't exist\n    \"\"\"\n\n    outputs: dict[str, str] = Field(default_factory=dict)\n    llm_calls: list[LLMCall]\n    count_tokens_fn: Callable = lambda x: 0\n    make_training_text_fn: Callable = lambda x, y: TrainingText(text=\"\", n_predicted=0)\n\n    @classmethod\n    def from_llm(cls, llm: LLM, run_dir: str, prompts_file: str = DB_DEFAULT_FILENAME):\n        \"\"\"\n        Create a ReplayLLM instance from an existing LLM and a SQLite database file.\n\n        Args:\n            cls (Type): The class to instantiate.\n            llm (LLM): The original LLM instance.\n            run_dir (str): The directory where the SQLite database file is located.\n            prompts_file (str, optional): The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.\n\n        Returns:\n            (ReplayLLM): An instance of ReplayLLM initialized with the LLM calls from the SQLite database.\n\n        Raises:\n            AssertionError: If the SQLite database file does not exist at the specified path.\n        \"\"\"\n        sqlite_fpath = os.path.join(run_dir, prompts_file)\n        assert os.path.exists(sqlite_fpath), f\"Sqlite not found: {sqlite_fpath}\"\n        llm_calls = retrieve_all_llm_calls(sqlite_fpath)\n        replay_llm = ReplayLLM(\n            llm_calls=llm_calls,\n            model_name=llm.tokenizer_name or llm.model_name,\n            context_size=llm.context_size,\n        )\n        replay_llm.tokenizer = llm.tokenizer\n        replay_llm.count_tokens_fn = llm.count_tokens\n        replay_llm.make_training_text_fn = llm.make_training_text\n        return replay_llm\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        dups = 0\n        for llm_call in self.llm_calls:\n            prompt_key = json.dumps(llm_call.prompt.messages, indent=2, ensure_ascii=False, sort_keys=True)\n            output = llm_call.output.content or \"\"\n            if prompt_key in self.outputs and output != self.outputs[prompt_key]:\n                logger.debug(f\"Output duplicate, using last value!\\nOLD:{self.outputs[prompt_key]}\\nNEW:{output}\")\n                dups += 1\n            self.outputs[prompt_key] = output\n        logger.info(f\"Loaded {len(self.outputs)} outputs, {dups} duplicates\")\n        return super().model_post_init(__context)\n\n    def generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n        \"\"\"\n        Generates an LLMStream based on the provided prompt.\n\n        This method checks if the prompt has been previously processed and cached. If a cached output is found,\n        it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no\n        similar prompt is found, a FatalError is raised.\n\n        Args:\n            prompt (Prompt): The prompt object containing the messages to be processed.\n            **kwargs (dict, optional): Additional keyword arguments.\n\n        Returns:\n            LLMStream: A stream of LLM events containing the generated output.\n\n        Raises:\n            FatalError: If the prompt is not found in the cache and no similar prompt is found.\n        \"\"\"\n\n        def _implementation():\n            prompt_key = json.dumps(prompt.messages, indent=2, ensure_ascii=False, sort_keys=True)\n            if prompt_key in self.outputs:\n                logger.debug(colored(\"prompt cache hit\", \"green\"))\n                output = self.outputs[prompt_key]\n            else:\n                logger.warning(\n                    colored(f\"prompt of size {len(prompt_key)} not found, checking similar ones..\", \"yellow\")\n                )\n                known_prompts = list(self.outputs.keys())\n                closest, score = closest_prompt(prompt_key, known_prompts)\n                if score &gt;= 0.7:\n                    logger.warning(f\"Closest prompt score {score:.3f}\")\n                    for i, (a, b) in enumerate(zip_longest(prompt.messages, json.loads(closest), fillvalue={})):\n                        logger.warning(f\"STEP{i}: {diff_strings(a.get('content', str(a)), b.get('content', str(b)))}\\n\")\n                raise FatalError(\"prompt not found\")\n            yield LLMEvent(output=LLMOutput(content=output))\n\n        return LLMStream(_implementation(), prompt=prompt)\n\n    def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n        \"\"\"\n        Generates training text based on the provided prompt and output.\n\n        Args:\n            prompt (Prompt): The input prompt to generate training text from.\n            output (LLMOutput): The output generated by the language model.\n\n        Returns:\n            TrainingText: The generated training text.\n        \"\"\"\n        return self.make_training_text_fn(prompt, output)\n\n    def count_tokens(self, messages: list[dict] | str) -&gt; int:\n        \"\"\"\n        Counts the number of tokens in the given messages.\n\n        Args:\n            messages (Union[list[dict], str]): A list of message dictionaries or a single string message.\n\n        Returns:\n            int: The total number of tokens in the messages.\n        \"\"\"\n        return self.count_tokens_fn(messages)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.ReplayLLM.count_tokens","title":"<code>count_tokens(messages)</code>","text":"<p>Counts the number of tokens in the given messages.</p> <p>Parameters:</p> <ul> <li> <code>messages</code>               (<code>Union[list[dict], str]</code>)           \u2013            <p>A list of message dictionaries or a single string message.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The total number of tokens in the messages.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def count_tokens(self, messages: list[dict] | str) -&gt; int:\n    \"\"\"\n    Counts the number of tokens in the given messages.\n\n    Args:\n        messages (Union[list[dict], str]): A list of message dictionaries or a single string message.\n\n    Returns:\n        int: The total number of tokens in the messages.\n    \"\"\"\n    return self.count_tokens_fn(messages)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.ReplayLLM.from_llm","title":"<code>from_llm(llm, run_dir, prompts_file=DB_DEFAULT_FILENAME)</code>  <code>classmethod</code>","text":"<p>Create a ReplayLLM instance from an existing LLM and a SQLite database file.</p> <p>Parameters:</p> <ul> <li> <code>cls</code>               (<code>Type</code>)           \u2013            <p>The class to instantiate.</p> </li> <li> <code>llm</code>               (<code>LLM</code>)           \u2013            <p>The original LLM instance.</p> </li> <li> <code>run_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the SQLite database file is located.</p> </li> <li> <code>prompts_file</code>               (<code>str</code>, default:                   <code>DB_DEFAULT_FILENAME</code> )           \u2013            <p>The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ReplayLLM</code>           \u2013            <p>An instance of ReplayLLM initialized with the LLM calls from the SQLite database.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If the SQLite database file does not exist at the specified path.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>@classmethod\ndef from_llm(cls, llm: LLM, run_dir: str, prompts_file: str = DB_DEFAULT_FILENAME):\n    \"\"\"\n    Create a ReplayLLM instance from an existing LLM and a SQLite database file.\n\n    Args:\n        cls (Type): The class to instantiate.\n        llm (LLM): The original LLM instance.\n        run_dir (str): The directory where the SQLite database file is located.\n        prompts_file (str, optional): The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.\n\n    Returns:\n        (ReplayLLM): An instance of ReplayLLM initialized with the LLM calls from the SQLite database.\n\n    Raises:\n        AssertionError: If the SQLite database file does not exist at the specified path.\n    \"\"\"\n    sqlite_fpath = os.path.join(run_dir, prompts_file)\n    assert os.path.exists(sqlite_fpath), f\"Sqlite not found: {sqlite_fpath}\"\n    llm_calls = retrieve_all_llm_calls(sqlite_fpath)\n    replay_llm = ReplayLLM(\n        llm_calls=llm_calls,\n        model_name=llm.tokenizer_name or llm.model_name,\n        context_size=llm.context_size,\n    )\n    replay_llm.tokenizer = llm.tokenizer\n    replay_llm.count_tokens_fn = llm.count_tokens\n    replay_llm.make_training_text_fn = llm.make_training_text\n    return replay_llm\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.ReplayLLM.generate","title":"<code>generate(prompt, **kwargs)</code>","text":"<p>Generates an LLMStream based on the provided prompt.</p> <p>This method checks if the prompt has been previously processed and cached. If a cached output is found, it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no similar prompt is found, a FatalError is raised.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt object containing the messages to be processed.</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMStream</code> (              <code>LLMStream</code> )          \u2013            <p>A stream of LLM events containing the generated output.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FatalError</code>             \u2013            <p>If the prompt is not found in the cache and no similar prompt is found.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def generate(self, prompt: Prompt, **kwargs) -&gt; LLMStream:\n    \"\"\"\n    Generates an LLMStream based on the provided prompt.\n\n    This method checks if the prompt has been previously processed and cached. If a cached output is found,\n    it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no\n    similar prompt is found, a FatalError is raised.\n\n    Args:\n        prompt (Prompt): The prompt object containing the messages to be processed.\n        **kwargs (dict, optional): Additional keyword arguments.\n\n    Returns:\n        LLMStream: A stream of LLM events containing the generated output.\n\n    Raises:\n        FatalError: If the prompt is not found in the cache and no similar prompt is found.\n    \"\"\"\n\n    def _implementation():\n        prompt_key = json.dumps(prompt.messages, indent=2, ensure_ascii=False, sort_keys=True)\n        if prompt_key in self.outputs:\n            logger.debug(colored(\"prompt cache hit\", \"green\"))\n            output = self.outputs[prompt_key]\n        else:\n            logger.warning(\n                colored(f\"prompt of size {len(prompt_key)} not found, checking similar ones..\", \"yellow\")\n            )\n            known_prompts = list(self.outputs.keys())\n            closest, score = closest_prompt(prompt_key, known_prompts)\n            if score &gt;= 0.7:\n                logger.warning(f\"Closest prompt score {score:.3f}\")\n                for i, (a, b) in enumerate(zip_longest(prompt.messages, json.loads(closest), fillvalue={})):\n                    logger.warning(f\"STEP{i}: {diff_strings(a.get('content', str(a)), b.get('content', str(b)))}\\n\")\n            raise FatalError(\"prompt not found\")\n        yield LLMEvent(output=LLMOutput(content=output))\n\n    return LLMStream(_implementation(), prompt=prompt)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.ReplayLLM.make_training_text","title":"<code>make_training_text(prompt, output)</code>","text":"<p>Generates training text based on the provided prompt and output.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The input prompt to generate training text from.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output generated by the language model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>The generated training text.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n    \"\"\"\n    Generates training text based on the provided prompt and output.\n\n    Args:\n        prompt (Prompt): The input prompt to generate training text from.\n        output (LLMOutput): The output generated by the language model.\n\n    Returns:\n        TrainingText: The generated training text.\n    \"\"\"\n    return self.make_training_text_fn(prompt, output)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM","title":"<code>TrainableLLM</code>","text":"<p>               Bases: <code>CachedLLM</code></p> <p>Class for interacting with trainable language models through OpenAI-compatible API endpoints.</p> <p>This class implements functionality for both inference and training-related operations with language models served via Text Generation Inference (TGI) or vLLM endpoints that expose an OpenAI-compatible API interface. It supports both streaming and non-streaming modes, and includes methods for token counting and log probability calculations.</p> <p>Attributes:</p> <ul> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>Base URL of the API endpoint</p> </li> <li> <code>api_token</code>               (<code>str</code>)           \u2013            <p>Authentication token for API access</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>count_tokens</code>             \u2013              <p>Count the number of tokens in the given messages.</p> </li> <li> <code>get_log_probs</code>             \u2013              <p>Calculate the log probabilities of the given output based on the provided prompt.</p> </li> <li> <code>get_log_probs_chat_complete</code>             \u2013              <p>Calculate the log probabilities of the tokens in the completion generated by the language model.</p> </li> <li> <code>get_log_probs_complete</code>             \u2013              <p>Get the log probabilities of the tokens in the output given the prompt.</p> </li> <li> <code>load_tokenizer</code>             \u2013              <p>Loads the tokenizer for the model.</p> </li> <li> <code>make_training_text</code>             \u2013              <p>Generates training text from a given prompt and LLM output.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>class TrainableLLM(CachedLLM):\n    \"\"\"\n    Class for interacting with trainable language models through OpenAI-compatible API endpoints.\n\n    This class implements functionality for both inference and training-related operations with\n    language models served via Text Generation Inference (TGI) or vLLM endpoints that expose\n    an OpenAI-compatible API interface. It supports both streaming and non-streaming modes,\n    and includes methods for token counting and log probability calculations.\n\n    Attributes:\n        base_url (str): Base URL of the API endpoint\n        api_token (str): Authentication token for API access\n    \"\"\"\n\n    # TODO: use OpenAI Python client when the certificate issue is resolved.\n    # TODO: consider using litellm\n\n    base_url: str\n    api_token: str = Field(default=\"\", exclude=True)\n\n    def model_post_init(self, __context):\n        super().model_post_init(__context)\n        self.api_token = os.getenv(TAPEAGENTS_LLM_TOKEN, \"\")\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2))\n    def _generate(self, prompt: Prompt) -&gt; Generator[LLMEvent, None, None]:\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_token:\n            headers |= {\"Authorization\": f\"Bearer {self.api_token}\"}\n        data = {\n            \"model\": self.model_name,\n            \"messages\": prompt.messages,\n            \"stream\": self.stream,\n        }\n        r = requests.post(\n            url=f\"{self.base_url}/v1/chat/completions\",\n            json=data | self.parameters,\n            headers=headers,\n            stream=self.stream,\n            verify=False,\n        )\n        if not r.ok:\n            logger.error(f\"Failed to get completion: {r.text}\")\n            r.raise_for_status()\n        if self.stream:\n            response_buffer = []\n            for byte_payload in r.iter_lines():\n                if byte_payload == b\"\\n\":\n                    continue\n                payload = byte_payload.decode(\"utf-8\")\n                if payload.startswith(\"data:\"):\n                    if payload == \"data: [DONE]\":\n                        continue\n                    json_payload = json.loads(payload.lstrip(\"data:\").rstrip(\"\\n\"))\n                    response_delta = json_payload[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n                    if not response_delta:\n                        continue\n                    response_buffer.append(response_delta)\n                    yield LLMEvent(chunk=response_delta)\n            output = LLMOutput(content=\"\".join(response_buffer))\n        else:\n            data = r.json()\n            try:\n                content = data[\"choices\"][0][\"message\"][\"content\"]\n                if not content:\n                    logger.warning(f\"Empty completion {data}\")\n                output = LLMOutput(content=content)\n            except Exception as e:\n                logger.exception(f\"Failed to parse llm response: {r}\")\n                raise e\n        self.log_output(prompt, output)\n        yield LLMEvent(output=output)\n\n    def load_tokenizer(self):\n        \"\"\"\n        Loads the tokenizer for the model.\n\n        If the tokenizer is not already loaded, this method will import the\n        `transformers` library and load the tokenizer using the model name or\n        tokenizer name. If `_MOCK_TOKENIZER` is set, it will use that instead.\n\n        Raises:\n            ValueError: If neither `self.tokenizer_name` nor `self.model_name`\n                        is provided and `_MOCK_TOKENIZER` is not set.\n        \"\"\"\n        if self.tokenizer is None:\n            import transformers\n\n            name = _MOCK_TOKENIZER if _MOCK_TOKENIZER else (self.tokenizer_name or self.model_name)\n            self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)\n\n    def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n        \"\"\"\n        Generates training text from a given prompt and LLM output.\n\n        This method loads the tokenizer and uses it to create training text\n        suitable for training a language model.\n\n        Args:\n            prompt (Prompt): The input prompt to generate training text from.\n            output (LLMOutput): The output from the language model to be used in training.\n\n        Returns:\n            TrainingText: The generated training text.\n        \"\"\"\n        self.load_tokenizer()\n        return trainable_llm_make_training_text(prompt, output, self.tokenizer)\n\n    def get_log_probs_complete(self, prompt: str, output: str) -&gt; list[float]:\n        \"\"\"\n        Get the log probabilities of the tokens in the output given the prompt.\n\n        This method sends a request to the language model API to generate the log probabilities\n        for the tokens in the provided output, given the prompt. It uses the tokenizer to encode\n        the prompt and output, and extracts the log probabilities from the API response.\n\n        Args:\n            prompt (str): The input prompt text.\n            output (str): The output text for which log probabilities are to be calculated.\n\n        Returns:\n            list[float]: A list of log probabilities for each token in the output.\n\n        Raises:\n            RuntimeError: If the API response is not as expected or if there is a mismatch\n                          between the tokens in the response and the provided output.\n        \"\"\"\n        if not self.tokenizer:\n            self.load_tokenizer()\n\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_token:\n            headers |= {\"Authorization\": f\"Bearer {self.api_token}\"}\n\n        if self.tokenizer.bos_token and prompt.startswith(self.tokenizer.bos_token):\n            prompt = prompt[len(self.tokenizer.bos_token) :]\n\n        prompt_text = prompt + output\n        generation_args = {\n            \"model\": self.model_name,\n            \"prompt\": prompt_text,\n            \"temperature\": 0.0,\n            \"max_tokens\": 0,\n            \"logprobs\": 1,\n            \"echo\": True,\n            \"include_stop_str_in_output\": True,  # self.include_stop_str_in_output,\n            \"skip_special_tokens\": False,\n            \"n\": 1,  # number of completions to generate\n            \"stream\": False,  # return a single completion and not a stream of lines\n        }\n        url = f\"{self.base_url}/v1/completions\"\n        logger.debug(f\"POST request to {url}\")\n        r = requests.post(url, json=generation_args, headers=headers, verify=False)\n        r.raise_for_status()  # raise exception if status code is not in the 200s\n        try:\n            response = r.json()\n            log_probs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n            prompt_encoded = self.tokenizer.encode(prompt, add_special_tokens=True)\n            prompt_completion_encoded = self.tokenizer.encode(prompt + output, add_special_tokens=True)\n            log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]\n            tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n            tokens = tokens[len(prompt_encoded) : len(prompt_completion_encoded)]\n            assert \"\".join(tokens) == output, f\"Tokens do not match completion: {''.join(tokens)} != {output}\"\n        except Exception as e:\n            raise RuntimeError(f\"Generation API wrong response: {r.text}\", e)\n        return log_probs\n\n    def get_log_probs_chat_complete(self, prompt: Prompt, output: LLMOutput) -&gt; list[float]:\n        \"\"\"\n        Calculate the log probabilities of the tokens in the completion generated by the language model.\n\n        This function sends a request to the language model API to generate completions and calculate log probabilities.\n        The function uses the tokenizer to encode the prompt and completion texts.\n        The log probabilities are extracted from the API response and validated against the original completion.\n\n        Args:\n            prompt (Prompt): The prompt containing the messages to be sent to the language model.\n            output (LLMOutput): The output from the language model containing the generated completion.\n\n        Returns:\n            list[float]: A list of log probabilities for each token in the generated completion.\n\n        Raises:\n            RuntimeError: If the response from the generation API is incorrect or cannot be parsed.\n        \"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_token:\n            headers |= {\"Authorization\": f\"Bearer {self.api_token}\"}\n\n        time_t0 = time.time()\n        prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False)\n        completion = output.content or \"\"\n        messages = prompt.messages + [output.model_dump()]\n        prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False, add_generation_prompt=True)\n        prompt_completion_text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n        if self.tokenizer.bos_token and prompt_text.startswith(self.tokenizer.bos_token):\n            prompt_text = prompt_text[len(self.tokenizer.bos_token) :]\n            prompt_completion_text = prompt_completion_text[len(self.tokenizer.bos_token) :]\n\n        prompt_encoded = self.tokenizer.encode(prompt_text, add_special_tokens=True)\n        prompt_completion_encoded = self.tokenizer.encode(prompt_completion_text, add_special_tokens=True)\n\n        generation_args = {\n            \"model\": self.model_name,\n            \"messages\": messages,\n            \"temperature\": 0.0,\n            \"max_tokens\": 1,\n            \"logprobs\": 1,\n            \"echo\": True,\n            \"include_stop_str_in_output\": True,  # self.include_stop_str_in_output,\n            \"skip_special_tokens\": False,\n            \"n\": 1,  # number of completions to generate\n            \"stream\": False,  # return a single completion and not a stream of lines\n        }\n        r = requests.post(\n            url=f\"{self.base_url}/v1/chat/completions\",\n            json=generation_args,\n            headers=headers,\n            verify=False,\n        )\n        r.raise_for_status()\n\n        try:\n            response = r.json()\n            log_probs = []\n            decoded_tokens = []\n            for log_prob in response[\"prompt_logprobs\"]:\n                if log_prob:\n                    token_key = next(iter(log_prob))\n                    token_info = log_prob[token_key]\n                    log_probs.append(token_info[\"logprob\"])\n                    decoded_tokens.append(token_info[\"decoded_token\"])\n                else:\n                    log_probs.append(0.0)\n                    decoded_tokens.append(\"\")\n\n            log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]\n            decoded_tokens = decoded_tokens[len(prompt_encoded) : len(prompt_completion_encoded)]\n            reconstructed_completion = \"\".join(decoded_tokens)\n            if self.tokenizer.eos_token in reconstructed_completion:\n                reconstructed_completion = reconstructed_completion[: -len(self.tokenizer.eos_token)]\n            assert (\n                reconstructed_completion == completion\n            ), f\"Tokens do not match completion: {reconstructed_completion} != {completion}\"\n        except Exception as e:\n            raise RuntimeError(f\"Generation API wrong response: {r.text}\", e)\n\n        logger.debug(f\"Log likelihood calculation took {time.time() - time_t0:.2f} seconds\")\n        logger.debug(f\"Tokens per second: {len(log_probs) / (time.time() - time_t0):.2f}\")\n\n        return log_probs\n\n    def get_log_probs(self, prompt: str | Prompt, output: str | LLMOutput) -&gt; list[float]:\n        \"\"\"\n        Calculate the log probabilities of the given output based on the provided prompt.\n\n        Args:\n            prompt (Union[str, Prompt]): The input prompt, which can be either a string or a Prompt object.\n            output (Union[str, LLMOutput]): The output to evaluate, which can be either a string or an LLMOutput object.\n\n        Returns:\n            list[float]: A list of log probabilities corresponding to the given output.\n\n        Raises:\n            ValueError: If the input types are not valid.\n        \"\"\"\n        if isinstance(prompt, str) and isinstance(output, str):\n            return self.get_log_probs_complete(prompt=prompt, output=output)\n        elif isinstance(prompt, Prompt) and isinstance(output, LLMOutput):\n            return self.get_log_probs_chat_complete(prompt=prompt, output=output)\n        else:\n            raise ValueError(\"Invalid input types\")\n\n    def count_tokens(self, messages: list[dict] | str) -&gt; int:\n        \"\"\"\n        Count the number of tokens in the given messages.\n\n        This method loads the tokenizer and then counts the number of tokens\n        in the provided messages. The messages can be either a string or a list\n        of dictionaries.\n\n        Args:\n            messages (Union[list[dict], str]): The messages to count tokens for. It can\n                               be a single string or a list of dictionaries.\n\n        Returns:\n            int: The number of tokens in the provided messages.\n        \"\"\"\n        self.load_tokenizer()\n        if isinstance(messages, str):\n            return len(self.tokenizer(messages).input_ids)\n        else:\n            return len(self.tokenizer.apply_chat_template(messages))\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.count_tokens","title":"<code>count_tokens(messages)</code>","text":"<p>Count the number of tokens in the given messages.</p> <p>This method loads the tokenizer and then counts the number of tokens in the provided messages. The messages can be either a string or a list of dictionaries.</p> <p>Parameters:</p> <ul> <li> <code>messages</code>               (<code>Union[list[dict], str]</code>)           \u2013            <p>The messages to count tokens for. It can                be a single string or a list of dictionaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>The number of tokens in the provided messages.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def count_tokens(self, messages: list[dict] | str) -&gt; int:\n    \"\"\"\n    Count the number of tokens in the given messages.\n\n    This method loads the tokenizer and then counts the number of tokens\n    in the provided messages. The messages can be either a string or a list\n    of dictionaries.\n\n    Args:\n        messages (Union[list[dict], str]): The messages to count tokens for. It can\n                           be a single string or a list of dictionaries.\n\n    Returns:\n        int: The number of tokens in the provided messages.\n    \"\"\"\n    self.load_tokenizer()\n    if isinstance(messages, str):\n        return len(self.tokenizer(messages).input_ids)\n    else:\n        return len(self.tokenizer.apply_chat_template(messages))\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.get_log_probs","title":"<code>get_log_probs(prompt, output)</code>","text":"<p>Calculate the log probabilities of the given output based on the provided prompt.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Union[str, Prompt]</code>)           \u2013            <p>The input prompt, which can be either a string or a Prompt object.</p> </li> <li> <code>output</code>               (<code>Union[str, LLMOutput]</code>)           \u2013            <p>The output to evaluate, which can be either a string or an LLMOutput object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: A list of log probabilities corresponding to the given output.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input types are not valid.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def get_log_probs(self, prompt: str | Prompt, output: str | LLMOutput) -&gt; list[float]:\n    \"\"\"\n    Calculate the log probabilities of the given output based on the provided prompt.\n\n    Args:\n        prompt (Union[str, Prompt]): The input prompt, which can be either a string or a Prompt object.\n        output (Union[str, LLMOutput]): The output to evaluate, which can be either a string or an LLMOutput object.\n\n    Returns:\n        list[float]: A list of log probabilities corresponding to the given output.\n\n    Raises:\n        ValueError: If the input types are not valid.\n    \"\"\"\n    if isinstance(prompt, str) and isinstance(output, str):\n        return self.get_log_probs_complete(prompt=prompt, output=output)\n    elif isinstance(prompt, Prompt) and isinstance(output, LLMOutput):\n        return self.get_log_probs_chat_complete(prompt=prompt, output=output)\n    else:\n        raise ValueError(\"Invalid input types\")\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.get_log_probs_chat_complete","title":"<code>get_log_probs_chat_complete(prompt, output)</code>","text":"<p>Calculate the log probabilities of the tokens in the completion generated by the language model.</p> <p>This function sends a request to the language model API to generate completions and calculate log probabilities. The function uses the tokenizer to encode the prompt and completion texts. The log probabilities are extracted from the API response and validated against the original completion.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The prompt containing the messages to be sent to the language model.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output from the language model containing the generated completion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: A list of log probabilities for each token in the generated completion.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the response from the generation API is incorrect or cannot be parsed.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def get_log_probs_chat_complete(self, prompt: Prompt, output: LLMOutput) -&gt; list[float]:\n    \"\"\"\n    Calculate the log probabilities of the tokens in the completion generated by the language model.\n\n    This function sends a request to the language model API to generate completions and calculate log probabilities.\n    The function uses the tokenizer to encode the prompt and completion texts.\n    The log probabilities are extracted from the API response and validated against the original completion.\n\n    Args:\n        prompt (Prompt): The prompt containing the messages to be sent to the language model.\n        output (LLMOutput): The output from the language model containing the generated completion.\n\n    Returns:\n        list[float]: A list of log probabilities for each token in the generated completion.\n\n    Raises:\n        RuntimeError: If the response from the generation API is incorrect or cannot be parsed.\n    \"\"\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if self.api_token:\n        headers |= {\"Authorization\": f\"Bearer {self.api_token}\"}\n\n    time_t0 = time.time()\n    prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False)\n    completion = output.content or \"\"\n    messages = prompt.messages + [output.model_dump()]\n    prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False, add_generation_prompt=True)\n    prompt_completion_text = self.tokenizer.apply_chat_template(messages, tokenize=False)\n    if self.tokenizer.bos_token and prompt_text.startswith(self.tokenizer.bos_token):\n        prompt_text = prompt_text[len(self.tokenizer.bos_token) :]\n        prompt_completion_text = prompt_completion_text[len(self.tokenizer.bos_token) :]\n\n    prompt_encoded = self.tokenizer.encode(prompt_text, add_special_tokens=True)\n    prompt_completion_encoded = self.tokenizer.encode(prompt_completion_text, add_special_tokens=True)\n\n    generation_args = {\n        \"model\": self.model_name,\n        \"messages\": messages,\n        \"temperature\": 0.0,\n        \"max_tokens\": 1,\n        \"logprobs\": 1,\n        \"echo\": True,\n        \"include_stop_str_in_output\": True,  # self.include_stop_str_in_output,\n        \"skip_special_tokens\": False,\n        \"n\": 1,  # number of completions to generate\n        \"stream\": False,  # return a single completion and not a stream of lines\n    }\n    r = requests.post(\n        url=f\"{self.base_url}/v1/chat/completions\",\n        json=generation_args,\n        headers=headers,\n        verify=False,\n    )\n    r.raise_for_status()\n\n    try:\n        response = r.json()\n        log_probs = []\n        decoded_tokens = []\n        for log_prob in response[\"prompt_logprobs\"]:\n            if log_prob:\n                token_key = next(iter(log_prob))\n                token_info = log_prob[token_key]\n                log_probs.append(token_info[\"logprob\"])\n                decoded_tokens.append(token_info[\"decoded_token\"])\n            else:\n                log_probs.append(0.0)\n                decoded_tokens.append(\"\")\n\n        log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]\n        decoded_tokens = decoded_tokens[len(prompt_encoded) : len(prompt_completion_encoded)]\n        reconstructed_completion = \"\".join(decoded_tokens)\n        if self.tokenizer.eos_token in reconstructed_completion:\n            reconstructed_completion = reconstructed_completion[: -len(self.tokenizer.eos_token)]\n        assert (\n            reconstructed_completion == completion\n        ), f\"Tokens do not match completion: {reconstructed_completion} != {completion}\"\n    except Exception as e:\n        raise RuntimeError(f\"Generation API wrong response: {r.text}\", e)\n\n    logger.debug(f\"Log likelihood calculation took {time.time() - time_t0:.2f} seconds\")\n    logger.debug(f\"Tokens per second: {len(log_probs) / (time.time() - time_t0):.2f}\")\n\n    return log_probs\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.get_log_probs_complete","title":"<code>get_log_probs_complete(prompt, output)</code>","text":"<p>Get the log probabilities of the tokens in the output given the prompt.</p> <p>This method sends a request to the language model API to generate the log probabilities for the tokens in the provided output, given the prompt. It uses the tokenizer to encode the prompt and output, and extracts the log probabilities from the API response.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>)           \u2013            <p>The input prompt text.</p> </li> <li> <code>output</code>               (<code>str</code>)           \u2013            <p>The output text for which log probabilities are to be calculated.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: A list of log probabilities for each token in the output.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the API response is not as expected or if there is a mismatch           between the tokens in the response and the provided output.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def get_log_probs_complete(self, prompt: str, output: str) -&gt; list[float]:\n    \"\"\"\n    Get the log probabilities of the tokens in the output given the prompt.\n\n    This method sends a request to the language model API to generate the log probabilities\n    for the tokens in the provided output, given the prompt. It uses the tokenizer to encode\n    the prompt and output, and extracts the log probabilities from the API response.\n\n    Args:\n        prompt (str): The input prompt text.\n        output (str): The output text for which log probabilities are to be calculated.\n\n    Returns:\n        list[float]: A list of log probabilities for each token in the output.\n\n    Raises:\n        RuntimeError: If the API response is not as expected or if there is a mismatch\n                      between the tokens in the response and the provided output.\n    \"\"\"\n    if not self.tokenizer:\n        self.load_tokenizer()\n\n    headers = {\"Content-Type\": \"application/json\"}\n    if self.api_token:\n        headers |= {\"Authorization\": f\"Bearer {self.api_token}\"}\n\n    if self.tokenizer.bos_token and prompt.startswith(self.tokenizer.bos_token):\n        prompt = prompt[len(self.tokenizer.bos_token) :]\n\n    prompt_text = prompt + output\n    generation_args = {\n        \"model\": self.model_name,\n        \"prompt\": prompt_text,\n        \"temperature\": 0.0,\n        \"max_tokens\": 0,\n        \"logprobs\": 1,\n        \"echo\": True,\n        \"include_stop_str_in_output\": True,  # self.include_stop_str_in_output,\n        \"skip_special_tokens\": False,\n        \"n\": 1,  # number of completions to generate\n        \"stream\": False,  # return a single completion and not a stream of lines\n    }\n    url = f\"{self.base_url}/v1/completions\"\n    logger.debug(f\"POST request to {url}\")\n    r = requests.post(url, json=generation_args, headers=headers, verify=False)\n    r.raise_for_status()  # raise exception if status code is not in the 200s\n    try:\n        response = r.json()\n        log_probs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n        prompt_encoded = self.tokenizer.encode(prompt, add_special_tokens=True)\n        prompt_completion_encoded = self.tokenizer.encode(prompt + output, add_special_tokens=True)\n        log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]\n        tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n        tokens = tokens[len(prompt_encoded) : len(prompt_completion_encoded)]\n        assert \"\".join(tokens) == output, f\"Tokens do not match completion: {''.join(tokens)} != {output}\"\n    except Exception as e:\n        raise RuntimeError(f\"Generation API wrong response: {r.text}\", e)\n    return log_probs\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.load_tokenizer","title":"<code>load_tokenizer()</code>","text":"<p>Loads the tokenizer for the model.</p> <p>If the tokenizer is not already loaded, this method will import the <code>transformers</code> library and load the tokenizer using the model name or tokenizer name. If <code>_MOCK_TOKENIZER</code> is set, it will use that instead.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither <code>self.tokenizer_name</code> nor <code>self.model_name</code>         is provided and <code>_MOCK_TOKENIZER</code> is not set.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def load_tokenizer(self):\n    \"\"\"\n    Loads the tokenizer for the model.\n\n    If the tokenizer is not already loaded, this method will import the\n    `transformers` library and load the tokenizer using the model name or\n    tokenizer name. If `_MOCK_TOKENIZER` is set, it will use that instead.\n\n    Raises:\n        ValueError: If neither `self.tokenizer_name` nor `self.model_name`\n                    is provided and `_MOCK_TOKENIZER` is not set.\n    \"\"\"\n    if self.tokenizer is None:\n        import transformers\n\n        name = _MOCK_TOKENIZER if _MOCK_TOKENIZER else (self.tokenizer_name or self.model_name)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.TrainableLLM.make_training_text","title":"<code>make_training_text(prompt, output)</code>","text":"<p>Generates training text from a given prompt and LLM output.</p> <p>This method loads the tokenizer and uses it to create training text suitable for training a language model.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The input prompt to generate training text from.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The output from the language model to be used in training.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>The generated training text.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def make_training_text(self, prompt: Prompt, output: LLMOutput) -&gt; TrainingText:\n    \"\"\"\n    Generates training text from a given prompt and LLM output.\n\n    This method loads the tokenizer and uses it to create training text\n    suitable for training a language model.\n\n    Args:\n        prompt (Prompt): The input prompt to generate training text from.\n        output (LLMOutput): The output from the language model to be used in training.\n\n    Returns:\n        TrainingText: The generated training text.\n    \"\"\"\n    self.load_tokenizer()\n    return trainable_llm_make_training_text(prompt, output, self.tokenizer)\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.closest_prompt","title":"<code>closest_prompt(prompt_key, known_prompts)</code>","text":"<p>Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.</p> <p>Parameters:</p> <ul> <li> <code>prompt_key</code>               (<code>str</code>)           \u2013            <p>The prompt to compare against the known prompts.</p> </li> <li> <code>known_prompts</code>               (<code>list[str]</code>)           \u2013            <p>A list of known prompts to compare with the prompt_key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, float]</code>           \u2013            <p>tuple[str, float]: A tuple containing the closest matching prompt and its similarity score.                If no prompts are found, returns an empty string and a score of 0.0.</p> </li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def closest_prompt(prompt_key: str, known_prompts: list[str]) -&gt; tuple[str, float]:\n    \"\"\"\n    Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.\n\n    Args:\n        prompt_key (str): The prompt to compare against the known prompts.\n        known_prompts (list[str]): A list of known prompts to compare with the prompt_key.\n\n    Returns:\n        tuple[str, float]: A tuple containing the closest matching prompt and its similarity score.\n                           If no prompts are found, returns an empty string and a score of 0.0.\n    \"\"\"\n    ratios = [(k, ratio(prompt_key, k, score_cutoff=0.5)) for k in known_prompts]\n    if not len(ratios):\n        return \"\", 0.0\n    ratios = sorted(ratios, key=lambda x: x[1], reverse=True)\n    closest, score = sorted(ratios, key=lambda x: x[1], reverse=True)[0]\n    return closest, score\n</code></pre>"},{"location":"reference/llms/#tapeagents.llms.trainable_llm_make_training_text","title":"<code>trainable_llm_make_training_text(prompt, output, tokenizer)</code>","text":"<p>Generates training text for LLM fine-tuning by combining prompt and output using tokenizer's chat template.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Prompt</code>)           \u2013            <p>The input prompt containing conversation messages.</p> </li> <li> <code>output</code>               (<code>LLMOutput</code>)           \u2013            <p>The model's output/response.</p> </li> <li> <code>tokenizer</code>               (<code>PreTrainedTokenizer</code>)           \u2013            <p>The tokenizer used to format the conversation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TrainingText</code> (              <code>TrainingText</code> )          \u2013            <p>A dataclass containing:</p> <ul> <li>text (str): The formatted conversation text</li> <li>n_predicted (int): Length of the output text portion</li> </ul> </li> </ul> Note <ul> <li>Uses tokenizer's chat template to format conversations</li> <li>Removes BOS token if present in the beginning of the text</li> </ul> Source code in <code>tapeagents/llms.py</code> <pre><code>def trainable_llm_make_training_text(prompt: Prompt, output: LLMOutput, tokenizer) -&gt; TrainingText:\n    \"\"\"\n    Generates training text for LLM fine-tuning by combining prompt and output using tokenizer's chat template.\n\n    Args:\n        prompt (Prompt): The input prompt containing conversation messages.\n        output (LLMOutput): The model's output/response.\n        tokenizer (PreTrainedTokenizer): The tokenizer used to format the conversation.\n\n    Returns:\n        TrainingText: A dataclass containing:\n\n            - text (str): The formatted conversation text\n            - n_predicted (int): Length of the output text portion\n\n    Note:\n        - Uses tokenizer's chat template to format conversations\n        - Removes BOS token if present in the beginning of the text\n    \"\"\"\n    prompt_text = tokenizer.apply_chat_template(\n        conversation=prompt.messages, tokenize=False, add_generation_prompt=True\n    )\n    text = tokenizer.apply_chat_template(\n        prompt.messages + [{\"role\": \"assistant\", \"content\": output.content}], tokenize=False\n    )\n    output_text = text[len(prompt_text) :]\n\n    if tokenizer.bos_token and text.startswith(tokenizer.bos_token):\n        text = text[len(tokenizer.bos_token) :]\n\n    return TrainingText(text=text, n_predicted=len(output_text))\n</code></pre>"},{"location":"reference/nodes/","title":"Nodes","text":"<p>Nodes are the building blocks of a TapeAgent, representing atomic units of the agent's behavior.</p> <p>Classes:</p> <ul> <li> <code>CallSubagent</code>           \u2013            <p>Node that calls a subagent with inputs from the current tape view.</p> </li> <li> <code>ControlFlowNode</code>           \u2013            <p>A node that controls the flow of execution by selecting the next node based on tape content.</p> </li> <li> <code>FixedStepsNode</code>           \u2013            <p>A Node that generates a fixed sequence of predefined steps.</p> </li> <li> <code>MonoNode</code>           \u2013            <p>A node for simple monolithic agents that handles simple prompt generation, and universal LLM output parsing.</p> </li> <li> <code>ObservationControlNode</code>           \u2013            <p>A control flow node that selects the next node based on the last observation in the tape.</p> </li> </ul>"},{"location":"reference/nodes/#tapeagents.nodes.CallSubagent","title":"<code>CallSubagent</code>","text":"<p>               Bases: <code>Node</code></p> <p>Node that calls a subagent with inputs from the current tape view.</p> Source code in <code>tapeagents/nodes.py</code> <pre><code>class CallSubagent(Node):\n    \"\"\"\n    Node that calls a subagent with inputs from the current tape view.\n    \"\"\"\n\n    agent: Agent\n    inputs: tuple[str | int, ...] = Field(\n        default_factory=tuple,\n        description=\"Names of the subagents which outputs are required for the current subagent to run\",\n    )\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        self.name = f\"{self.agent.name}Node\"\n\n    def generate_steps(self, _: Any, tape: Tape, llm_stream: LLMStream):\n        view = TapeViewStack.compute(tape)\n        yield Call(agent_name=self.agent.name)\n        for input_ in self.inputs:\n            yield view.top.get_output(input_).model_copy(deep=True)\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.ControlFlowNode","title":"<code>ControlFlowNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node that controls the flow of execution by selecting the next node based on tape content.</p> <p>This abstract class provides a framework for implementing control flow logic in a node. It determines which node should be executed next based on the current state of the tape.</p> Example <pre><code>class MyControlFlow(ControlFlowNode):\n    def select_node(self, tape):\n        if isinstance(tape[-1], SuccessObservation):\n            return 'node_a'\n        return 'node_b'\n</code></pre> <p>Methods:</p> <ul> <li> <code>generate_steps</code>             \u2013              <p>Generates steps that moves the execution to the next node based on the tape content.</p> </li> <li> <code>select_node</code>             \u2013              <p>Select the next node based on the provided tape.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>class ControlFlowNode(Node):\n    \"\"\"\n    A node that controls the flow of execution by selecting the next node based on tape content.\n\n    This abstract class provides a framework for implementing control flow logic in a node.\n    It determines which node should be executed next based on the current state of the tape.\n\n    Example:\n        ```python\n        class MyControlFlow(ControlFlowNode):\n            def select_node(self, tape):\n                if isinstance(tape[-1], SuccessObservation):\n                    return 'node_a'\n                return 'node_b'\n        ```\n    \"\"\"\n\n    def generate_steps(\n        self, agent: Any, tape: Tape, llm_stream: LLMStream\n    ) -&gt; Generator[Step | PartialStep, None, None]:\n        \"\"\"\n        Generates steps that moves the execution to the next node based on the tape content.\n\n        Args:\n            agent (Any): The agent instance executing the node\n            tape (Tape): The tape object containing the context and state\n            llm_stream (LLMStream): Stream for language model interaction\n\n        Yields:\n            step (SetNextNode): A step indicating which node should be executed next\n        \"\"\"\n        yield SetNextNode(next_node=self.select_node(tape))\n\n    def select_node(self, tape: Tape) -&gt; str:\n        \"\"\"\n        Select the next node based on the provided tape.\n\n        This method should be implemented in a subclass to define the logic for selecting the next node.\n\n        Args:\n            tape (Tape): The tape object containing the necessary information for node selection.\n\n        Returns:\n            str: The identifier of the next node.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in the subclass.\n        \"\"\"\n        raise NotImplementedError(\"Implement this method in the subclass to set the next node according to your logic\")\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.ControlFlowNode.generate_steps","title":"<code>generate_steps(agent, tape, llm_stream)</code>","text":"<p>Generates steps that moves the execution to the next node based on the tape content.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent instance executing the node</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the context and state</p> </li> <li> <code>llm_stream</code>               (<code>LLMStream</code>)           \u2013            <p>Stream for language model interaction</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>step</code> (              <code>SetNextNode</code> )          \u2013            <p>A step indicating which node should be executed next</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def generate_steps(\n    self, agent: Any, tape: Tape, llm_stream: LLMStream\n) -&gt; Generator[Step | PartialStep, None, None]:\n    \"\"\"\n    Generates steps that moves the execution to the next node based on the tape content.\n\n    Args:\n        agent (Any): The agent instance executing the node\n        tape (Tape): The tape object containing the context and state\n        llm_stream (LLMStream): Stream for language model interaction\n\n    Yields:\n        step (SetNextNode): A step indicating which node should be executed next\n    \"\"\"\n    yield SetNextNode(next_node=self.select_node(tape))\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.ControlFlowNode.select_node","title":"<code>select_node(tape)</code>","text":"<p>Select the next node based on the provided tape.</p> <p>This method should be implemented in a subclass to define the logic for selecting the next node.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the necessary information for node selection.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The identifier of the next node.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the method is not implemented in the subclass.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def select_node(self, tape: Tape) -&gt; str:\n    \"\"\"\n    Select the next node based on the provided tape.\n\n    This method should be implemented in a subclass to define the logic for selecting the next node.\n\n    Args:\n        tape (Tape): The tape object containing the necessary information for node selection.\n\n    Returns:\n        str: The identifier of the next node.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in the subclass.\n    \"\"\"\n    raise NotImplementedError(\"Implement this method in the subclass to set the next node according to your logic\")\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.FixedStepsNode","title":"<code>FixedStepsNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A Node that generates a fixed sequence of predefined steps.</p> <p>This node simply yields a sequence of steps that were provided during initialization, without any dynamic generation or modification.</p> <p>Attributes:</p> <ul> <li> <code>steps</code>               (<code>list[Step]</code>)           \u2013            <p>A list of Step objects to be yielded in sequence.</p> </li> </ul> Example <pre><code>fixed_node = FixedStepsNode(steps=[\n    AssistantStep(text=\"Hello\"),\n    SetNextNode(next_node=\"node_a\")\n])\n</code></pre> Source code in <code>tapeagents/nodes.py</code> <pre><code>class FixedStepsNode(Node):\n    \"\"\"A Node that generates a fixed sequence of predefined steps.\n\n    This node simply yields a sequence of steps that were provided during initialization,\n    without any dynamic generation or modification.\n\n    Attributes:\n        steps (list[Step]): A list of Step objects to be yielded in sequence.\n\n    Example:\n        ```python\n        fixed_node = FixedStepsNode(steps=[\n            AssistantStep(text=\"Hello\"),\n            SetNextNode(next_node=\"node_a\")\n        ])\n        ```\n    \"\"\"\n\n    steps: list[Step]\n\n    def generate_steps(\n        self, agent: Any, tape: Tape, llm_stream: LLMStream\n    ) -&gt; Generator[Step | PartialStep, None, None]:\n        for step in self.steps:\n            yield step\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode","title":"<code>MonoNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>A node for simple monolithic agents that handles simple prompt generation, and universal LLM output parsing.</p> <p>This node performs the following functions:</p> <ul> <li>Renders the entire tape into a prompt, trimming if needed</li> <li>Attaches guidance text to the end of the prompt after rendering the tape</li> <li>Parses the LLM output into provided step classes (class provided as annotated union)</li> </ul> <p>Attributes:</p> <ul> <li> <code>guidance</code>               (<code>str</code>)           \u2013            <p>Guidance text attached to the end of the prompt</p> </li> <li> <code>system_prompt</code>               (<code>str</code>)           \u2013            <p>System prompt used in message construction</p> </li> <li> <code>steps_prompt</code>               (<code>str</code>)           \u2013            <p>Prompt describing the steps the agent can take</p> </li> <li> <code>agent_step_cls</code>               (<code>Any</code>)           \u2013            <p>Class used for step validation, excluded from model</p> </li> <li> <code>next_node</code>               (<code>str</code>)           \u2013            <p>Identifier for the next node in sequence</p> </li> </ul> Example <pre><code>node = MonoNode(\n    guidance=\"Please respond with next action\",\n    system_prompt=\"You are a helpful assistant\",\n    steps_prompt=\"Available steps: think, act, finish\",\n    agent_step_cls=AgentStep\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>generate_steps</code>             \u2013              <p>Generates a sequence of steps based on the LLM stream output.</p> </li> <li> <code>get_steps_description</code>             \u2013              <p>Get the steps description for the agent's task.</p> </li> <li> <code>make_llm_output</code>             \u2013              <p>Creates an LLMOutput from a sequence of steps in the tape that share the same prompt_id.</p> </li> <li> <code>make_prompt</code>             \u2013              <p>Create a prompt from tape interactions.</p> </li> <li> <code>parse_completion</code>             \u2013              <p>Parse LLM completion output into a sequence of agent steps.</p> </li> <li> <code>postprocess_step</code>             \u2013              <p>Post-processes a step after its generation.</p> </li> <li> <code>prepare_tape</code>             \u2013              <p>Prepares tape by filtering out control flow steps.</p> </li> <li> <code>tape_to_messages</code>             \u2013              <p>Converts a Tape object and steps description into a list of messages for LLM conversation.</p> </li> <li> <code>trim_tape</code>             \u2013              <p>Trims the tape by removing unnecessary positions.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>class MonoNode(Node):\n    \"\"\"\n    A node for simple monolithic agents that handles simple prompt generation, and universal LLM output parsing.\n\n    This node performs the following functions:\n\n    - Renders the entire tape into a prompt, trimming if needed\n    - Attaches guidance text to the end of the prompt after rendering the tape\n    - Parses the LLM output into provided step classes (class provided as annotated union)\n\n    Attributes:\n        guidance (str): Guidance text attached to the end of the prompt\n        system_prompt (str): System prompt used in message construction\n        steps_prompt (str): Prompt describing the steps the agent can take\n        agent_step_cls (Any): Class used for step validation, excluded from model\n        next_node (str): Identifier for the next node in sequence\n\n    Example:\n        ```python\n        node = MonoNode(\n            guidance=\"Please respond with next action\",\n            system_prompt=\"You are a helpful assistant\",\n            steps_prompt=\"Available steps: think, act, finish\",\n            agent_step_cls=AgentStep\n        )\n        ```\n    \"\"\"\n\n    guidance: str = \"\"  # guidance text that is attached to the end of the prompt\n    system_prompt: str = \"\"\n    steps_prompt: str = \"\"  # prompt that describes the steps that the agent can take\n    agent_step_cls: Any = Field(exclude=True)\n    next_node: str = \"\"\n\n    def make_prompt(self, agent: Any, tape: Tape) -&gt; Prompt:\n        \"\"\"Create a prompt from tape interactions.\n\n        This method constructs a prompt by processing the tape content and agent steps description\n        into a format suitable for LLM consumption. It includes token count checks and tape trimming\n        if needed to fit within context size limits.\n\n        Args:\n            agent (Any): The agent object containing LLM configuration.\n            tape (Tape): The tape object containing interaction history.\n\n        Returns:\n            Prompt: A Prompt object containing formatted messages for LLM consumption.\n\n        Note:\n            The method performs the following steps:\n\n            1. Cleans the tape content\n            2. Gets steps description\n            3. Converts tape to messages\n            4. Checks token count and trims if needed\n            5. Reconstructs messages if trimming occurred\n        \"\"\"\n        cleaned_tape = self.prepare_tape(tape)\n        steps_description = self.get_steps_description(tape, agent)\n        messages = self.tape_to_messages(cleaned_tape, steps_description)\n        if agent.llm.count_tokens(messages) &gt; (agent.llm.context_size - 500):\n            cleaned_tape = self.trim_tape(cleaned_tape)\n        messages = self.tape_to_messages(cleaned_tape, steps_description)\n        return Prompt(messages=messages)\n\n    def prepare_tape(self, tape: Tape) -&gt; Tape:\n        \"\"\"\n        Prepares tape by filtering out control flow steps.\n\n        This method creates a new tape instance with only non-control flow steps,\n        specifically excluding SetNextNode instances.\n\n        Args:\n            tape (Tape): The input tape containing a sequence of steps.\n\n        Returns:\n            Tape: A new tape instance containing only non-control flow steps.\n        \"\"\"\n        steps_without_control_flow = [step for step in tape.steps if not isinstance(step, SetNextNode)]\n        return tape.model_copy(update=dict(steps=steps_without_control_flow))\n\n    def make_llm_output(self, agent: Any, tape: Tape, index: int) -&gt; LLMOutput:\n        \"\"\"\n        Creates an LLMOutput from a sequence of steps in the tape that share the same prompt_id.\n\n        Args:\n            agent (Any): The agent instance associated with the output.\n            tape (Tape): The tape containing the sequence of steps.\n            index (int): The starting index in the tape to process steps from.\n\n        Returns:\n            LLMOutput: An output object containing:\n\n                - role: Set to \"assistant\"\n                - content: JSON string of step data, formatted as either: a single dictionary\n                if there is only one step, or a list of dictionaries\n\n        Note:\n            - Only processes steps with matching prompt_id from the starting index\n            - Excludes SetNextNode steps from the output\n            - JSON content is formatted with indentation\n        \"\"\"\n        steps = []\n        i = index\n        first_prompt_id = tape.steps[i].metadata.prompt_id\n        while i &lt; len(tape) and tape.steps[i].metadata.prompt_id == first_prompt_id:\n            if not isinstance(tape.steps[i], SetNextNode):\n                steps.append(tape.steps[i])\n            i += 1\n\n        # if there is only one step, return it as a single dict, not a list\n        content = [step.llm_dict() for step in steps] if len(steps) &gt; 1 else steps[0].llm_dict()\n        return LLMOutput(role=\"assistant\", content=json.dumps(content, indent=2, ensure_ascii=False))\n\n    def tape_to_messages(self, tape: Tape, steps_description: str) -&gt; list[dict]:\n        \"\"\"\n        Converts a Tape object and steps description into a list of messages for LLM conversation.\n\n        Args:\n            tape (Tape): A Tape object containing conversation steps.\n            steps_description (str): A description of the conversation steps.\n\n        Returns:\n            list[dict]: A list of dictionaries representing the conversation messages.\n                       Each dictionary contains 'role' and 'content' keys.\n                       Roles can be 'system', 'user', or 'assistant'.\n                       The system prompt is always the first message.\n                       If steps_description is provided, it's added as a user message.\n                       Messages from tape are added with roles based on step type.\n                       If guidance exists, it's added as the final user message.\n        \"\"\"\n        messages: list[dict] = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n        ]\n        if steps_description:\n            messages.append({\"role\": \"user\", \"content\": steps_description})\n        for step in tape:\n            role = \"assistant\" if isinstance(step, AgentStep) else \"user\"\n            messages.append({\"role\": role, \"content\": step.llm_view()})\n        if self.guidance:\n            messages.append({\"role\": \"user\", \"content\": self.guidance})\n        return messages\n\n    def get_steps_description(self, tape: Tape, agent: Any) -&gt; str:\n        \"\"\"\n        Get the steps description for the agent's task.\n\n        This method returns the predefined steps prompt which describes the sequence of actions\n        or steps that the agent should follow.\n\n        Args:\n            tape (Tape): The tape object containing the context and state information.\n            agent (Any): The agent object that will execute the steps.\n\n        Returns:\n            str: The steps prompt describing the sequence of actions.\n        \"\"\"\n        return self.steps_prompt\n\n    def generate_steps(\n        self, agent: Any, tape: Tape, llm_stream: LLMStream\n    ) -&gt; Generator[Step | PartialStep, None, None]:\n        \"\"\"\n        Generates a sequence of steps based on the LLM stream output.\n\n        This method processes the output from a language model stream and converts it into a series of steps.\n        It handles the parsing of completions and post-processing of steps.\n\n        Args:\n            agent (Any): The agent instance that will execute the steps.\n            tape (Tape): The tape object containing the execution context and history.\n            llm_stream (LLMStream): The stream of language model outputs to process.\n\n        Yields:\n            Union[Step, PartialStep]: Individual steps generated from the LLM stream output.\n\n        Raises:\n            FatalError: If no completions are generated from the LLM stream.\n\n        Note:\n            - If the node has a next_node defined and the final step is not a StopStep,\n              it will yield a SetNextNode step to continue the execution flow.\n        \"\"\"\n        new_steps = []\n        try:\n            cnt = 0\n            for event in llm_stream:\n                if event.output:\n                    cnt += 1\n                    assert event.output.content\n                    for step in self.parse_completion(event.output.content, llm_stream.prompt.id):\n                        step = self.postprocess_step(tape, new_steps, step)\n                        new_steps.append(step)\n                        yield step\n            if not cnt:\n                raise FatalError(\"No completions!\")\n        except FatalError:\n            raise\n\n        if self.next_node and not isinstance(new_steps[-1], StopStep):\n            yield SetNextNode(next_node=self.next_node)\n\n    def postprocess_step(self, tape: Tape, new_steps: list[Step], step: Step) -&gt; Step:\n        \"\"\"\n        Post-processes a step after its generation.\n\n        By default returns the step unchanged.\n\n        Args:\n            tape (Tape): The tape\n            new_steps (list[Step]): List of new steps that were generated during the current iteration\n            step (Step): The step that was just generated\n\n        Returns:\n            Step: The processed step, by default returns the original step unmodified\n        \"\"\"\n        return step\n\n    def parse_completion(self, llm_output: str, prompt_id: str) -&gt; Generator[Step, None, None]:\n        \"\"\"Parse LLM completion output into a sequence of agent steps.\n\n        This method processes the LLM output string by parsing it as JSON and validating it against\n        the agent step class schema. It handles both single step and multi-step outputs.\n\n        Args:\n            llm_output (str): The raw output string from the LLM to be parsed\n            prompt_id (str): Identifier for the prompt that generated this completion\n\n        Yields:\n            Step: Individual validated agent steps with prompt_id metadata\n            LLMOutputParsingFailureAction: Error information if parsing or validation fails\n\n        Note:\n            All parsing errors are handled internally and yielded as\n            LLMOutputParsingFailureAction objects.\n        \"\"\"\n        try:\n            step_dicts = json.loads(sanitize_json_completion(llm_output))\n            if isinstance(step_dicts, dict):\n                step_dicts = [step_dicts]\n        except Exception as e:\n            logger.exception(f\"Failed to parse LLM output as json: {llm_output}\\n\\nError: {e}\")\n            yield LLMOutputParsingFailureAction(error=f\"Failed to parse LLM output as json: {e}\", llm_output=llm_output)\n            return\n        try:\n            steps = [TypeAdapter(self.agent_step_cls).validate_python(step_dict) for step_dict in step_dicts]\n        except ValidationError as e:\n            err_text = \"\"\n            for err in e.errors():\n                loc = \".\".join([str(loc) for loc in err[\"loc\"]])\n                err_text += f\"{loc}: {err['msg']}\\n\"\n            logger.exception(f\"Failed to validate LLM output: {step_dicts}\\n\\nErrors:\\n{err_text}\")\n            yield LLMOutputParsingFailureAction(\n                error=f\"Failed to validate LLM output: {err_text}\", llm_output=llm_output\n            )\n            return\n        except Exception as e:\n            logger.exception(f\"Failed to parse LLM output dict: {step_dicts}\\n\\nError: {e}\")\n            yield LLMOutputParsingFailureAction(error=f\"Failed to parse LLM output dict: {e}\", llm_output=llm_output)\n            return\n        for step in steps:\n            step.metadata.prompt_id = prompt_id\n            yield step\n\n    def trim_tape(self, tape: Tape) -&gt; Tape:\n        \"\"\"\n        Trims the tape by removing unnecessary positions.\n\n        Args:\n            tape (Tape): The tape object to be trimmed.\n\n        Returns:\n            Tape: The trimmed tape object.\n\n        Note:\n            Currently this is a placeholder method that returns the tape unchanged.\n        \"\"\"\n        return tape\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.generate_steps","title":"<code>generate_steps(agent, tape, llm_stream)</code>","text":"<p>Generates a sequence of steps based on the LLM stream output.</p> <p>This method processes the output from a language model stream and converts it into a series of steps. It handles the parsing of completions and post-processing of steps.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent instance that will execute the steps.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the execution context and history.</p> </li> <li> <code>llm_stream</code>               (<code>LLMStream</code>)           \u2013            <p>The stream of language model outputs to process.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Step | PartialStep</code>           \u2013            <p>Union[Step, PartialStep]: Individual steps generated from the LLM stream output.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FatalError</code>             \u2013            <p>If no completions are generated from the LLM stream.</p> </li> </ul> Note <ul> <li>If the node has a next_node defined and the final step is not a StopStep,   it will yield a SetNextNode step to continue the execution flow.</li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def generate_steps(\n    self, agent: Any, tape: Tape, llm_stream: LLMStream\n) -&gt; Generator[Step | PartialStep, None, None]:\n    \"\"\"\n    Generates a sequence of steps based on the LLM stream output.\n\n    This method processes the output from a language model stream and converts it into a series of steps.\n    It handles the parsing of completions and post-processing of steps.\n\n    Args:\n        agent (Any): The agent instance that will execute the steps.\n        tape (Tape): The tape object containing the execution context and history.\n        llm_stream (LLMStream): The stream of language model outputs to process.\n\n    Yields:\n        Union[Step, PartialStep]: Individual steps generated from the LLM stream output.\n\n    Raises:\n        FatalError: If no completions are generated from the LLM stream.\n\n    Note:\n        - If the node has a next_node defined and the final step is not a StopStep,\n          it will yield a SetNextNode step to continue the execution flow.\n    \"\"\"\n    new_steps = []\n    try:\n        cnt = 0\n        for event in llm_stream:\n            if event.output:\n                cnt += 1\n                assert event.output.content\n                for step in self.parse_completion(event.output.content, llm_stream.prompt.id):\n                    step = self.postprocess_step(tape, new_steps, step)\n                    new_steps.append(step)\n                    yield step\n        if not cnt:\n            raise FatalError(\"No completions!\")\n    except FatalError:\n        raise\n\n    if self.next_node and not isinstance(new_steps[-1], StopStep):\n        yield SetNextNode(next_node=self.next_node)\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.get_steps_description","title":"<code>get_steps_description(tape, agent)</code>","text":"<p>Get the steps description for the agent's task.</p> <p>This method returns the predefined steps prompt which describes the sequence of actions or steps that the agent should follow.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the context and state information.</p> </li> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent object that will execute the steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The steps prompt describing the sequence of actions.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def get_steps_description(self, tape: Tape, agent: Any) -&gt; str:\n    \"\"\"\n    Get the steps description for the agent's task.\n\n    This method returns the predefined steps prompt which describes the sequence of actions\n    or steps that the agent should follow.\n\n    Args:\n        tape (Tape): The tape object containing the context and state information.\n        agent (Any): The agent object that will execute the steps.\n\n    Returns:\n        str: The steps prompt describing the sequence of actions.\n    \"\"\"\n    return self.steps_prompt\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.make_llm_output","title":"<code>make_llm_output(agent, tape, index)</code>","text":"<p>Creates an LLMOutput from a sequence of steps in the tape that share the same prompt_id.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent instance associated with the output.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape containing the sequence of steps.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The starting index in the tape to process steps from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMOutput</code> (              <code>LLMOutput</code> )          \u2013            <p>An output object containing:</p> <ul> <li>role: Set to \"assistant\"</li> <li>content: JSON string of step data, formatted as either: a single dictionary if there is only one step, or a list of dictionaries</li> </ul> </li> </ul> Note <ul> <li>Only processes steps with matching prompt_id from the starting index</li> <li>Excludes SetNextNode steps from the output</li> <li>JSON content is formatted with indentation</li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def make_llm_output(self, agent: Any, tape: Tape, index: int) -&gt; LLMOutput:\n    \"\"\"\n    Creates an LLMOutput from a sequence of steps in the tape that share the same prompt_id.\n\n    Args:\n        agent (Any): The agent instance associated with the output.\n        tape (Tape): The tape containing the sequence of steps.\n        index (int): The starting index in the tape to process steps from.\n\n    Returns:\n        LLMOutput: An output object containing:\n\n            - role: Set to \"assistant\"\n            - content: JSON string of step data, formatted as either: a single dictionary\n            if there is only one step, or a list of dictionaries\n\n    Note:\n        - Only processes steps with matching prompt_id from the starting index\n        - Excludes SetNextNode steps from the output\n        - JSON content is formatted with indentation\n    \"\"\"\n    steps = []\n    i = index\n    first_prompt_id = tape.steps[i].metadata.prompt_id\n    while i &lt; len(tape) and tape.steps[i].metadata.prompt_id == first_prompt_id:\n        if not isinstance(tape.steps[i], SetNextNode):\n            steps.append(tape.steps[i])\n        i += 1\n\n    # if there is only one step, return it as a single dict, not a list\n    content = [step.llm_dict() for step in steps] if len(steps) &gt; 1 else steps[0].llm_dict()\n    return LLMOutput(role=\"assistant\", content=json.dumps(content, indent=2, ensure_ascii=False))\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.make_prompt","title":"<code>make_prompt(agent, tape)</code>","text":"<p>Create a prompt from tape interactions.</p> <p>This method constructs a prompt by processing the tape content and agent steps description into a format suitable for LLM consumption. It includes token count checks and tape trimming if needed to fit within context size limits.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Any</code>)           \u2013            <p>The agent object containing LLM configuration.</p> </li> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing interaction history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Prompt</code> (              <code>Prompt</code> )          \u2013            <p>A Prompt object containing formatted messages for LLM consumption.</p> </li> </ul> Note <p>The method performs the following steps:</p> <ol> <li>Cleans the tape content</li> <li>Gets steps description</li> <li>Converts tape to messages</li> <li>Checks token count and trims if needed</li> <li>Reconstructs messages if trimming occurred</li> </ol> Source code in <code>tapeagents/nodes.py</code> <pre><code>def make_prompt(self, agent: Any, tape: Tape) -&gt; Prompt:\n    \"\"\"Create a prompt from tape interactions.\n\n    This method constructs a prompt by processing the tape content and agent steps description\n    into a format suitable for LLM consumption. It includes token count checks and tape trimming\n    if needed to fit within context size limits.\n\n    Args:\n        agent (Any): The agent object containing LLM configuration.\n        tape (Tape): The tape object containing interaction history.\n\n    Returns:\n        Prompt: A Prompt object containing formatted messages for LLM consumption.\n\n    Note:\n        The method performs the following steps:\n\n        1. Cleans the tape content\n        2. Gets steps description\n        3. Converts tape to messages\n        4. Checks token count and trims if needed\n        5. Reconstructs messages if trimming occurred\n    \"\"\"\n    cleaned_tape = self.prepare_tape(tape)\n    steps_description = self.get_steps_description(tape, agent)\n    messages = self.tape_to_messages(cleaned_tape, steps_description)\n    if agent.llm.count_tokens(messages) &gt; (agent.llm.context_size - 500):\n        cleaned_tape = self.trim_tape(cleaned_tape)\n    messages = self.tape_to_messages(cleaned_tape, steps_description)\n    return Prompt(messages=messages)\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.parse_completion","title":"<code>parse_completion(llm_output, prompt_id)</code>","text":"<p>Parse LLM completion output into a sequence of agent steps.</p> <p>This method processes the LLM output string by parsing it as JSON and validating it against the agent step class schema. It handles both single step and multi-step outputs.</p> <p>Parameters:</p> <ul> <li> <code>llm_output</code>               (<code>str</code>)           \u2013            <p>The raw output string from the LLM to be parsed</p> </li> <li> <code>prompt_id</code>               (<code>str</code>)           \u2013            <p>Identifier for the prompt that generated this completion</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Step</code> (              <code>Step</code> )          \u2013            <p>Individual validated agent steps with prompt_id metadata</p> </li> <li> <code>LLMOutputParsingFailureAction</code> (              <code>Step</code> )          \u2013            <p>Error information if parsing or validation fails</p> </li> </ul> Note <p>All parsing errors are handled internally and yielded as LLMOutputParsingFailureAction objects.</p> Source code in <code>tapeagents/nodes.py</code> <pre><code>def parse_completion(self, llm_output: str, prompt_id: str) -&gt; Generator[Step, None, None]:\n    \"\"\"Parse LLM completion output into a sequence of agent steps.\n\n    This method processes the LLM output string by parsing it as JSON and validating it against\n    the agent step class schema. It handles both single step and multi-step outputs.\n\n    Args:\n        llm_output (str): The raw output string from the LLM to be parsed\n        prompt_id (str): Identifier for the prompt that generated this completion\n\n    Yields:\n        Step: Individual validated agent steps with prompt_id metadata\n        LLMOutputParsingFailureAction: Error information if parsing or validation fails\n\n    Note:\n        All parsing errors are handled internally and yielded as\n        LLMOutputParsingFailureAction objects.\n    \"\"\"\n    try:\n        step_dicts = json.loads(sanitize_json_completion(llm_output))\n        if isinstance(step_dicts, dict):\n            step_dicts = [step_dicts]\n    except Exception as e:\n        logger.exception(f\"Failed to parse LLM output as json: {llm_output}\\n\\nError: {e}\")\n        yield LLMOutputParsingFailureAction(error=f\"Failed to parse LLM output as json: {e}\", llm_output=llm_output)\n        return\n    try:\n        steps = [TypeAdapter(self.agent_step_cls).validate_python(step_dict) for step_dict in step_dicts]\n    except ValidationError as e:\n        err_text = \"\"\n        for err in e.errors():\n            loc = \".\".join([str(loc) for loc in err[\"loc\"]])\n            err_text += f\"{loc}: {err['msg']}\\n\"\n        logger.exception(f\"Failed to validate LLM output: {step_dicts}\\n\\nErrors:\\n{err_text}\")\n        yield LLMOutputParsingFailureAction(\n            error=f\"Failed to validate LLM output: {err_text}\", llm_output=llm_output\n        )\n        return\n    except Exception as e:\n        logger.exception(f\"Failed to parse LLM output dict: {step_dicts}\\n\\nError: {e}\")\n        yield LLMOutputParsingFailureAction(error=f\"Failed to parse LLM output dict: {e}\", llm_output=llm_output)\n        return\n    for step in steps:\n        step.metadata.prompt_id = prompt_id\n        yield step\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.postprocess_step","title":"<code>postprocess_step(tape, new_steps, step)</code>","text":"<p>Post-processes a step after its generation.</p> <p>By default returns the step unchanged.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape</p> </li> <li> <code>new_steps</code>               (<code>list[Step]</code>)           \u2013            <p>List of new steps that were generated during the current iteration</p> </li> <li> <code>step</code>               (<code>Step</code>)           \u2013            <p>The step that was just generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Step</code> (              <code>Step</code> )          \u2013            <p>The processed step, by default returns the original step unmodified</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def postprocess_step(self, tape: Tape, new_steps: list[Step], step: Step) -&gt; Step:\n    \"\"\"\n    Post-processes a step after its generation.\n\n    By default returns the step unchanged.\n\n    Args:\n        tape (Tape): The tape\n        new_steps (list[Step]): List of new steps that were generated during the current iteration\n        step (Step): The step that was just generated\n\n    Returns:\n        Step: The processed step, by default returns the original step unmodified\n    \"\"\"\n    return step\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.prepare_tape","title":"<code>prepare_tape(tape)</code>","text":"<p>Prepares tape by filtering out control flow steps.</p> <p>This method creates a new tape instance with only non-control flow steps, specifically excluding SetNextNode instances.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The input tape containing a sequence of steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tape</code> (              <code>Tape</code> )          \u2013            <p>A new tape instance containing only non-control flow steps.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def prepare_tape(self, tape: Tape) -&gt; Tape:\n    \"\"\"\n    Prepares tape by filtering out control flow steps.\n\n    This method creates a new tape instance with only non-control flow steps,\n    specifically excluding SetNextNode instances.\n\n    Args:\n        tape (Tape): The input tape containing a sequence of steps.\n\n    Returns:\n        Tape: A new tape instance containing only non-control flow steps.\n    \"\"\"\n    steps_without_control_flow = [step for step in tape.steps if not isinstance(step, SetNextNode)]\n    return tape.model_copy(update=dict(steps=steps_without_control_flow))\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.tape_to_messages","title":"<code>tape_to_messages(tape, steps_description)</code>","text":"<p>Converts a Tape object and steps description into a list of messages for LLM conversation.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>A Tape object containing conversation steps.</p> </li> <li> <code>steps_description</code>               (<code>str</code>)           \u2013            <p>A description of the conversation steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[dict]</code>           \u2013            <p>list[dict]: A list of dictionaries representing the conversation messages.        Each dictionary contains 'role' and 'content' keys.        Roles can be 'system', 'user', or 'assistant'.        The system prompt is always the first message.        If steps_description is provided, it's added as a user message.        Messages from tape are added with roles based on step type.        If guidance exists, it's added as the final user message.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def tape_to_messages(self, tape: Tape, steps_description: str) -&gt; list[dict]:\n    \"\"\"\n    Converts a Tape object and steps description into a list of messages for LLM conversation.\n\n    Args:\n        tape (Tape): A Tape object containing conversation steps.\n        steps_description (str): A description of the conversation steps.\n\n    Returns:\n        list[dict]: A list of dictionaries representing the conversation messages.\n                   Each dictionary contains 'role' and 'content' keys.\n                   Roles can be 'system', 'user', or 'assistant'.\n                   The system prompt is always the first message.\n                   If steps_description is provided, it's added as a user message.\n                   Messages from tape are added with roles based on step type.\n                   If guidance exists, it's added as the final user message.\n    \"\"\"\n    messages: list[dict] = [\n        {\"role\": \"system\", \"content\": self.system_prompt},\n    ]\n    if steps_description:\n        messages.append({\"role\": \"user\", \"content\": steps_description})\n    for step in tape:\n        role = \"assistant\" if isinstance(step, AgentStep) else \"user\"\n        messages.append({\"role\": role, \"content\": step.llm_view()})\n    if self.guidance:\n        messages.append({\"role\": \"user\", \"content\": self.guidance})\n    return messages\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.MonoNode.trim_tape","title":"<code>trim_tape(tape)</code>","text":"<p>Trims the tape by removing unnecessary positions.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object to be trimmed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tape</code> (              <code>Tape</code> )          \u2013            <p>The trimmed tape object.</p> </li> </ul> Note <p>Currently this is a placeholder method that returns the tape unchanged.</p> Source code in <code>tapeagents/nodes.py</code> <pre><code>def trim_tape(self, tape: Tape) -&gt; Tape:\n    \"\"\"\n    Trims the tape by removing unnecessary positions.\n\n    Args:\n        tape (Tape): The tape object to be trimmed.\n\n    Returns:\n        Tape: The trimmed tape object.\n\n    Note:\n        Currently this is a placeholder method that returns the tape unchanged.\n    \"\"\"\n    return tape\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.ObservationControlNode","title":"<code>ObservationControlNode</code>","text":"<p>               Bases: <code>ControlFlowNode</code></p> <p>A control flow node that selects the next node based on the last observation in the tape.</p> <p>This node examines the last observation in the tape and uses it to determine which node to execute next based on a mapping of observation types to node names.</p> <p>Attributes:</p> <ul> <li> <code>observation_to_node</code>               (<code>dict[Type, str]</code>)           \u2013            <p>Mapping of observation types to destination node names</p> </li> <li> <code>default_node</code>               (<code>str</code>)           \u2013            <p>Default node to jump to if no matching observation type is found</p> </li> </ul> Example <pre><code>node = ObservationControlNode(\n    observation_to_node={\n        SuccessObservation: \"success_node\",\n        ErrorObservation: \"error_node\"\n    },\n    default_node=\"fallback_node\"\n)\n</code></pre> <p>Methods:</p> <ul> <li> <code>select_node</code>             \u2013              <p>Selects the next node based on the type of the last observation in the tape.</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>class ObservationControlNode(ControlFlowNode):\n    \"\"\"\n    A control flow node that selects the next node based on the last observation in the tape.\n\n    This node examines the last observation in the tape and uses it to determine which node\n    to execute next based on a mapping of observation types to node names.\n\n    Attributes:\n        observation_to_node (dict[Type, str]): Mapping of observation types to destination node names\n        default_node (str): Default node to jump to if no matching observation type is found\n\n    Example:\n        ```python\n        node = ObservationControlNode(\n            observation_to_node={\n                SuccessObservation: \"success_node\",\n                ErrorObservation: \"error_node\"\n            },\n            default_node=\"fallback_node\"\n        )\n        ```\n    \"\"\"\n\n    observation_to_node: dict[Type, str] = {}\n    default_node: str = \"\"  # jump to the last node by default\n\n    def select_node(self, tape: Tape) -&gt; str:\n        \"\"\"\n        Selects the next node based on the type of the last observation in the tape.\n\n        Returns default_node if no observations exist or no matching type is found.\n\n        Args:\n            tape (Tape): The tape object containing the context and state\n\n        Returns:\n            str: The name of the next node to execute\n        \"\"\"\n        observations = [step for step in tape.steps if isinstance(step, Observation)]\n        last_observation = observations[-1] if observations else None\n        return self.observation_to_node.get(type(last_observation), self.default_node)\n</code></pre>"},{"location":"reference/nodes/#tapeagents.nodes.ObservationControlNode.select_node","title":"<code>select_node(tape)</code>","text":"<p>Selects the next node based on the type of the last observation in the tape.</p> <p>Returns default_node if no observations exist or no matching type is found.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the context and state</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The name of the next node to execute</p> </li> </ul> Source code in <code>tapeagents/nodes.py</code> <pre><code>def select_node(self, tape: Tape) -&gt; str:\n    \"\"\"\n    Selects the next node based on the type of the last observation in the tape.\n\n    Returns default_node if no observations exist or no matching type is found.\n\n    Args:\n        tape (Tape): The tape object containing the context and state\n\n    Returns:\n        str: The name of the next node to execute\n    \"\"\"\n    observations = [step for step in tape.steps if isinstance(step, Observation)]\n    last_observation = observations[-1] if observations else None\n    return self.observation_to_node.get(type(last_observation), self.default_node)\n</code></pre>"},{"location":"reference/observe/","title":"Observe","text":"<p>Functions to observe and store LLM calls and Tapes in a persistent storage</p> <p>Classes:</p> <ul> <li> <code>SQLiteWriterThread</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>init_sqlite_if_not_exists</code>             \u2013              <p>Ensure that the tables exist in the sqlite database.</p> </li> <li> <code>sqlite_store_llm_call</code>             \u2013              <p>Standalone function to store LLM calls.</p> </li> </ul>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread","title":"<code>SQLiteWriterThread</code>","text":"<p>Methods:</p> <ul> <li> <code>__enter__</code>             \u2013              <p>Start the SQLite queue writer when entering the context.</p> </li> <li> <code>__exit__</code>             \u2013              <p>Stop the SQLite queue writer when exiting the context.</p> </li> <li> <code>wait_for_empty</code>             \u2013              <p>Wait for the queue to be empty and all tasks to be processed.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>is_empty</code>               (<code>bool</code>)           \u2013            <p>Check if the queue is empty.</p> </li> <li> <code>queue</code>               (<code>Optional[Queue]</code>)           \u2013            <p>Access the write queue.</p> </li> </ul> Source code in <code>tapeagents/observe.py</code> <pre><code>class SQLiteWriterThread:\n    def __init__(self):\n        self.write_queue: Optional[queue.Queue] = None\n        self.writer_thread: Optional[threading.Thread] = None\n\n    def __enter__(self):\n        \"\"\"Start the SQLite queue writer when entering the context.\"\"\"\n        if self.write_queue is not None:\n            return self  # Already running\n\n        self.write_queue = queue.Queue()\n        self.writer_thread = threading.Thread(target=self._queue_sqlite_writer, daemon=True)\n        self.writer_thread.start()\n\n        # Set the global reference\n        global _WRITER_THREAD\n        _WRITER_THREAD = self\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Stop the SQLite queue writer when exiting the context.\"\"\"\n        if self.write_queue is not None and self.writer_thread is not None:\n            self.wait_for_empty()\n            self.write_queue.put(None)  # Signal thread to stop\n            self.writer_thread.join()  # Wait for thread to finish\n            self.write_queue = None\n            self.writer_thread = None\n\n            # Clear the global reference\n            global _WRITER_THREAD\n            _WRITER_THREAD = None\n\n    def _queue_sqlite_writer(self):\n        \"\"\"The worker function that processes the queue.\"\"\"\n        while True:\n            item = self.write_queue.get()\n            if item is None:  # Stop signal\n                break\n            sqlite_writer(item)\n            self.write_queue.task_done()\n\n    def wait_for_empty(self, timeout: Optional[float] = None) -&gt; bool:\n        \"\"\"Wait for the queue to be empty and all tasks to be processed.\"\"\"\n        if self.write_queue is None:\n            return True\n\n        try:\n            self.write_queue.join()\n            start_time = time.monotonic()\n            logger.info(\"Waiting for SQLite queue to empty...\")\n            while not self.write_queue.empty():\n                if timeout is not None:\n                    elapsed = time.monotonic() - start_time\n                    if elapsed &gt;= timeout:\n                        return False\n                time.sleep(0.1)\n                self.write_queue.join()\n            return True\n        except Exception as e:\n            logger.error(f\"Error while waiting for queue to empty: {e}\")\n            return False\n\n    @property\n    def queue(self) -&gt; Optional[queue.Queue]:\n        \"\"\"Access the write queue.\"\"\"\n        return self.write_queue\n\n    @property\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the queue is empty.\"\"\"\n        return self.write_queue is None or self.write_queue.empty()\n</code></pre>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread.is_empty","title":"<code>is_empty: bool</code>  <code>property</code>","text":"<p>Check if the queue is empty.</p>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread.queue","title":"<code>queue: Optional[queue.Queue]</code>  <code>property</code>","text":"<p>Access the write queue.</p>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread.__enter__","title":"<code>__enter__()</code>","text":"<p>Start the SQLite queue writer when entering the context.</p> Source code in <code>tapeagents/observe.py</code> <pre><code>def __enter__(self):\n    \"\"\"Start the SQLite queue writer when entering the context.\"\"\"\n    if self.write_queue is not None:\n        return self  # Already running\n\n    self.write_queue = queue.Queue()\n    self.writer_thread = threading.Thread(target=self._queue_sqlite_writer, daemon=True)\n    self.writer_thread.start()\n\n    # Set the global reference\n    global _WRITER_THREAD\n    _WRITER_THREAD = self\n    return self\n</code></pre>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Stop the SQLite queue writer when exiting the context.</p> Source code in <code>tapeagents/observe.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Stop the SQLite queue writer when exiting the context.\"\"\"\n    if self.write_queue is not None and self.writer_thread is not None:\n        self.wait_for_empty()\n        self.write_queue.put(None)  # Signal thread to stop\n        self.writer_thread.join()  # Wait for thread to finish\n        self.write_queue = None\n        self.writer_thread = None\n\n        # Clear the global reference\n        global _WRITER_THREAD\n        _WRITER_THREAD = None\n</code></pre>"},{"location":"reference/observe/#tapeagents.observe.SQLiteWriterThread.wait_for_empty","title":"<code>wait_for_empty(timeout=None)</code>","text":"<p>Wait for the queue to be empty and all tasks to be processed.</p> Source code in <code>tapeagents/observe.py</code> <pre><code>def wait_for_empty(self, timeout: Optional[float] = None) -&gt; bool:\n    \"\"\"Wait for the queue to be empty and all tasks to be processed.\"\"\"\n    if self.write_queue is None:\n        return True\n\n    try:\n        self.write_queue.join()\n        start_time = time.monotonic()\n        logger.info(\"Waiting for SQLite queue to empty...\")\n        while not self.write_queue.empty():\n            if timeout is not None:\n                elapsed = time.monotonic() - start_time\n                if elapsed &gt;= timeout:\n                    return False\n            time.sleep(0.1)\n            self.write_queue.join()\n        return True\n    except Exception as e:\n        logger.error(f\"Error while waiting for queue to empty: {e}\")\n        return False\n</code></pre>"},{"location":"reference/observe/#tapeagents.observe.init_sqlite_if_not_exists","title":"<code>init_sqlite_if_not_exists(only_once=True)</code>","text":"<p>Ensure that the tables exist in the sqlite database.</p> <p>This is only done once per Python process. If you want to change the SQLite path during at run time, you can run this function manually with only_once=False.</p> Source code in <code>tapeagents/observe.py</code> <pre><code>def init_sqlite_if_not_exists(only_once: bool = True):\n    \"\"\"\n    Ensure that the tables exist in the sqlite database.\n\n    This is only done once per Python process.\n    If you want to change the SQLite path during at run time, you can run this function manually\n    with only_once=False.\n\n    \"\"\"\n    global _checked_sqlite\n    if _checked_sqlite and only_once:\n        return\n\n    path = sqlite_db_path()\n    logger.info(f\"use SQLite db at {path}\")\n    conn = sqlite3.connect(path)\n    cursor = conn.cursor()\n    cursor.execute(\"\"\"               \n    CREATE TABLE IF NOT EXISTS LLMCalls (\n        prompt_id TEXT PRIMARY KEY,\n        timestamp TEXT,\n        prompt TEXT,\n        output TEXT,\n        prompt_length_tokens INTEGER,\n        output_length_tokens INTEGER,\n        cached INTEGER\n    )\n    \"\"\")\n    # now create tape table with tape_id index and data column\n    cursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS Tapes (\n        tape_id TEXT PRIMARY KEY,\n        timestamp TEXT,\n        length INTEGER,\n        metadata TEXT,\n        context TEXT,\n        steps TEXT\n    )\n    \"\"\")\n    cursor.close()\n    _checked_sqlite = True\n</code></pre>"},{"location":"reference/observe/#tapeagents.observe.sqlite_store_llm_call","title":"<code>sqlite_store_llm_call(call)</code>","text":"<p>Standalone function to store LLM calls.</p> <p>Will use the queue if available (within context manager), otherwise falls back to single-threaded mode.</p> Source code in <code>tapeagents/observe.py</code> <pre><code>def sqlite_store_llm_call(call: LLMCall):\n    \"\"\"Standalone function to store LLM calls.\n\n    Will use the queue if available (within context manager),\n    otherwise falls back to single-threaded mode.\n    \"\"\"\n    if _WRITER_THREAD is not None and _WRITER_THREAD.queue is not None:\n        # We're in a context manager, use the queue\n        logger.debug(\"Using SQLite queue writing mode\")\n        _WRITER_THREAD.queue.put(call)\n    else:\n        # We're not in a context manager, use single-threaded mode\n        logger.debug(\"Using single-threaded SQLite writing mode\")\n        sqlite_writer(call)\n</code></pre>"},{"location":"reference/orchestrator/","title":"Orchestrator","text":"<p>Module contains the main loops of the agent-environment interaction and replay functions.</p> <p>Classes:</p> <ul> <li> <code>MainLoopStream</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>main_loop</code>             \u2013              <p>Main loop of the agent-environment interaction. The agent is run on the tape, then the environment reacts to the</p> </li> <li> <code>replay_tape</code>             \u2013              <p>Replay the tape with the agent and compare the steps with the old tape.</p> </li> <li> <code>replay_tapes</code>             \u2013              <p>Validate the list of tapes with the agent and environment.</p> </li> </ul>"},{"location":"reference/orchestrator/#tapeagents.orchestrator.MainLoopStream","title":"<code>MainLoopStream</code>","text":"<p>               Bases: <code>Generic[TapeType]</code></p> <p>Methods:</p> <ul> <li> <code>get_final_tape</code>             \u2013              <p>Return the last tape by either the agent or the environment.</p> </li> </ul> Source code in <code>tapeagents/orchestrator.py</code> <pre><code>class MainLoopStream(Generic[TapeType]):\n    def __init__(self, generator: Generator[MainLoopEvent[TapeType], None, None]):\n        self.generator = generator\n\n    def __bool__(self):\n        return self.generator is not None\n\n    def __iter__(self):\n        if self.generator is None:\n            raise ValueError(\"can't iterate a null stream\")\n        return self\n\n    def __next__(self) -&gt; MainLoopEvent:\n        if self.generator is None:\n            raise StopIteration\n        return next(self.generator)\n\n    def agent_events(self) -&gt; Generator[AgentEvent[TapeType], None, None]:\n        for event in self:\n            if event.agent_event:\n                yield event.agent_event\n\n    def get_final_tape(self) -&gt; TapeType:\n        \"\"\"Return the last tape by either the agent or the environment.\"\"\"\n        last_final_tape = None\n        for event in self:\n            if event.agent_tape:\n                last_final_tape = event.agent_tape\n            if event.env_tape:\n                last_final_tape = event.env_tape\n        if last_final_tape is not None:\n            return last_final_tape\n        raise ValueError(\"No tape by either the agent or the environment\")\n</code></pre>"},{"location":"reference/orchestrator/#tapeagents.orchestrator.MainLoopStream.get_final_tape","title":"<code>get_final_tape()</code>","text":"<p>Return the last tape by either the agent or the environment.</p> Source code in <code>tapeagents/orchestrator.py</code> <pre><code>def get_final_tape(self) -&gt; TapeType:\n    \"\"\"Return the last tape by either the agent or the environment.\"\"\"\n    last_final_tape = None\n    for event in self:\n        if event.agent_tape:\n            last_final_tape = event.agent_tape\n        if event.env_tape:\n            last_final_tape = event.env_tape\n    if last_final_tape is not None:\n        return last_final_tape\n    raise ValueError(\"No tape by either the agent or the environment\")\n</code></pre>"},{"location":"reference/orchestrator/#tapeagents.orchestrator.main_loop","title":"<code>main_loop(agent, start_tape, environment, max_loops=-1)</code>","text":"<p>Main loop of the agent-environment interaction. The agent is run on the tape, then the environment reacts to the agent's tape, then the agent is run on the environment's tape, and so on. The loop stops when the agent emits a final step or the environment emits a final step, or the maximum number of loops is reached.</p> <p>:param agent: Agent object :param start_tape: initial tape :param environment: Environment object :param max_loops: maximum number of loops, -1 for infinite</p> <p>:return: generator of MainLoopEvent objects</p> Source code in <code>tapeagents/orchestrator.py</code> <pre><code>def main_loop(\n    agent: Agent[TapeType],\n    start_tape: TapeType,\n    environment: Environment,\n    max_loops: int = -1,\n) -&gt; MainLoopStream[TapeType]:\n    \"\"\"\n    Main loop of the agent-environment interaction. The agent is run on the tape, then the environment reacts to the\n    agent's tape, then the agent is run on the environment's tape, and so on.\n    The loop stops when the agent emits a final step or the environment emits a final step,\n    or the maximum number of loops is reached.\n\n    :param agent: Agent object\n    :param start_tape: initial tape\n    :param environment: Environment object\n    :param max_loops: maximum number of loops, -1 for infinite\n\n    :return: generator of MainLoopEvent objects\n    \"\"\"\n\n    def _implementation():\n        n_loops = 0\n        tape = start_tape\n        event = None\n        while n_loops &lt; max_loops or max_loops == -1:\n            # --- RUN THE AGENT ---\n            for event in agent.run(tape):\n                yield MainLoopEvent(agent_event=event)\n                if event.step:\n                    logger.debug(colored(f\"AGENT: {step_view(event.step)}\", \"green\"))\n                if event.final_tape:\n                    break\n            assert event and event.final_tape\n            agent_tape = event.final_tape\n            yield MainLoopEvent(agent_tape=agent_tape)\n\n            # --- RUN THE ENVIRONMENT ---\n            if isinstance(agent_tape.steps[-1], StopStep):\n                logger.debug(f\"Agent emitted final step {agent_tape.steps[-1]}\")\n                yield MainLoopEvent(status=MainLoopStatus.FINISHED)\n                return\n            try:\n                tape = environment.react(agent_tape)\n            except NoActionsToReactTo:\n                yield MainLoopEvent(status=MainLoopStatus.NO_ACTIONS)\n                return\n            except ExternalObservationNeeded:\n                yield MainLoopEvent(status=MainLoopStatus.EXTERNAL_INPUT_NEEDED)\n                return\n            for observation in tape[len(agent_tape) :]:\n                logger.debug(colored(f\"ENV: {step_view(observation, trim=True)}\", \"yellow\"))\n                yield MainLoopEvent(observation=observation)\n            yield MainLoopEvent[TapeType](env_tape=tape)\n\n            # --- REPEAT ---\n            n_loops += 1\n\n    return MainLoopStream(_implementation())\n</code></pre>"},{"location":"reference/orchestrator/#tapeagents.orchestrator.replay_tape","title":"<code>replay_tape(agent, tape, env=None, start_tape=None, reuse_observations=False, stop_on_mismatch=True)</code>","text":"<p>Replay the tape with the agent and compare the steps with the old tape. Count mismatches and print diffs of them.</p> <p>:param agent: Agent object :param tape: Old tape object :param env: Environment object :param start_tape: initial tape, if None, the first observations of the tape are used :param reuse_observations: reuse observations from the tape instead of calling the environment :param stop_on_mismatch: stop the replay on the first mismatch</p> <p>:return: True if all steps match, False otherwise</p> Source code in <code>tapeagents/orchestrator.py</code> <pre><code>def replay_tape(\n    agent: Agent[TapeType],\n    tape: TapeType,\n    env: Environment[TapeType] | None = None,\n    start_tape: TapeType | None = None,\n    reuse_observations: bool = False,\n    stop_on_mismatch: bool = True,\n) -&gt; bool:\n    \"\"\"\n    Replay the tape with the agent and compare the steps with the old tape.\n    Count mismatches and print diffs of them.\n\n    :param agent: Agent object\n    :param tape: Old tape object\n    :param env: Environment object\n    :param start_tape: initial tape, if None, the first observations of the tape are used\n    :param reuse_observations: reuse observations from the tape instead of calling the environment\n    :param stop_on_mismatch: stop the replay on the first mismatch\n\n    :return: True if all steps match, False otherwise\n    \"\"\"\n    if env is None and not reuse_observations:\n        raise ValueError(\"Environment is required when not reusing observations\")\n    match: bool = True\n    if start_tape is None:\n        start_steps: list[Step] = []\n        for step in tape.steps:\n            if isinstance(step, Observation):\n                start_steps.append(step)\n            else:\n                break\n        start_tape = tape.model_copy(update=dict(steps=start_steps))\n\n    event = None\n    new_tape: TapeType = start_tape\n    new_steps_count = len(start_tape)\n    while new_steps_count &lt; len(tape):\n        for event in agent.run(new_tape):\n            if event.step:\n                step_dict = event.step.llm_dict()\n                if new_steps_count &gt;= len(tape.steps):\n                    logger.error(f\"Extra step {new_steps_count} from agent, kind: {step_dict.get('kind')}\")\n                    match = False\n                    if stop_on_mismatch:\n                        return False\n                    break\n                old_step_dict = tape.steps[new_steps_count].llm_dict()\n                new_steps_count += 1\n                kind = step_dict.get(\"kind\")\n                old_kind = old_step_dict.get(\"kind\")\n                if kind != old_kind:\n                    logger.error(\n                        f\"Step {new_steps_count} kind mismatch: Old {old_kind}, New {kind}\\nOld step: {old_step_dict}\\nNew step: {step_dict}\"\n                    )\n                    match = False\n                    if stop_on_mismatch:\n                        return False\n                elif old_step_dict != step_dict:\n                    logger.error(f\"Step {new_steps_count} mismatch\")\n                    logger.error(diff_dicts(old_step_dict, step_dict))\n                    match = False\n                    if stop_on_mismatch:\n                        return False\n                else:\n                    logger.debug(f\"Step {new_steps_count} ok\")\n            if event.final_tape:\n                break\n        assert event and event.final_tape\n        agent_tape = event.final_tape\n        new_tape = agent_tape\n        if isinstance(new_tape.steps[-1], StopStep):\n            logger.info(\"Agent emitted final step, stop\")\n            break\n\n        if reuse_observations:\n            observations: list[Step] = []\n            for step in tape.steps[new_steps_count:]:\n                if isinstance(step, Observation):\n                    observations.append(step)\n                else:\n                    break\n            if len(observations):\n                logger.debug(f\"Reusing {len(observations)} observations from tape\")\n            new_tape = agent_tape + observations\n            new_steps_count += len(observations)\n        else:\n            assert env is not None\n            new_tape = env.react(agent_tape)\n            observations = new_tape.steps[len(agent_tape) :]\n            for observation in observations:\n                step_dict = observation.llm_dict()\n                old_step_dict = tape.steps[new_steps_count].llm_dict()\n                new_steps_count += 1\n                if old_step_dict != step_dict:\n                    logger.error(f\"Observation {new_steps_count} mismatch\")\n                    logger.error(diff_dicts(old_step_dict, step_dict))\n                    match = False\n                    if stop_on_mismatch:\n                        return False\n                else:\n                    logger.debug(f\"Observation {new_steps_count} ok\")\n\n                if isinstance(observation, StopStep):\n                    logger.info(f\"Environment emitted final step {observation}\")\n                    break\n        if isinstance(new_tape.steps[-1], StopStep):\n            logger.info(\"Env emitted final step, stop\")\n            break\n    if new_steps_count != len(tape.steps):\n        logger.error(f\"New tape has {new_steps_count} steps, old tape has {len(tape.steps)}\")\n        match = False\n    return match\n</code></pre>"},{"location":"reference/orchestrator/#tapeagents.orchestrator.replay_tapes","title":"<code>replay_tapes(agent, tapes, env=None, reuse_observations=False, pause_on_error=False)</code>","text":"<p>Validate the list of tapes with the agent and environment. Check that the agent produce exactly the same steps as the original ones. Returns the number of failed tapes.</p> Source code in <code>tapeagents/orchestrator.py</code> <pre><code>def replay_tapes(\n    agent: Agent[TapeType],\n    tapes: list[TapeType],\n    env: Environment[TapeType] | None = None,\n    reuse_observations: bool = False,\n    pause_on_error: bool = False,\n) -&gt; int:\n    \"\"\"\n    Validate the list of tapes with the agent and environment.\n    Check that the agent produce exactly the same steps as the original ones.\n    Returns the number of failed tapes.\n    \"\"\"\n    ok = 0\n    fails = 0\n    for i, tape in enumerate(tapes):\n        logger.debug(f\"Tape {i}\")\n        try:\n            matched = replay_tape(agent, tape, env, reuse_observations=reuse_observations)\n            if not matched:\n                raise FatalError(\"Tape mismatch\")\n            ok += 1\n        except FatalError as f:\n            logger.error(colored(f\"Fatal error: {f}, skip tape\", \"red\"))\n            fails += 1\n            if pause_on_error:\n                input(\"Press Enter to continue...\")\n\n        logger.debug(colored(f\"Ok: {ok}, Fails: {fails}\", \"green\"))\n    return fails\n</code></pre>"},{"location":"reference/parallel_processing/","title":"Parallel Processing","text":"<p>Functions for parallel processing of agent workers.</p> <p>Functions:</p> <ul> <li> <code>lazy_thread_pool_processor</code>             \u2013              <p>Processes a stream of items in a thread pool with pull semantics.</p> </li> <li> <code>process_pool_processor</code>             \u2013              <p>Processes a stream of items using a process pool with push semantics.</p> </li> </ul>"},{"location":"reference/parallel_processing/#tapeagents.parallel_processing.lazy_thread_pool_processor","title":"<code>lazy_thread_pool_processor(stream, worker_func, n_workers, initializer=None, initargs=(), futures_queue_timeout=1.0, producer_thread_shutdown_timeout=10.0)</code>","text":"<p>Processes a stream of items in a thread pool with pull semantics.</p> <p>If n_workers is set to 0, the items are processed using a simple loop without threading (for debugging purposes).</p> <p>Tasks from the input stream are submitted to the thread pool only when the main generator is asked for a value. This ensures tasks are executed on-demand, as their results are consumed.</p> <p>Parameters:</p> <ul> <li> <code>stream</code>               (<code>Generator[InputType, None, None]</code>)           \u2013            <p>InputType generator that yields items to process.</p> </li> <li> <code>n_workers</code>               (<code>int</code>)           \u2013            <p>The number of worker threads to use.</p> </li> <li> <code>worker_func</code>               (<code>Callable[[InputType], OutputType]</code>)           \u2013            <p>The function to process each item.</p> </li> <li> <code>initializer</code>               (<code>Optional[Callable[..., None]]</code>, default:                   <code>None</code> )           \u2013            <p>An optional initializer function to call for each worker thread.</p> </li> <li> <code>initargs</code>               (<code>tuple[Any, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Arguments for the initializer.</p> </li> <li> <code>futures_queue_timeout</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>The timeout for the futures queue (default: 1.).</p> </li> <li> <code>producer_thread_shutdown_timeout</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The timeout for the producer thread (default: 10.).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>OutputType | Exception</code>           \u2013            <p>OutputType | Exception: The processed result for each input item, or an exception raised during processing of the corresponding input item.</p> </li> </ul> Source code in <code>tapeagents/parallel_processing.py</code> <pre><code>def lazy_thread_pool_processor(\n    stream: Iterable[InputType],\n    worker_func: Callable[[InputType], OutputType],\n    n_workers: int,\n    initializer: None | Callable[..., None] = None,\n    initargs: tuple[Any, ...] = (),\n    futures_queue_timeout: float = 1.0,\n    producer_thread_shutdown_timeout: float = 10.0,\n) -&gt; Generator[OutputType | Exception, None, None]:\n    \"\"\"\n    Processes a stream of items in a thread pool with pull semantics.\n\n    If n_workers is set to 0, the items are processed using a simple loop\n    without threading (for debugging purposes).\n\n    Tasks from the input stream are submitted to the thread pool only when\n    the main generator is asked for a value. This ensures tasks are executed\n    on-demand, as their results are consumed.\n\n    Args:\n        stream (Generator[InputType, None, None]): InputType generator that yields items to process.\n        n_workers (int): The number of worker threads to use.\n        worker_func (Callable[[InputType], OutputType]): The function to process each item.\n        initializer (Optional[Callable[..., None]]): An optional initializer function\n            to call for each worker thread.\n        initargs (tuple[Any, ...]): Arguments for the initializer.\n        futures_queue_timeout (float): The timeout for the futures queue (default: 1.).\n        producer_thread_shutdown_timeout (float): The timeout for the producer thread (default: 10.).\n\n    Yields:\n        OutputType | Exception: The processed result for each input item, or an exception raised during processing\n            of the corresponding input item.\n    \"\"\"\n    if n_workers == 0:\n        # Process items using a simple loop without threading\n        yield from sequential_processor(stream, worker_func, 1, initializer, initargs)\n    else:\n        with ThreadPoolExecutor(\n            max_workers=n_workers,\n            initializer=initializer,\n            initargs=initargs,\n        ) as executor:\n            # Bounded queue to store futures\n            futures_queue: Queue[Future[OutputType]] = Queue(maxsize=n_workers)\n\n            # Events for inter-thread communication\n            producer_done_event = Event()\n            consumer_stopped_event = Event()\n\n            # Producer thread function\n            def producer():\n                for item in stream:\n                    if consumer_stopped_event.is_set():\n                        return\n                    # Submit task and add to the queue\n                    future = executor.submit(worker_func, item)\n                    # Use put with a timeout to periodically check consumer_stopped_event\n                    while True:\n                        try:\n                            futures_queue.put(future, timeout=futures_queue_timeout)\n                            break\n                        except Full:\n                            if consumer_stopped_event.is_set():\n                                return\n                producer_done_event.set()\n\n            producer_thread = Thread(target=producer)\n            producer_thread.start()\n\n            try:\n                # Main (consumer) thread\n                producer_was_done_before_get = None\n                while True:\n                    try:\n                        # Get the next completed future\n                        producer_was_done_before_get = producer_done_event.is_set()\n                        future = futures_queue.get_nowait()\n                        try:\n                            yield future.result()\n                        except Exception as e:\n                            yield e\n                    except Empty:\n                        # If queue is empty and producer was done before we checked the queue,\n                        # break out of loop\n                        if producer_was_done_before_get:\n                            break\n\n            finally:\n                # If consumer stops early, signal the producer to stop\n                consumer_stopped_event.set()\n                producer_thread.join(timeout=producer_thread_shutdown_timeout)\n\n                if producer_thread.is_alive():\n                    raise RuntimeError(\"Producer thread is still alive after timeout\")\n</code></pre>"},{"location":"reference/parallel_processing/#tapeagents.parallel_processing.process_pool_processor","title":"<code>process_pool_processor(stream, worker_func, n_workers, initializer=None, initargs=(), keep_order=False)</code>","text":"<p>Processes a stream of items using a process pool with push semantics.</p> <p>If n_workers is set to 0, the items are processed using a simple loop without multiprocessing (for debugging purposes).</p> <p>This function submits all tasks from the input stream to a process pool for concurrent processing. Unlike a thread pool, a process pool uses multiple processes, which can provide true parallelism especially useful for CPU-bound tasks. The tasks are executed immediately and run to completion irrespective of whether their results are consumed.</p> <p>Parameters:</p> <ul> <li> <code>stream</code>               (<code>Generator[InputType, None, None]</code>)           \u2013            <p>A generator that yields items to process.</p> </li> <li> <code>n_workers</code>               (<code>int</code>)           \u2013            <p>The number of worker processes to use.</p> </li> <li> <code>worker_func</code>               (<code>Callable[[InputType], OutputType]</code>)           \u2013            <p>The function to process each item. This function should be picklable since it will be passed across process boundaries.</p> </li> <li> <code>initializer</code>               (<code>None | Callable[..., None]</code>, default:                   <code>None</code> )           \u2013            <p>An optional initializer function to call for each worker process. Useful for setting up process-specific resources.</p> </li> <li> <code>initargs</code>               (<code>tuple[Any, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to pass to the initializer.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>OutputType | Exception</code>           \u2013            <p>OutputType | Exception: The processed result for each input item, or an exception raised during processing of the corresponding input item. Results are yielded as they become available.</p> </li> </ul> Note <p>Since this uses multiple processes, the <code>worker_func</code> and the items in the stream should not rely on shared state with the main process unless that state is safe to share across processes (like using multiprocessing-safe constructs).</p> Source code in <code>tapeagents/parallel_processing.py</code> <pre><code>def process_pool_processor(\n    stream: Iterable[InputType],\n    worker_func: Callable[[InputType], OutputType],\n    n_workers: int,\n    initializer: None | Callable[..., None] = None,\n    initargs: tuple[Any, ...] = (),\n    keep_order=False,\n) -&gt; Generator[OutputType | Exception, None, None]:\n    \"\"\"\n    Processes a stream of items using a process pool with push semantics.\n\n    If n_workers is set to 0, the items are processed using a simple loop\n    without multiprocessing (for debugging purposes).\n\n    This function submits all tasks from the input stream to a process pool\n    for concurrent processing. Unlike a thread pool, a process pool uses\n    multiple processes, which can provide true parallelism especially useful\n    for CPU-bound tasks. The tasks are executed immediately and run to\n    completion irrespective of whether their results are consumed.\n\n    Args:\n        stream (Generator[InputType, None, None]): A generator that yields items to process.\n        n_workers (int): The number of worker processes to use.\n        worker_func (Callable[[InputType], OutputType]): The function to process each item. This\n            function should be picklable since it will be passed across process boundaries.\n        initializer (None | Callable[..., None]): An optional initializer function\n            to call for each worker process. Useful for setting up process-specific resources.\n        initargs (tuple[Any, ...]): Arguments to pass to the initializer.\n\n    Yields:\n        OutputType | Exception: The processed result for each input item, or an exception raised during processing\n            of the corresponding input item. Results are yielded as they become available.\n\n    Note:\n        Since this uses multiple processes, the `worker_func` and the items in the stream\n        should not rely on shared state with the main process unless that state is safe to\n        share across processes (like using multiprocessing-safe constructs).\n    \"\"\"\n    if n_workers == 0:\n        # Process items using a simple loop without threading\n        yield from sequential_processor(stream, worker_func, 1, initializer, initargs)\n    else:\n        with ProcessPoolExecutor(\n            max_workers=n_workers,\n            initializer=initializer,\n            initargs=initargs,\n        ) as executor:\n            futures: Iterable[Future[OutputType]] = [executor.submit(worker_func, a) for a in stream]\n            if not keep_order:\n                futures = as_completed(futures)\n            for future in futures:\n                try:\n                    yield future.result()\n                except Exception as e:\n                    yield e\n</code></pre>"},{"location":"reference/prompting/","title":"Prompting","text":"<p>Utilities for converting between tape steps and LLM messages.</p> <p>Functions:</p> <ul> <li> <code>tape_to_messages</code>             \u2013              <p>The default way of representing tape steps as LLM messages.</p> </li> </ul>"},{"location":"reference/prompting/#tapeagents.prompting.tape_to_messages","title":"<code>tape_to_messages(tape, agent=None)</code>","text":"<p>The default way of representing tape steps as LLM messages.</p> Source code in <code>tapeagents/prompting.py</code> <pre><code>def tape_to_messages(tape: Tape, agent: Agent | None = None) -&gt; list[dict]:\n    \"\"\"The default way of representing tape steps as LLM messages.\"\"\"\n    messages = []\n    for step in tape.steps:\n        if isinstance(step, (Pass, SetNextNode)):\n            continue\n        llm_message = step_to_message(step, agent)\n        messages.append(llm_message)\n    return messages\n</code></pre>"},{"location":"reference/studio/","title":"Studio","text":""},{"location":"reference/tape_browser/","title":"Tape Browser","text":"<p>GUI for browsing tapes.</p> <p>Classes:</p> <ul> <li> <code>TapeBrowser</code>           \u2013            <p>Base class for tape browser GUI.</p> </li> </ul>"},{"location":"reference/tape_browser/#tapeagents.tape_browser.TapeBrowser","title":"<code>TapeBrowser</code>","text":"<p>Base class for tape browser GUI. Displays the list of tapes from the files in the folder, allows to select the file with a bunch of tapes and then select tape and view it. Allows to navigate between parent and children tapes by links.</p> Source code in <code>tapeagents/tape_browser.py</code> <pre><code>class TapeBrowser:\n    \"\"\"\n    Base class for tape browser GUI.\n    Displays the list of tapes from the files in the folder, allows to select the file with a bunch of tapes\n    and then select tape and view it. Allows to navigate between parent and children tapes by links.\n    \"\"\"\n\n    def __init__(\n        self,\n        tape_cls: type[BaseModel],\n        tapes_folder: str,\n        renderer: BasicRenderer,\n        file_extension: str = \".yaml\",\n    ):\n        self.tape_cls = tape_cls\n        self.file_extension: str = file_extension\n        self.tapes_folder: str = tapes_folder\n        self.renderer: BasicRenderer = renderer\n        self.files: list[str] = []\n        self.tape_index: dict[str, tuple[int, int]] = {}\n        self.tape_children: dict[str, list[str]] = {}\n        self.request: gr.Request | None = None\n        self.selected_tape: int = 0\n        self.tapes: list[Tape] = []\n        self.llm_calls: dict = {}\n\n    def load_tapes(self, fname: str) -&gt; list[Tape]:\n        fpath = os.path.join(self.tapes_folder, fname)\n        try:\n            tapes = load_tapes(self.tape_cls, fpath)\n            logger.info(f\"{len(tapes)} tapes loaded from {fname}\")\n        except:\n            logger.error(f\"Could not load tapes from {fpath}\")\n            raise\n        return tapes\n\n    def load_llm_calls(self):\n        logger.info(\"Loading LLM calls\")\n        self.llm_calls = retrieve_tape_llm_calls(self.tapes)\n        logger.info(f\"Loaded {len(self.llm_calls)} LLM calls\")\n\n    def get_steps(self, tape: Tape) -&gt; list:\n        return tape.steps\n\n    def get_context(self, tape: Tape) -&gt; list:\n        return getattr(tape.context, \"steps\", [])\n\n    def get_tape_files(self) -&gt; list[str]:\n        files = sorted([f for f in os.listdir(self.tapes_folder) if f.endswith(self.file_extension)])\n        assert files, f\"No files found in {self.tapes_folder}\"\n        logger.info(f\"{len(files)} files found in {self.tapes_folder}\")\n        indexed = 0\n        nonempty_files = []\n        for i, file in enumerate(files):\n            if file in self.files:\n                continue  # already indexed\n            tapes = self.load_tapes(file)\n            if not len(tapes):\n                logger.warning(f\"File {file} does not contain any known tapes, skip\")\n                continue\n            for j, tape in enumerate(tapes):\n                tape_id = tape.metadata.id\n                parent_id = tape.metadata.parent_id\n                if tape_id:\n                    if tape_id in self.tape_index and self.tape_index[tape_id] != (i, j):\n                        raise ValueError(\n                            f\"Duplicate tape id {tape_id}. Both in {self.tape_index[tape_id]} and {(i, j)}\"\n                        )\n                    indexed += 1\n                    self.tape_index[tape_id] = (i, j)\n                    if parent_id:\n                        if parent_id not in self.tape_children:\n                            self.tape_children[parent_id] = []\n                        self.tape_children[parent_id].append(tape_id)\n            nonempty_files.append(file)\n        logger.info(f\"Indexed {indexed} new tapes, index size: {len(self.tape_index)}\")\n        logger.info(f\"{len(self.tape_children)} tapes with children found\")\n        return nonempty_files\n\n    def get_tape_label(self, tape: Tape) -&gt; str:\n        label = \"\"\n        parent_tape_id = tape.metadata.parent_id\n        tape_id = tape.metadata.id\n        if tape_id:\n            label += f\"&lt;br&gt;&lt;b&gt;ID: {tape_id}&lt;/b&gt;\"\n        if parent_tape_id:\n            if parent_tape_id in self.tape_index:\n                logger.info(f\"Tape index value {self.tape_index[parent_tape_id]}\")\n                fid, tid = self.tape_index[parent_tape_id]\n                logger.info(f\"Parent tape {parent_tape_id} found in {self.files[fid]}\")\n                tape_name = f\"{self.files[fid]}/{tid}\"\n                label += f'&lt;br&gt;Parent Tape: &lt;a href=\"?tape_id={parent_tape_id}\"&gt;{tape_name}&lt;/a&gt;'\n            else:\n                label += f\"&lt;br&gt;Parent tape: {parent_tape_id}\"\n        if tape_id and tape_id in self.tape_children:\n            children = []\n            for cid in self.tape_children[tape_id]:\n                tape_name = f\"{self.files[self.tape_index[cid][0]]}/{self.tape_index[cid][1]}\"\n                children.append(f'&lt;div style=\"margin-left:2em;\"&gt;&lt;a href=\"?tape_id={cid}\"&gt;{tape_name}&lt;/a&gt;&lt;/div&gt;')\n            label += f\"&lt;br&gt;Children tapes:&lt;br&gt;{''.join(children)}\"\n        label += f\"&lt;br&gt;Length: {len(tape)} steps\"\n        if tape.metadata:\n            m = {k: v for k, v in tape.metadata.model_dump().items() if k not in [\"id\", \"parent_id\"]}\n            label += f'&lt;h3&gt;Metadata&lt;/h3&gt;&lt;div style=\"white-space: pre-wrap;\"&gt;{yaml.dump(m, allow_unicode=True)}&lt;/div&gt;'\n        return label\n\n    def get_file_label(self, filename: str, tapes: list[Tape]) -&gt; str:\n        tapelengths = [len(tape) for tape in tapes]\n        tapelen = sum(tapelengths) / len(tapelengths)\n        return f\"&lt;h3&gt;{len(self.tape_index)} indexed tapes in {len(self.files)} files&lt;br&gt;{len(tapes)} tapes in the current file&lt;br&gt;Avg. tape length: {tapelen:.1f} steps&lt;/h3&gt;\"\n\n    def get_tape_name(self, i: int, tape: Tape) -&gt; str:\n        return f\"Tape {i}\"\n\n    def update_view(self, selected_file: str):\n        logger.info(f\"Loading tapes from {selected_file}\")\n        self.tapes = self.load_tapes(selected_file)\n        self.load_llm_calls()\n        file_label = self.get_file_label(selected_file, self.tapes)\n        tape_names = [(self.get_tape_name(i, tape), i) for i, tape in enumerate(self.tapes)]\n        logger.info(f\"Selected file: {selected_file}, selected tape: {self.selected_tape}\")\n        files = gr.Dropdown(self.files, label=\"File\", value=selected_file)  # type: ignore\n        tape_names = gr.Dropdown(tape_names, label=\"Tape\", value=self.selected_tape)  # type: ignore\n        tape_html, label = self.update_tape_view(self.selected_tape)\n        return files, tape_names, file_label, tape_html, label\n\n    def update_tape_view(self, tape_id: int) -&gt; tuple[str, str]:\n        logger.info(f\"Loading tape {tape_id}\")\n        if tape_id &gt;= len(self.tapes):\n            logger.error(f\"Tape {tape_id} not found in the index\")\n            return f\"&lt;h1&gt;Failed to load tape {tape_id}&lt;/h1&gt;\", \"\"\n        tape = self.tapes[tape_id]\n        label = self.get_tape_label(tape)\n        html = f\"{self.renderer.style}{self.renderer.render_tape(tape, self.llm_calls)}\"\n        return html, label\n\n    def reload_tapes(self, selected_file: str):\n        logger.info(f\"Reloading tapes from {selected_file}\")\n        return self.update_view(selected_file)\n\n    def switch_file(self, selected_file: str):\n        logger.info(f\"Switching to file {selected_file}\")\n        self.selected_tape = 0\n        return self.update_view(selected_file)\n\n    def launch(self, server_name: str = \"0.0.0.0\", port=7860, debug: bool = False, static_dir: str = \"\"):\n        def get_request_params(request: gr.Request):\n            self.request = request\n            tape_id = self.request.query_params.get(\"tape_id\")\n            self.files = self.get_tape_files()\n            selected_file = self.files[0]\n            if tape_id and tape_id in self.tape_index:\n                i, j = self.tape_index[tape_id]\n                selected_file = self.files[i]\n                self.selected_tape = j\n                logger.info(f\"Selected tape {selected_file}/{j} from query params\")\n            return self.update_view(selected_file)\n\n        gr.set_static_paths(paths=[\"outputs/\"])  # Allow HTML to load files (img) from this directory\n        with gr.Blocks(analytics_enabled=False) as blocks:\n            with gr.Row():\n                with gr.Column(scale=4):\n                    tape_view = gr.HTML(\"\")\n                with gr.Column(scale=1):\n                    reload_button = gr.Button(\"Reload Tapes\")\n                    file_selector = gr.Dropdown([], label=\"File\")\n                    file_label = gr.HTML(\"\")\n                    tape_selector = gr.Dropdown([], label=\"Tape\")\n                    tape_label = gr.HTML(\"\")\n                    reload_button.click(\n                        fn=self.reload_tapes,\n                        inputs=[file_selector],\n                        outputs=[file_selector, tape_selector, file_label, tape_view, tape_label],\n                    )\n            tape_selector.input(fn=self.update_tape_view, inputs=tape_selector, outputs=[tape_view, tape_label])\n            file_selector.input(\n                fn=self.switch_file,\n                inputs=file_selector,\n                outputs=[file_selector, tape_selector, file_label, tape_view, tape_label],\n            )\n            blocks.load(\n                get_request_params,\n                None,\n                outputs=[file_selector, tape_selector, file_label, tape_view, tape_label],\n            )\n        if static_dir:\n            logger.info(f\"Starting FastAPI server with static dir {static_dir}\")\n            # mount Gradio app to FastAPI app\n            app = FastAPI()\n            app.mount(\"/static\", StaticFiles(directory=static_dir), name=\"static\")\n            app = gr.mount_gradio_app(app, blocks, path=\"/\")\n            uvicorn.run(app, host=server_name, port=port)\n        else:\n            blocks.launch(server_name=server_name, server_port=port, debug=debug)\n</code></pre>"},{"location":"reference/team/","title":"Team","text":"<p>Multiagent building blocks for orchestrating multiple agents in a team.</p> <p>Classes:</p> <ul> <li> <code>ActiveTeamAgentView</code>           \u2013            </li> <li> <code>Chain</code>           \u2013            <p>Calls agents sequentially. Copies thoughts of previous agents for the next agents.</p> </li> <li> <code>TeamAgent</code>           \u2013            <p>Agent designed to work in the team with similar other agents performing different kinds</p> </li> </ul>"},{"location":"reference/team/#tapeagents.team.ActiveTeamAgentView","title":"<code>ActiveTeamAgentView</code>","text":"<p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>ActiveTeamAgentView contains the ephemeral state computed from the tape. This class extracts the data relevant to</p> </li> </ul> Source code in <code>tapeagents/team.py</code> <pre><code>class ActiveTeamAgentView:\n    def __init__(self, agent: TeamAgent, tape: TeamTape):\n        \"\"\"\n        ActiveTeamAgentView contains the ephemeral state computed from the tape. This class extracts the data relevant to\n        the given agent and also computes some additional information from it, e.g. whether the agent\n        should call the LLM to generate a message or respond with an already available one.\n        \"\"\"\n        view = TapeViewStack.compute(tape)\n        self.messages = view.messages_by_agent[agent.full_name]\n        self.last_non_empty_message = next((m for m in reversed(self.messages) if m.content), None)\n        self.node = agent.select_node(tape)\n        self.steps = view.top.steps\n        self.steps_by_kind = view.top.steps_by_kind\n        self.exec_result = self.steps[-1] if self.steps and isinstance(self.steps[-1], CodeExecutionResult) else None\n        self.should_generate_message = (\n            isinstance(self.node, (CallNode, RespondNode))\n            and self.messages\n            and not self.exec_result\n            and \"system\" in agent.templates\n        )\n        self.should_stop = (\n            agent.max_calls and (agent.max_calls and len(self.steps_by_kind.get(\"call\", [])) &gt;= agent.max_calls)\n        ) or (self.messages and (\"TERMINATE\" in self.messages[-1].content))\n</code></pre>"},{"location":"reference/team/#tapeagents.team.ActiveTeamAgentView.__init__","title":"<code>__init__(agent, tape)</code>","text":"<p>ActiveTeamAgentView contains the ephemeral state computed from the tape. This class extracts the data relevant to the given agent and also computes some additional information from it, e.g. whether the agent should call the LLM to generate a message or respond with an already available one.</p> Source code in <code>tapeagents/team.py</code> <pre><code>def __init__(self, agent: TeamAgent, tape: TeamTape):\n    \"\"\"\n    ActiveTeamAgentView contains the ephemeral state computed from the tape. This class extracts the data relevant to\n    the given agent and also computes some additional information from it, e.g. whether the agent\n    should call the LLM to generate a message or respond with an already available one.\n    \"\"\"\n    view = TapeViewStack.compute(tape)\n    self.messages = view.messages_by_agent[agent.full_name]\n    self.last_non_empty_message = next((m for m in reversed(self.messages) if m.content), None)\n    self.node = agent.select_node(tape)\n    self.steps = view.top.steps\n    self.steps_by_kind = view.top.steps_by_kind\n    self.exec_result = self.steps[-1] if self.steps and isinstance(self.steps[-1], CodeExecutionResult) else None\n    self.should_generate_message = (\n        isinstance(self.node, (CallNode, RespondNode))\n        and self.messages\n        and not self.exec_result\n        and \"system\" in agent.templates\n    )\n    self.should_stop = (\n        agent.max_calls and (agent.max_calls and len(self.steps_by_kind.get(\"call\", [])) &gt;= agent.max_calls)\n    ) or (self.messages and (\"TERMINATE\" in self.messages[-1].content))\n</code></pre>"},{"location":"reference/team/#tapeagents.team.Chain","title":"<code>Chain</code>","text":"<p>               Bases: <code>Agent[TapeType]</code>, <code>Generic[TapeType]</code></p> <p>Calls agents sequentially. Copies thoughts of previous agents for the next agents.</p> Source code in <code>tapeagents/team.py</code> <pre><code>class Chain(Agent[TapeType], Generic[TapeType]):\n    \"\"\"Calls agents sequentially. Copies thoughts of previous agents for the next agents.\"\"\"\n\n    @classmethod\n    def create(cls, nodes: list[CallSubagent], **kwargs) -&gt; Self:\n        subagents = []\n        for node in nodes:\n            subagents.append(node.agent)\n        return super().create(nodes=nodes + [RespondIfNotRootNode()], subagents=subagents, **kwargs)\n</code></pre>"},{"location":"reference/team/#tapeagents.team.TeamAgent","title":"<code>TeamAgent</code>","text":"<p>               Bases: <code>Agent[TeamTape]</code></p> <p>Agent designed to work in the team with similar other agents performing different kinds</p> <p>Methods:</p> <ul> <li> <code>create</code>             \u2013              <p>Create a simple agent that can execute code, think and respond to messages</p> </li> <li> <code>create_initiator</code>             \u2013              <p>Create an agent that sets the team's initial message and calls the team manager</p> </li> <li> <code>create_team_manager</code>             \u2013              <p>Create a team manager that broadcasts the last message to all subagents, selects one of them to call, call it and</p> </li> </ul> Source code in <code>tapeagents/team.py</code> <pre><code>class TeamAgent(Agent[TeamTape]):\n    \"\"\"\n    Agent designed to work in the team with similar other agents performing different kinds\n    \"\"\"\n\n    max_calls: int | None = None\n    init_message: str | None = None\n\n    model_config = ConfigDict(use_enum_values=True)\n\n    @classmethod\n    def create(\n        cls,\n        name: str,\n        system_prompt: str | None = None,\n        llm: LLM | None = None,\n        execute_code: bool = False,\n    ):  # type: ignore\n        \"\"\"\n        Create a simple agent that can execute code, think and respond to messages\n        \"\"\"\n        return cls(\n            name=name,\n            templates={\"system\": system_prompt} if system_prompt else {},\n            llms={DEFAULT: llm} if llm else {},\n            nodes=([ExecuteCodeNode()] if execute_code else []) + [RespondNode()],  # type: ignore\n        )\n\n    @classmethod\n    def create_team_manager(\n        cls,\n        name: str,\n        subagents: list[Agent[TeamTape]],\n        llm: LLM,\n        templates: dict[str, str],\n        max_calls: int = 1,\n    ):\n        \"\"\"\n        Create a team manager that broadcasts the last message to all subagents, selects one of them to call, call it and\n        responds to the last message if the termination message is not received.\n        \"\"\"\n        return cls(\n            name=name,\n            subagents=subagents,\n            nodes=[\n                BroadcastLastMessageNode(),\n                SelectAndCallNode(),\n                RespondOrRepeatNode(next_node=\"broadcast_last_message\"),\n            ],\n            max_calls=max_calls,\n            templates=templates,\n            llms={DEFAULT: llm},\n        )\n\n    @classmethod\n    def create_initiator(\n        cls,\n        name: str,\n        teammate: Agent[TeamTape],\n        init_message: str,\n        system_prompt: str = \"\",\n        llm: LLM | None = None,\n        max_calls: int = 1,\n        execute_code: bool = False,\n    ):\n        \"\"\"\n        Create an agent that sets the team's initial message and calls the team manager\n        \"\"\"\n        nodes = []\n        if execute_code:\n            nodes = [ExecuteCodeNode(), CallNode(), TerminateOrRepeatNode(next_node=\"execute_code\")]\n        else:\n            nodes = [CallNode(), TerminateOrRepeatNode(next_node=\"call\")]\n        return cls(\n            name=name,\n            templates={\n                \"system\": system_prompt,\n            },\n            llms={DEFAULT: llm} if llm else {},\n            subagents=[teammate],\n            nodes=nodes,\n            max_calls=max_calls,\n            init_message=init_message,\n        )\n</code></pre>"},{"location":"reference/team/#tapeagents.team.TeamAgent.create","title":"<code>create(name, system_prompt=None, llm=None, execute_code=False)</code>  <code>classmethod</code>","text":"<p>Create a simple agent that can execute code, think and respond to messages</p> Source code in <code>tapeagents/team.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    name: str,\n    system_prompt: str | None = None,\n    llm: LLM | None = None,\n    execute_code: bool = False,\n):  # type: ignore\n    \"\"\"\n    Create a simple agent that can execute code, think and respond to messages\n    \"\"\"\n    return cls(\n        name=name,\n        templates={\"system\": system_prompt} if system_prompt else {},\n        llms={DEFAULT: llm} if llm else {},\n        nodes=([ExecuteCodeNode()] if execute_code else []) + [RespondNode()],  # type: ignore\n    )\n</code></pre>"},{"location":"reference/team/#tapeagents.team.TeamAgent.create_initiator","title":"<code>create_initiator(name, teammate, init_message, system_prompt='', llm=None, max_calls=1, execute_code=False)</code>  <code>classmethod</code>","text":"<p>Create an agent that sets the team's initial message and calls the team manager</p> Source code in <code>tapeagents/team.py</code> <pre><code>@classmethod\ndef create_initiator(\n    cls,\n    name: str,\n    teammate: Agent[TeamTape],\n    init_message: str,\n    system_prompt: str = \"\",\n    llm: LLM | None = None,\n    max_calls: int = 1,\n    execute_code: bool = False,\n):\n    \"\"\"\n    Create an agent that sets the team's initial message and calls the team manager\n    \"\"\"\n    nodes = []\n    if execute_code:\n        nodes = [ExecuteCodeNode(), CallNode(), TerminateOrRepeatNode(next_node=\"execute_code\")]\n    else:\n        nodes = [CallNode(), TerminateOrRepeatNode(next_node=\"call\")]\n    return cls(\n        name=name,\n        templates={\n            \"system\": system_prompt,\n        },\n        llms={DEFAULT: llm} if llm else {},\n        subagents=[teammate],\n        nodes=nodes,\n        max_calls=max_calls,\n        init_message=init_message,\n    )\n</code></pre>"},{"location":"reference/team/#tapeagents.team.TeamAgent.create_team_manager","title":"<code>create_team_manager(name, subagents, llm, templates, max_calls=1)</code>  <code>classmethod</code>","text":"<p>Create a team manager that broadcasts the last message to all subagents, selects one of them to call, call it and responds to the last message if the termination message is not received.</p> Source code in <code>tapeagents/team.py</code> <pre><code>@classmethod\ndef create_team_manager(\n    cls,\n    name: str,\n    subagents: list[Agent[TeamTape]],\n    llm: LLM,\n    templates: dict[str, str],\n    max_calls: int = 1,\n):\n    \"\"\"\n    Create a team manager that broadcasts the last message to all subagents, selects one of them to call, call it and\n    responds to the last message if the termination message is not received.\n    \"\"\"\n    return cls(\n        name=name,\n        subagents=subagents,\n        nodes=[\n            BroadcastLastMessageNode(),\n            SelectAndCallNode(),\n            RespondOrRepeatNode(next_node=\"broadcast_last_message\"),\n        ],\n        max_calls=max_calls,\n        templates=templates,\n        llms={DEFAULT: llm},\n    )\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":"<p>Various utility functions.</p> <p>Functions:</p> <ul> <li> <code>sanitize_json_completion</code>             \u2013              <p>Return only content inside the first pair of triple backticks if they are present.</p> </li> </ul>"},{"location":"reference/utils/#tapeagents.utils.sanitize_json_completion","title":"<code>sanitize_json_completion(completion)</code>","text":"<p>Return only content inside the first pair of triple backticks if they are present.</p> Source code in <code>tapeagents/utils.py</code> <pre><code>def sanitize_json_completion(completion: str) -&gt; str:\n    \"\"\"\n    Return only content inside the first pair of triple backticks if they are present.\n    \"\"\"\n    tiks_counter = 0\n    lines = completion.strip().split(\"\\n\")\n    clean_lines = []\n    for line in lines:\n        if line.startswith(\"```\"):\n            tiks_counter += 1\n            if tiks_counter == 1:\n                clean_lines = []\n            elif tiks_counter == 2:\n                break\n            continue\n        clean_lines.append(line)\n    return \"\\n\".join(clean_lines)\n</code></pre>"},{"location":"reference/view/","title":"View","text":"<p>Views and view stacks for subagents context isolation.</p> <p>Classes:</p> <ul> <li> <code>Broadcast</code>           \u2013            <p>Broadcast a message to many subagents.</p> </li> <li> <code>TapeView</code>           \u2013            <p>Ephemeral view of an agent's part of the tape.</p> </li> <li> <code>TapeViewStack</code>           \u2013            <p>Stack of tape views of the agents in the call chain.</p> </li> </ul>"},{"location":"reference/view/#tapeagents.view.Broadcast","title":"<code>Broadcast</code>","text":"<p>               Bases: <code>Thought</code></p> <p>Broadcast a message to many subagents.</p> <p>The current agent remains active.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The content of the message to broadcast.</p> </li> <li> <code>from_</code>               (<code>str</code>)           \u2013            <p>The name of the agent broadcasting the message.</p> </li> <li> <code>to</code>               (<code>list[str]</code>)           \u2013            <p>The list of subagents to broadcast the message to.</p> </li> <li> <code>kind</code>               (<code>Literal['broadcast']</code>)           \u2013            <p>The kind of the step, which is \"broadcast\".</p> </li> </ul> Source code in <code>tapeagents/view.py</code> <pre><code>class Broadcast(Thought):\n    \"\"\"\n    Broadcast a message to many subagents.\n\n    The current agent remains active.\n\n    Attributes:\n        content (str): The content of the message to broadcast.\n        from_ (str): The name of the agent broadcasting the message.\n        to (list[str]): The list of subagents to broadcast the message to.\n        kind (Literal[\"broadcast\"]): The kind of the step, which is \"broadcast\".\n    \"\"\"\n\n    content: str\n    from_: str\n    to: list[str]\n    kind: Literal[\"broadcast\"] = \"broadcast\"\n</code></pre>"},{"location":"reference/view/#tapeagents.view.TapeView","title":"<code>TapeView</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[StepType]</code></p> <p>Ephemeral view of an agent's part of the tape.</p> <p>Presents tape data in the form that is describing for describing the agent's logic.</p> <p>Attributes:</p> <ul> <li> <code>agent_name</code>               (<code>str</code>)           \u2013            <p>The name of the agent.</p> </li> <li> <code>agent_full_name</code>               (<code>str</code>)           \u2013            <p>The full name of the agent, including the names of all its parent agents.</p> </li> <li> <code>steps</code>               (<code>list[StepType]</code>)           \u2013            <p>The list of steps in the agent's part of the tape.</p> </li> <li> <code>steps_by_kind</code>               (<code>dict[str, list[StepType]]</code>)           \u2013            <p>A dictionary of steps grouped by their kind.</p> </li> <li> <code>outputs_by_subagent</code>               (<code>dict[str, StepType]</code>)           \u2013            <p>A dictionary of the output steps of the agent's subagents.</p> </li> <li> <code>last_node</code>               (<code>str</code>)           \u2013            <p>The last node the agent was in.</p> </li> <li> <code>last_prompt_id</code>               (<code>str</code>)           \u2013            <p>The ID of the prompt of the last step.</p> </li> <li> <code>next_node</code>               (<code>str</code>)           \u2013            <p>The next node the agent will go to next.</p> </li> </ul> Source code in <code>tapeagents/view.py</code> <pre><code>class TapeView(BaseModel, Generic[StepType]):\n    \"\"\"\n    Ephemeral view of an agent's part of the tape.\n\n    Presents tape data in the form that is describing for describing the agent's logic.\n\n    Attributes:\n        agent_name (str): The name of the agent.\n        agent_full_name (str): The full name of the agent, including the names of all its parent agents.\n        steps (list[StepType]): The list of steps in the agent's part of the tape.\n        steps_by_kind (dict[str, list[StepType]]): A dictionary of steps grouped by their kind.\n        outputs_by_subagent (dict[str, StepType]): A dictionary of the output steps of the agent's subagents.\n        last_node (str): The last node the agent was in.\n        last_prompt_id (str): The ID of the prompt of the last step.\n        next_node (str): The next node the agent will go to next.\n    \"\"\"\n\n    agent_name: str\n    agent_full_name: str\n    steps: list[StepType] = []\n    steps_by_kind: dict[str, list[StepType]] = {}\n    outputs_by_subagent: dict[str, StepType] = {}\n    last_node: str = \"\"\n    last_prompt_id: str = \"\"\n    next_node: str = \"\"\n\n    def add_step(self, step: StepType):\n        self.steps.append(step)\n        kind = step.kind  # type: ignore\n        if kind not in self.steps_by_kind:\n            self.steps_by_kind[kind] = []\n        if self.is_step_by_active_agent(step):\n            if step.metadata.prompt_id != self.last_prompt_id and not isinstance(step, Respond):\n                # respond should not reset the previous set_next_node in the caller agent\n                self.next_node = \"\"\n            self.last_prompt_id = step.metadata.prompt_id\n            self.last_node = step.metadata.node\n        if isinstance(step, SetNextNode):\n            self.next_node = step.next_node\n        self.steps_by_kind[kind].append(step)\n\n    def get_output(self, subagent_name_or_index: int | str) -&gt; StepType:\n        if isinstance(subagent_name_or_index, int):\n            return list(self.outputs_by_subagent.values())[subagent_name_or_index]\n        return self.outputs_by_subagent[subagent_name_or_index]\n\n    def is_step_by_active_agent(self, step: StepType):\n        # state machine doesn't know the name of the root agent, so in the comparison here\n        # we need cut of the first component\n        if not isinstance(step, AgentStep):\n            return False\n        parts_by = step.metadata.agent.split(\"/\")\n        parts_frame_by = self.agent_full_name.split(\"/\")\n        return parts_by[1:] == parts_frame_by[1:]\n</code></pre>"},{"location":"reference/view/#tapeagents.view.TapeViewStack","title":"<code>TapeViewStack</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[StepType]</code></p> <p>Stack of tape views of the agents in the call chain.</p> <p>If Agent A calls Agent B, and then Agent B calls Agent C, the stack will looks as follows: 0: TapeView of Agent A 1: TapeView of Agent B 2: TapeView of Agent C</p> <p>Attributes:</p> <ul> <li> <code>stack</code>               (<code>list[TapeView[StepType]]</code>)           \u2013            <p>The stack of tape views.</p> </li> <li> <code>messages_by_agent</code>               (<code>dict[str, list[Union[Call, Respond, Broadcast]]]</code>)           \u2013            <p>A dictionary of messages by agent.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              <p>Computes a stack of tape views from a given tape.</p> </li> <li> <code>update</code>             \u2013              <p>Updates the view based on the given step type.</p> </li> </ul> Source code in <code>tapeagents/view.py</code> <pre><code>class TapeViewStack(BaseModel, Generic[StepType]):\n    \"\"\"\n    Stack of tape views of the agents in the call chain.\n\n    If Agent A calls Agent B, and then Agent B calls Agent C,\n    the stack will looks as follows:\n    0: TapeView of Agent A\n    1: TapeView of Agent B\n    2: TapeView of Agent C\n\n    Attributes:\n        stack (list[TapeView[StepType]]): The stack of tape views.\n        messages_by_agent (dict[str, list[Union[Call, Respond, Broadcast]]]): A dictionary of messages by agent.\n    \"\"\"\n\n    stack: list[TapeView[StepType]]\n    messages_by_agent: dict[str, list[Call | Respond | Broadcast]] = Field(default_factory=lambda: defaultdict(list))\n\n    @property\n    def top(self):\n        return self.stack[-1]\n\n    def update(self, step: StepType):\n        \"\"\"\n        Updates the view based on the given step type.\n\n        Args:\n            step (StepType): The step to process. Can be one of:\n\n                - Call: Creates and pushes new view onto the stack\n                - Broadcast: Processes broadcast messages\n                - Respond: Removes top view from stack\n                - AgentStep: Adds step to current top view\n                - Observation: Adds observation to current top view\n\n        Raises:\n            ValueError: If the step type is not supported\n        \"\"\"\n        top = self.stack[-1]\n        match step:\n            case Call():\n                self.put_new_view_on_stack(step)\n            case Broadcast():\n                self.broadcast(step)\n            case Respond():\n                self.pop_view_from_stack(step)\n            case AgentStep():\n                top.add_step(step)\n            case Observation():\n                top.add_step(step)\n            case _:\n                raise ValueError(f\"Unsupported step type {step}\")\n\n    def pop_view_from_stack(self, step: Respond):\n        top = self.stack[-1]\n        self.stack.pop()\n        new_top = self.stack[-1]\n\n        if step.copy_output:\n            for top_step in reversed(top.steps):\n                # How we choose the output step of the frame.\n                # - exclude the input steps that the caller agent added to the tape for the given agent\n                #   (note that by this line the former caller agent is the active agent)\n                # - exclude Call and Respond steps\n                # - exclude Observation steps\n                # - among the remaining steps pick the last one\n                if not self.top.is_step_by_active_agent(top_step) and not isinstance(\n                    top_step, (Call, Respond, Observation)\n                ):\n                    new_top.add_step(top_step)\n                    new_top.outputs_by_subagent[top.agent_name] = top_step\n                    break\n                    # TODO: what if the agent was not called by its immediate manager?\n\n        receiver = step.metadata.agent.rsplit(\"/\", 1)[0]\n        self.messages_by_agent[step.metadata.agent].append(step)\n        self.messages_by_agent[receiver].append(step)\n        new_top.add_step(step)\n\n    def broadcast(self, step):\n        top = self.stack[-1]\n        top.add_step(step)\n        for to in step.to:\n            receiver = f\"{step.metadata.agent}/{to}\"\n            self.messages_by_agent[receiver].append(step)\n\n    def put_new_view_on_stack(self, step):\n        top = self.stack[-1]\n        top.add_step(step)\n        self.stack.append(\n            TapeView(\n                agent_name=step.agent_name,\n                agent_full_name=top.agent_full_name + \"/\" + step.agent_name,\n            )\n        )\n        self.stack[-1].add_step(step)\n        receiver = f\"{step.metadata.agent}/{step.agent_name}\"\n        self.messages_by_agent[step.metadata.agent].append(step)\n        self.messages_by_agent[receiver].append(step)\n\n    @staticmethod\n    def compute(tape: Tape, root_agent_name: str = \"root\") -&gt; TapeViewStack[StepType]:\n        \"\"\"\n        Computes a stack of tape views from a given tape.\n\n        This function processes a tape step by step and builds a view stack that tracks\n        the execution flow through different agents.\n\n        Args:\n            tape (Tape): The tape containing steps to process\n            root_agent_name (str, optional): Name of the root agent. Defaults to \"root\"\n\n        Returns:\n            TapeViewStack[StepType]: A stack of tape views representing the execution flow\n\n        Note:\n            TODO: Implement view retrieval from tape prefix and recomputation functionality\n        \"\"\"\n        stack = TapeViewStack(stack=[TapeView(agent_name=root_agent_name, agent_full_name=root_agent_name)])\n        for step in tape.steps:\n            stack.update(step)\n        return stack  # type: ignore\n</code></pre>"},{"location":"reference/view/#tapeagents.view.TapeViewStack.compute","title":"<code>compute(tape, root_agent_name='root')</code>  <code>staticmethod</code>","text":"<p>Computes a stack of tape views from a given tape.</p> <p>This function processes a tape step by step and builds a view stack that tracks the execution flow through different agents.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape containing steps to process</p> </li> <li> <code>root_agent_name</code>               (<code>str</code>, default:                   <code>'root'</code> )           \u2013            <p>Name of the root agent. Defaults to \"root\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TapeViewStack[StepType]</code>           \u2013            <p>TapeViewStack[StepType]: A stack of tape views representing the execution flow</p> </li> </ul> Note <p>TODO: Implement view retrieval from tape prefix and recomputation functionality</p> Source code in <code>tapeagents/view.py</code> <pre><code>@staticmethod\ndef compute(tape: Tape, root_agent_name: str = \"root\") -&gt; TapeViewStack[StepType]:\n    \"\"\"\n    Computes a stack of tape views from a given tape.\n\n    This function processes a tape step by step and builds a view stack that tracks\n    the execution flow through different agents.\n\n    Args:\n        tape (Tape): The tape containing steps to process\n        root_agent_name (str, optional): Name of the root agent. Defaults to \"root\"\n\n    Returns:\n        TapeViewStack[StepType]: A stack of tape views representing the execution flow\n\n    Note:\n        TODO: Implement view retrieval from tape prefix and recomputation functionality\n    \"\"\"\n    stack = TapeViewStack(stack=[TapeView(agent_name=root_agent_name, agent_full_name=root_agent_name)])\n    for step in tape.steps:\n        stack.update(step)\n    return stack  # type: ignore\n</code></pre>"},{"location":"reference/view/#tapeagents.view.TapeViewStack.update","title":"<code>update(step)</code>","text":"<p>Updates the view based on the given step type.</p> <p>Parameters:</p> <ul> <li> <code>step</code>               (<code>StepType</code>)           \u2013            <p>The step to process. Can be one of:</p> <ul> <li>Call: Creates and pushes new view onto the stack</li> <li>Broadcast: Processes broadcast messages</li> <li>Respond: Removes top view from stack</li> <li>AgentStep: Adds step to current top view</li> <li>Observation: Adds observation to current top view</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the step type is not supported</p> </li> </ul> Source code in <code>tapeagents/view.py</code> <pre><code>def update(self, step: StepType):\n    \"\"\"\n    Updates the view based on the given step type.\n\n    Args:\n        step (StepType): The step to process. Can be one of:\n\n            - Call: Creates and pushes new view onto the stack\n            - Broadcast: Processes broadcast messages\n            - Respond: Removes top view from stack\n            - AgentStep: Adds step to current top view\n            - Observation: Adds observation to current top view\n\n    Raises:\n        ValueError: If the step type is not supported\n    \"\"\"\n    top = self.stack[-1]\n    match step:\n        case Call():\n            self.put_new_view_on_stack(step)\n        case Broadcast():\n            self.broadcast(step)\n        case Respond():\n            self.pop_view_from_stack(step)\n        case AgentStep():\n            top.add_step(step)\n        case Observation():\n            top.add_step(step)\n        case _:\n            raise ValueError(f\"Unsupported step type {step}\")\n</code></pre>"},{"location":"reference/finetune/","title":"Finetune","text":"<p>Finetuning LLMs with tape data.</p> <p>Modules:</p> <ul> <li> <code>checkpoints</code>           \u2013            </li> <li> <code>data</code>           \u2013            </li> <li> <code>logging_</code>           \u2013            </li> <li> <code>optim</code>           \u2013            </li> <li> <code>rl</code>           \u2013            </li> </ul>"},{"location":"reference/finetune/checkpoints/","title":"Checkpoints","text":"<p>Functions:</p> <ul> <li> <code>get_auto_model_class</code>             \u2013              <p>Get the AutoModel class corresponding to the model class.</p> </li> <li> <code>get_temporary_folder_and_move</code>             \u2013              <p>Context manager safe checkpointing.</p> </li> <li> <code>load_training_checkpoint</code>             \u2013              <p>Load checkpoint created by save_training_checkpoint() in-place:</p> </li> <li> <code>save_model_only</code>             \u2013              <p>Save model weights and config.</p> </li> <li> <code>save_tokenizer_only</code>             \u2013              <p>Save only tokenizer to output_dir</p> </li> </ul>"},{"location":"reference/finetune/checkpoints/#tapeagents.finetune.checkpoints.get_auto_model_class","title":"<code>get_auto_model_class(model_class)</code>","text":"<p>Get the AutoModel class corresponding to the model class.</p> Source code in <code>tapeagents/finetune/checkpoints.py</code> <pre><code>def get_auto_model_class(\n    model_class: ModelClass,\n) -&gt; Type[_BaseAutoModelClass]:\n    \"\"\"Get the AutoModel class corresponding to the model class.\"\"\"\n    match model_class:\n        case \"causal-language-modeling\":\n            return AutoModelForCausalLM\n        case \"seq2seq-language-modeling\":\n            return AutoModelForSeq2SeqLM\n        case _:\n            raise ValueError(f\"Unsupported model class: {model_class}\")\n</code></pre>"},{"location":"reference/finetune/checkpoints/#tapeagents.finetune.checkpoints.get_temporary_folder_and_move","title":"<code>get_temporary_folder_and_move(output_dir)</code>","text":"<p>Context manager safe checkpointing.</p> <p>Creates temporary folder <code>~output_dir</code>, then rename to final destination</p> Source code in <code>tapeagents/finetune/checkpoints.py</code> <pre><code>@contextlib.contextmanager\ndef get_temporary_folder_and_move(output_dir: Path):\n    \"\"\"\n    Context manager safe checkpointing.\n\n    Creates temporary folder `~output_dir`, then rename to final destination\n    \"\"\"\n    if os.path.exists(output_dir) and not os.path.isdir(output_dir):\n        raise ValueError(\"get_temporary_folder_and_move: output_dir is not a directory\")\n\n    output_dir = output_dir.resolve()\n    temporary_path = output_dir.parent / (\"~\" + output_dir.name)\n\n    if accelerator.is_main_process:\n        if os.path.exists(temporary_path):\n            logger.info(f\"Deleting temporary directory {temporary_path}\")\n            shutil.rmtree(temporary_path)\n        logger.info(f\"Creating temporary directory {temporary_path}\")\n        os.makedirs(temporary_path)\n\n    accelerator.wait_for_everyone()\n    yield temporary_path\n    accelerator.wait_for_everyone()\n\n    # Move to final path\n    if accelerator.is_main_process:\n        # delete output_dir if it exists\n        if os.path.exists(output_dir):\n            logger.info(\n                f\" -&gt; Deleting {output_dir}. \"\n                f\"If this fails, manually delete it and move {temporary_path} to {output_dir}\"\n            )\n            shutil.rmtree(output_dir)\n        logger.info(f\" -&gt; Renaming {temporary_path} to {output_dir}\")\n        os.rename(temporary_path, output_dir)\n        logger.info(f\"Done moving files to {output_dir}\")\n</code></pre>"},{"location":"reference/finetune/checkpoints/#tapeagents.finetune.checkpoints.load_training_checkpoint","title":"<code>load_training_checkpoint(training_state_dir, model, optimizer, lr_scheduler)</code>","text":"<p>Load checkpoint created by save_training_checkpoint() in-place:</p> <ul> <li>With deepspeed, this will load model, optimizer, lr_scheduler states in-place.</li> <li>Without deepspeed, this will only load optimizer, lr_scheduler states in-place,     but not model states!</li> </ul> Source code in <code>tapeagents/finetune/checkpoints.py</code> <pre><code>def load_training_checkpoint(\n    training_state_dir: Path,\n    model: transformers.PreTrainedModel,\n    optimizer,\n    lr_scheduler,\n):\n    \"\"\"\n    Load checkpoint created by save_training_checkpoint() in-place:\n\n    - With deepspeed, this will load model, optimizer, lr_scheduler states in-place.\n    - Without deepspeed, this will *only* load optimizer, lr_scheduler states in-place,\n        but *not* model states!\n    \"\"\"\n    assert (\n        not os.path.exists(training_state_dir) or training_state_dir.is_dir()\n    ), f\"output_dir {training_state_dir} must be a directory\"\n\n    if model.__class__.__name__.endswith(\"DeepSpeedEngine\"):\n        logger.info(\"Load deepspeed training state\")\n        # This magically loads optimizer and lr_scheduler states (if they were saved)\n        # (the passed optimizer and lr_scheduler arguments will be ignored)\n        load_path, extra_training_state = model.load_checkpoint(\n            training_state_dir,\n            tag=\"deepspeed\",\n            load_optimizer_states=True,\n            load_lr_scheduler_states=True,\n        )\n        if load_path is None:\n            raise RuntimeError(f\"Loading deepspeed checkpoint from {training_state_dir} failed\")\n        if (\n            model.lr_scheduler is None\n            and extra_training_state is not None\n            and \"lr_scheduler_state\" in extra_training_state\n        ):\n            # Manually load lr_scheduler states\n            logger.warning(f\"Manually loading ds-unsupported lr_scheduler of type {type(lr_scheduler).__name__}\")\n            lr_scheduler.load_state_dict(extra_training_state[\"lr_scheduler_state\"])\n        logger.info(f\"Loaded deepspeed checkpoint from {training_state_dir}\")\n    else:  # multi_gpu (no deepspeed)\n        # This needs to be called from all processes\n        training_state = torch.load(training_state_dir / \"training_state.pt\", map_location=\"cpu\")\n        optimizer.load_state_dict(training_state[\"optimizer_state\"])\n        lr_scheduler.load_state_dict(training_state[\"lr_scheduler_state\"])\n        del training_state[\"optimizer_state\"]\n        del training_state[\"lr_scheduler_state\"]\n        extra_training_state = training_state\n        logger.info(f\"Loaded accelerate checkpoint from {training_state_dir}\")\n    return extra_training_state\n</code></pre>"},{"location":"reference/finetune/checkpoints/#tapeagents.finetune.checkpoints.save_model_only","title":"<code>save_model_only(output_dir, model, unwrap=True, lora=False, safe_serialization=False)</code>","text":"<p>Save model weights and config.</p> <p>Creates the following files in output_dir/ :     - config.json and either:     - pytorch_model.bin (single-file model), OR     - pytorch_model-XXXXX-of-XXXXX.bin (multi-file model) and pytorch_model.bin.index.json</p> <p>Note that this does not save optimizer, lr_scheduler, scaler, etc. Use only for later JGA evaluation, not for resuming training</p> <p>Must be called on all accelerate processes because all of them must save their shards.</p> Source code in <code>tapeagents/finetune/checkpoints.py</code> <pre><code>def save_model_only(\n    output_dir: Path,\n    model: transformers.PreTrainedModel,\n    unwrap: bool = True,\n    lora: bool = False,\n    safe_serialization: bool = False,\n):\n    \"\"\"\n    Save model weights and config.\n\n    Creates the following files in output_dir/ :\n        - config.json\n    and either:\n        - pytorch_model.bin (single-file model), OR\n        - pytorch_model-XXXXX-of-XXXXX.bin (multi-file model) and pytorch_model.bin.index.json\n\n    Note that this does not save optimizer, lr_scheduler, scaler, etc.\n    Use only for later JGA evaluation, not for resuming training\n\n    Must be called on *all* accelerate processes because all of them must save their shards.\n    \"\"\"\n    assert not os.path.exists(output_dir) or output_dir.is_dir(), f\"output_dir {output_dir} must be a directory\"\n    accelerator.wait_for_everyone()\n\n    logger.info(f\"Save model to {output_dir}\")\n\n    unwrapped_model = accelerator.unwrap_model(model) if unwrap else model\n    if lora:\n        lora_save(output_dir, unwrapped_model)\n        return\n\n    if unwrapped_model.__class__.__name__.endswith(\"DeepSpeedEngine\"):\n        unwrapped_model.save_checkpoint(\n            save_dir=output_dir,\n        )\n        logger.info(f\"Saved deepspeed checkpoint to {output_dir}\")\n    elif isinstance(unwrapped_model, transformers.PreTrainedModel):\n        unwrapped_model.save_pretrained(  # type: ignore\n            output_dir,\n            is_main_process=accelerator.is_main_process,\n            save_function=accelerator.save,\n            state_dict=accelerator.get_state_dict(model),\n            safe_serialization=safe_serialization,\n        )\n        logger.info(f\"Saved model to {output_dir}\")\n    else:\n        raise ValueError(f\"model is neither a deepspeed model nor a transformers.PreTrainedModel: {type(model)}\")\n</code></pre>"},{"location":"reference/finetune/checkpoints/#tapeagents.finetune.checkpoints.save_tokenizer_only","title":"<code>save_tokenizer_only(output_dir, tokenizer)</code>","text":"<p>Save only tokenizer to output_dir</p> <p>Can be called on all processes.</p> Source code in <code>tapeagents/finetune/checkpoints.py</code> <pre><code>def save_tokenizer_only(\n    output_dir: Path,\n    tokenizer: transformers.PreTrainedTokenizer | transformers.PreTrainedTokenizerFast,\n):\n    \"\"\"\n    Save only tokenizer to output_dir\n\n    Can be called on *all* processes.\n    \"\"\"\n    assert not os.path.exists(output_dir) or output_dir.is_dir(), f\"output_dir {output_dir} must be a directory\"\n    if accelerator.is_main_process:\n        logger.info(f\"Save tokenizer to {output_dir}\")\n        tokenizer.save_pretrained(output_dir)\n</code></pre>"},{"location":"reference/finetune/context/","title":"Context","text":""},{"location":"reference/finetune/data/","title":"Data","text":"<p>Functions:</p> <ul> <li> <code>mask_labels</code>             \u2013              <p>This function creates labels from a sequence of input ids by masking</p> </li> <li> <code>validate_spans</code>             \u2013              <p>Make sure the spans are valid, don't overlap, and are in order.</p> </li> </ul>"},{"location":"reference/finetune/data/#tapeagents.finetune.data.mask_labels","title":"<code>mask_labels(input_ids, offset_mapping, predicted_spans, masked_token_id=MASKED_TOKEN_ID)</code>","text":"<p>This function creates labels from a sequence of input ids by masking the tokens that do not have any overlap with the character spans that are designated for prediction. The labels can then be used to train a model to predict everything except the masked tokens.</p> <p>The function also returns a list of midpoints for splitting the labels into a source and a target. The source is the part of the labels that is used to predict the target. There is one midpoint for each span that is designated for prediction. Each midpoint is the index of the first token that overlaps with the corresponding span.</p> <p>Parameters:</p> <ul> <li> <code>input_ids</code>               (<code>Sequence[int]</code>)           \u2013            <p>A sequence of token ids.</p> </li> <li> <code>offset_mapping</code>               (<code>Iterable[tuple[int, int]]</code>)           \u2013            <p>The offset mapping returned by the tokenizer.</p> </li> <li> <code>predicted_spans</code>               (<code>Iterable[Iterable[int]]</code>)           \u2013            <p>The character spans that are designated for prediction. The spans are given as a sequence of two-element sequences, where the first element is the beginning of the span (inclusive) and the second element is the end of the span (not inclusive).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[int], list[int]]</code>           \u2013            <p>tuple[list[int], list[int]]: A tuple of masked labels and corresponding midpoints for splitting the labels into a source and a target.</p> </li> </ul> Source code in <code>tapeagents/finetune/data.py</code> <pre><code>def mask_labels(\n    input_ids: Sequence[int],\n    offset_mapping: Iterable[tuple[int, int]],\n    predicted_spans: Iterable[Iterable[int]],\n    masked_token_id: int = MASKED_TOKEN_ID,\n) -&gt; tuple[list[int], list[int]]:\n    \"\"\"\n    This function creates labels from a sequence of input ids by masking\n    the tokens that do not have any overlap with the character spans that\n    are designated for prediction. The labels can then be used to train\n    a model to predict everything except the masked tokens.\n\n    The function also returns a list of midpoints for splitting the\n    labels into a source and a target. The source is the part of the\n    labels that is used to predict the target. There is one midpoint\n    for each span that is designated for prediction. Each midpoint is\n    the index of the first token that overlaps with the corresponding\n    span.\n\n    Args:\n        input_ids (Sequence[int]): A sequence of token ids.\n        offset_mapping (Iterable[tuple[int, int]]): The offset mapping\n            returned by the tokenizer.\n        predicted_spans (Iterable[Iterable[int]]): The character spans\n            that are designated for prediction. The spans are given as\n            a sequence of two-element sequences, where the first element\n            is the beginning of the span (inclusive) and the second\n            element is the end of the span (not inclusive).\n\n    Returns:\n        tuple[list[int], list[int]]: A tuple of masked labels and\n            corresponding midpoints for splitting the labels into\n            a source and a target.\n    \"\"\"\n    labels = [masked_token_id] * len(input_ids)\n    midpoints = []\n    # TODO: Make this O(n_tokens) instead of O(n_tokens * n_spans)\n    for span_begin, span_end in predicted_spans:\n        midpoint_found = False\n        for i, (offset_begin, offset_end) in enumerate(offset_mapping):\n            # visual inspection of the results shows that this is the correct way to check\n            if offset_begin &lt; span_end and span_begin &lt; offset_end:\n                if not midpoint_found:\n                    midpoints.append(i)\n                    midpoint_found = True\n                labels[i] = input_ids[i]\n    return labels, midpoints\n</code></pre>"},{"location":"reference/finetune/data/#tapeagents.finetune.data.validate_spans","title":"<code>validate_spans(text, predicted_spans)</code>","text":"<p>Make sure the spans are valid, don't overlap, and are in order.</p> Source code in <code>tapeagents/finetune/data.py</code> <pre><code>def validate_spans(text: str, predicted_spans: list[tuple[int, int]]) -&gt; None:\n    \"\"\"Make sure the spans are valid, don't overlap, and are in order.\"\"\"\n    for start, end in predicted_spans:\n        if start &lt; 0 or end &gt; len(text):\n            raise ValueError(f\"Span {start}:{end} is out of bounds for text {text!r}\")\n        if start &gt; end:\n            raise ValueError(f\"Span {start}:{end} is invalid\")\n    for (start1, end1), (start2, end2) in zip(predicted_spans, predicted_spans[1:]):\n        # Make sure the second span starts after the first one ends.\n        if start2 &lt; end1:\n            raise ValueError(\n                f\"Spans {start1}:{end1} ({text[start1:end1]!r}) and {start2}:{end2} ({text[start2:end2]!r}) overlap\"\n            )\n</code></pre>"},{"location":"reference/finetune/eval/","title":"Eval","text":""},{"location":"reference/finetune/finetune/","title":"Finetune","text":""},{"location":"reference/finetune/logging_/","title":"Logging","text":"<p>Functions:</p> <ul> <li> <code>init_wandb</code>             \u2013              <p>Initialize W&amp;B.</p> </li> </ul>"},{"location":"reference/finetune/logging_/#tapeagents.finetune.logging_.init_wandb","title":"<code>init_wandb(cfg, run_dir, config_for_wandb)</code>","text":"<p>Initialize W&amp;B.</p> <p>config_for_wandb is the configuration that will be logged to W&amp;B.</p> Source code in <code>tapeagents/finetune/logging_.py</code> <pre><code>def init_wandb(\n    cfg: DictConfig,\n    run_dir: Path,\n    config_for_wandb: DictConfig | dict,\n) -&gt; wandb_run.Run:\n    \"\"\"Initialize W&amp;B.\n\n    config_for_wandb is the configuration that will be logged to W&amp;B.\n\n    \"\"\"\n    if config_for_wandb is None:\n        config_for_wandb = cfg.dict()\n\n    wandb_id = cfg.finetune.wandb_id\n\n    if cfg.finetune.wandb_resume == \"always\":\n        resume = True\n    elif cfg.finetune.wandb_resume == \"if_not_interactive\":\n        resume = not cfg.finetune.force_restart\n    else:\n        raise ValueError(f\"Unknown value for wandb_resume: {cfg.finetune.wandb_resume}\")\n    wandb_name = run_dir.name if cfg.finetune.wandb_use_basename else str(run_dir)\n\n    if len(wandb_name) &gt; 128:\n        logger.warning(f\"wandb_name: {wandb_name} is longer than 128 characters. Truncating to 128 characters.\")\n\n    logging.info(f\"Initializing W&amp;B with name: {wandb_name[:128]}, resume: {resume}\")\n    run = wandb.init(\n        name=wandb_name[:128],  # wandb limits name to 128 characters\n        entity=cfg.finetune.wandb_entity_name,\n        project=cfg.finetune.wandb_project_name,\n        config=config_for_wandb,  # type: ignore\n        resume=resume,\n        id=wandb_id,\n        tags=cfg.finetune.tags,\n    )\n    if not isinstance(run, wandb_run.Run):\n        raise ValueError(\"W&amp;B init failed\")\n    return run\n</code></pre>"},{"location":"reference/finetune/lora/","title":"Lora","text":""},{"location":"reference/finetune/optim/","title":"Optim","text":"<p>Classes:</p> <ul> <li> <code>Lion</code>           \u2013            <p>PyTorch implementation of the Lion optimizer from https://github.com/google/automl/blob/master/lion/lion_pytorch.py</p> </li> </ul>"},{"location":"reference/finetune/optim/#tapeagents.finetune.optim.Lion","title":"<code>Lion</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>PyTorch implementation of the Lion optimizer from https://github.com/google/automl/blob/master/lion/lion_pytorch.py</p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize the hyperparameters.</p> </li> <li> <code>step</code>             \u2013              <p>Performs a single optimization step.</p> </li> </ul> Source code in <code>tapeagents/finetune/optim.py</code> <pre><code>class Lion(Optimizer):\n    r\"\"\"PyTorch implementation of the Lion optimizer from https://github.com/google/automl/blob/master/lion/lion_pytorch.py\"\"\"\n\n    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n        \"\"\"Initialize the hyperparameters.\n\n        Args:\n          params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n          lr (float, optional): learning rate (default: 1e-4)\n          betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.99))\n          weight_decay (float, optional): weight decay coefficient (default: 0)\n        \"\"\"\n\n        if not 0.0 &lt;= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 &lt;= betas[0] &lt; 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 &lt;= betas[1] &lt; 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Args:\n          closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n\n        Returns:\n          (tensor): the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                # Perform stepweight decay\n                p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n\n                grad = p.grad\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p)\n\n                exp_avg = state[\"exp_avg\"]\n                beta1, beta2 = group[\"betas\"]\n\n                # Weight update\n                update = exp_avg * beta1 + grad * (1 - beta1)\n                p.add_(torch.sign(update), alpha=-group[\"lr\"])\n                # Decay the momentum running average coefficient\n                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n\n        return loss\n</code></pre>"},{"location":"reference/finetune/optim/#tapeagents.finetune.optim.Lion.__init__","title":"<code>__init__(params, lr=0.0001, betas=(0.9, 0.99), weight_decay=0.0)</code>","text":"<p>Initialize the hyperparameters.</p> <p>Parameters:</p> <ul> <li> <code>params</code>               (<code>iterable</code>)           \u2013            <p>iterable of parameters to optimize or dicts defining parameter groups</p> </li> <li> <code>lr</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>learning rate (default: 1e-4)</p> </li> <li> <code>betas</code>               (<code>Tuple[float, float]</code>, default:                   <code>(0.9, 0.99)</code> )           \u2013            <p>coefficients used for computing running averages of gradient and its square (default: (0.9, 0.99))</p> </li> <li> <code>weight_decay</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>weight decay coefficient (default: 0)</p> </li> </ul> Source code in <code>tapeagents/finetune/optim.py</code> <pre><code>def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n    \"\"\"Initialize the hyperparameters.\n\n    Args:\n      params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n      lr (float, optional): learning rate (default: 1e-4)\n      betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.99))\n      weight_decay (float, optional): weight decay coefficient (default: 0)\n    \"\"\"\n\n    if not 0.0 &lt;= lr:\n        raise ValueError(\"Invalid learning rate: {}\".format(lr))\n    if not 0.0 &lt;= betas[0] &lt; 1.0:\n        raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n    if not 0.0 &lt;= betas[1] &lt; 1.0:\n        raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n    defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/finetune/optim/#tapeagents.finetune.optim.Lion.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> <p>Parameters:</p> <ul> <li> <code>closure</code>               (<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>A closure that reevaluates the model and returns the loss.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tensor</code>           \u2013            <p>the loss.</p> </li> </ul> Source code in <code>tapeagents/finetune/optim.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure=None):\n    \"\"\"Performs a single optimization step.\n\n    Args:\n      closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.\n\n    Returns:\n      (tensor): the loss.\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n\n            # Perform stepweight decay\n            p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n\n            grad = p.grad\n            state = self.state[p]\n            # State initialization\n            if len(state) == 0:\n                # Exponential moving average of gradient values\n                state[\"exp_avg\"] = torch.zeros_like(p)\n\n            exp_avg = state[\"exp_avg\"]\n            beta1, beta2 = group[\"betas\"]\n\n            # Weight update\n            update = exp_avg * beta1 + grad * (1 - beta1)\n            p.add_(torch.sign(update), alpha=-group[\"lr\"])\n            # Decay the momentum running average coefficient\n            exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n\n    return loss\n</code></pre>"},{"location":"reference/finetune/types/","title":"Types","text":""},{"location":"reference/finetune/rl/","title":"RL","text":"<p>Modules:</p> <ul> <li> <code>utils</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>populate_rl_data</code>             \u2013              <p>Populates a dataset with reinforcement learning specific data columns.</p> </li> <li> <code>prepare_rl_fields</code>             \u2013              <p>Convert reward per agent step to reward per token and add returns and advantages placeholders</p> </li> <li> <code>rl_step</code>             \u2013              <p>Perform a single RL step on the model using the given batch and config.</p> </li> <li> <code>update_rewards_and_advantages</code>             \u2013              <p>Updates the advantages column in the given dataset based on reward statistics.</p> </li> </ul>"},{"location":"reference/finetune/rl/#tapeagents.finetune.rl.populate_rl_data","title":"<code>populate_rl_data(dataset, columns, collate_fn, config)</code>","text":"<p>Populates a dataset with reinforcement learning specific data columns.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The input dataset to populate with RL data</p> </li> <li> <code>columns</code>               (<code>list[str]</code>)           \u2013            <p>List of column names to include in the dataset</p> </li> <li> <code>collate_fn</code>               (<code>Callable</code>)           \u2013            <p>Function to collate/batch the data</p> </li> <li> <code>config</code>               (<code>RLConfig</code>)           \u2013            <p>Configuration object containing RL training parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The dataset populated with RL-specific columns including rewards and advantages</p> </li> </ul> Source code in <code>tapeagents/finetune/rl/__init__.py</code> <pre><code>def populate_rl_data(\n    dataset: Dataset,\n    columns: list[str],\n    collate_fn: Callable,\n    config: RLConfig,\n) -&gt; Dataset:\n    \"\"\"\n    Populates a dataset with reinforcement learning specific data columns.\n\n    Args:\n        dataset (Dataset): The input dataset to populate with RL data\n        columns (list[str]): List of column names to include in the dataset\n        collate_fn (Callable): Function to collate/batch the data\n        config (RLConfig): Configuration object containing RL training parameters\n\n    Returns:\n        Dataset: The dataset populated with RL-specific columns including rewards and advantages\n    \"\"\"\n\n    logger.info(\"Populate RL Data\")\n\n    dataset = update_rewards_and_advantages(dataset, config)\n\n    logger.info(\"Finish Populate RL Data\")\n    return dataset\n</code></pre>"},{"location":"reference/finetune/rl/#tapeagents.finetune.rl.prepare_rl_fields","title":"<code>prepare_rl_fields(encoding, reward, old_logprobs, ref_logprobs)</code>","text":"<p>Convert reward per agent step to reward per token and add returns and advantages placeholders</p> Source code in <code>tapeagents/finetune/rl/__init__.py</code> <pre><code>def prepare_rl_fields(\n    encoding: BatchEncoding,\n    reward: float,\n    old_logprobs: list[float],\n    ref_logprobs: list[float],\n) -&gt; BatchEncoding:\n    \"\"\"\n    Convert reward per agent step to reward per token and add returns and advantages placeholders\n    \"\"\"\n    target_tokens = [token for token in encoding[\"labels\"] if token != -100]\n    assert len(target_tokens) == len(\n        old_logprobs\n    ), f\"Target tokens: {len(target_tokens)}, old logprobs: {len(old_logprobs)}\"\n\n    encoding[\"rewards\"] = [reward] * len(encoding[\"labels\"])\n    encoding[\"advantages\"] = [0.0] * len(encoding[\"labels\"])  # place holder\n    encoding[\"old_logprobs\"] = [0] * (len(encoding[\"labels\"]) - len(old_logprobs)) + old_logprobs\n    encoding[\"ref_logprobs\"] = [0] * (len(encoding[\"labels\"]) - len(ref_logprobs)) + ref_logprobs\n    return encoding\n</code></pre>"},{"location":"reference/finetune/rl/#tapeagents.finetune.rl.rl_step","title":"<code>rl_step(model, batch, config)</code>","text":"<p>Perform a single RL step on the model using the given batch and config.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>PreTrainedModel</code>)           \u2013            <p>The model to train</p> </li> <li> <code>batch</code>               (<code>dict</code>)           \u2013            <p>Batch of data containing rewards, advantages, masks, input_ids etc.</p> </li> <li> <code>config</code>               (<code>RLConfig</code>)           \u2013            <p>Configuration for the RL training</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, dict[str, float]]</code>           \u2013            <p>tuple[torch.Tensor, dict[str, float]]: Loss tensor and metrics dictionary</p> </li> </ul> Source code in <code>tapeagents/finetune/rl/__init__.py</code> <pre><code>def rl_step(model: PreTrainedModel, batch: dict, config: RLConfig) -&gt; tuple[torch.Tensor, dict[str, float]]:\n    \"\"\"\n    Perform a single RL step on the model using the given batch and config.\n\n    Args:\n        model (PreTrainedModel): The model to train\n        batch (dict): Batch of data containing rewards, advantages, masks, input_ids etc.\n        config (RLConfig): Configuration for the RL training\n\n    Returns:\n        tuple[torch.Tensor, dict[str, float]]: Loss tensor and metrics dictionary\n\n    \"\"\"\n    rewards = batch.pop(\"rewards\")[:, 1:]\n    advantages = batch.pop(\"advantages\")[:, 1:]\n    masks = batch[\"labels\"] != -100\n    outputs = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"],\n    )\n\n    new_log_probs = torch.gather(\n        F.log_softmax(outputs.logits[:, :-1, :], dim=-1),  # the last log probs has no target\n        dim=2,\n        index=batch[\"input_ids\"][:, 1:].unsqueeze(2),\n    ).squeeze(2)\n\n    masks_ = masks[:, 1:]\n    ref_logprobs = batch[\"ref_logprobs\"][:, 1:]\n    old_logprobs = batch[\"old_logprobs\"][:, 1:]\n    assert new_log_probs.shape == ref_logprobs.shape\n\n    # First compute the PPO surrogate loss, see https://arxiv.org/pdf/2402.03300 eq 3\n    log_ratio_new_old = new_log_probs - old_logprobs\n    ratio_new_old = torch.exp(log_ratio_new_old)\n    log_p_weights = advantages if config.use_advantages else rewards\n    log_p_weights = torch.clamp(log_p_weights, min=0) if config.relu_log_p_weights else log_p_weights\n    # Second compute the approximated KL, see https://arxiv.org/pdf/2402.03300 eq 4\n    log_ratio_ref_new = ref_logprobs - new_log_probs\n    approx_kl = torch.exp(log_ratio_ref_new) - log_ratio_ref_new - 1  # Schulman KL approx\n    match config.algo:\n        case \"grpo\":\n            # GRPO is based on https://arxiv.org/pdf/2402.03300\n            surr1 = ratio_new_old * log_p_weights\n\n            clamped_ratio = torch.clamp(ratio_new_old, 1 - config.epsilon, 1 + config.epsilon)\n\n            surr2 = clamped_ratio * log_p_weights\n\n            surrogate_loss = torch.min(surr1, surr2)\n\n            assert approx_kl.shape == masks_.shape\n            assert approx_kl.shape == surrogate_loss.shape\n            loss = -masked_mean(surrogate_loss - config.kl_coef * approx_kl, masks_)\n        case \"reinforce\":\n            surr1 = torch.zeros_like(ratio_new_old)\n            surr2 = torch.zeros_like(ratio_new_old)\n            loss = -masked_mean(new_log_probs * log_p_weights - config.kl_coef * approx_kl, masks_)\n        case _:\n            raise ValueError(f\"Unknown algorithm {config.algo}\")\n    assert torch.isfinite(loss).all(), \"loss contains NaN or inf\"\n\n    stats = {\n        \"max_new_log_probs\": new_log_probs[masks_].max().item(),\n        \"max_ratio_new_old\": ratio_new_old[masks_].max().item(),\n        \"max_loss\": loss.max().item(),\n        \"reward\": masked_mean(rewards, masks_).item(),\n        \"max_reward\": rewards[masks_].max().item(),\n        \"min_reward\": rewards[masks_].min().item(),\n        \"mean_old_logprobs\": masked_mean(old_logprobs, masks_).item(),\n        \"mean_new_logprobs\": masked_mean(new_log_probs, masks_).item(),\n        \"mean_new_logprobs_positive_log_p_weights\": masked_mean(\n            new_log_probs[log_p_weights &gt; 0], masks_[log_p_weights &gt; 0]\n        ).item()\n        if (log_p_weights &gt; 0).any()\n        else 0,\n        \"mean_new_logprobs_negative_log_p_weights\": masked_mean(\n            new_log_probs[log_p_weights &lt; 0], masks_[log_p_weights &lt; 0]\n        ).item()\n        if (log_p_weights &lt; 0).any()\n        else 0,\n        \"mean_ref_logprobs\": masked_mean(ref_logprobs, masks_).item(),\n        \"advantage\": masked_mean(advantages, masks_).item(),\n        \"max_advantage\": advantages[masks_].max().item(),\n        \"min_advantage\": advantages[masks_].min().item(),\n        \"loss\": loss.item(),\n        \"kl\": masked_mean(approx_kl, masks_).item(),\n        \"max_kl\": approx_kl[masks_].max().item(),\n        \"min_kl\": approx_kl[masks_].min().item(),\n        \"surr1\": masked_mean(surr1, masks_).item(),\n        \"surr2\": masked_mean(surr2, masks_).item(),\n        \"ratio_new_old\": masked_mean(ratio_new_old, masks_).item(),\n        \"ratio_ref_new\": masked_mean(torch.exp(log_ratio_ref_new), masks_).item(),\n        \"ratio_ref_old\": masked_mean(torch.exp(ref_logprobs - old_logprobs), masks_).item(),\n    }\n    return loss, stats\n</code></pre>"},{"location":"reference/finetune/rl/#tapeagents.finetune.rl.update_rewards_and_advantages","title":"<code>update_rewards_and_advantages(dataset, config)</code>","text":"<p>Updates the advantages column in the given dataset based on reward statistics.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The input dataset containing rewards and placeholder advantages.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>The updated dataset with the updated advantages column.</p> </li> </ul> Source code in <code>tapeagents/finetune/rl/__init__.py</code> <pre><code>def update_rewards_and_advantages(dataset: Dataset, config: RLConfig) -&gt; Dataset:\n    \"\"\"\n    Updates the advantages column in the given dataset based on reward statistics.\n\n    Args:\n        dataset (Dataset): The input dataset containing rewards and placeholder advantages.\n\n    Returns:\n        Dataset: The updated dataset with the updated advantages column.\n\n    \"\"\"\n    df = dataset.to_pandas()\n\n    if config.reward_minus_kl_coef &gt; 0:\n        logger.info(\"Updating Reward with Implicit KL\")\n        calculate_reward_with_implicit_kl_ = partial(\n            calculate_reward_with_implicit_kl, reward_minus_kl_coef=config.reward_minus_kl\n        )\n        df[\"reward\"] = df.apply(calculate_reward_with_implicit_kl_, axis=1)\n\n    # Group by group_id and compute mean and std of reward\n    grouped = df.groupby(\"group_id\")[\"reward\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n\n    # Rename columns for clarity\n    grouped.columns = [\"group_id\", \"reward_mean\", \"reward_std\", \"count\"]\n\n    # Merge the computed statistics back to the original dataset\n    df_with_stats = pd.merge(df, grouped, on=\"group_id\", how=\"left\")\n\n    df_with_stats[\"advantages\"] = df_with_stats.apply(calculate_advantage, axis=1)\n\n    # replace advantages entry\n    dataset = replace_dataset_column(dataset, \"advantages\", df_with_stats[\"advantages\"].tolist())\n\n    # Convert back to a Hugging Face Dataset\n    return dataset\n</code></pre>"},{"location":"reference/finetune/rl/utils/","title":"Utils","text":"<p>Functions:</p> <ul> <li> <code>calculate_advantage</code>             \u2013              <p>Calculate advantage values for a row of data.</p> </li> <li> <code>calculate_reward_with_implicit_kl</code>             \u2013              <p>Calculate reward with implicit KL penalty.</p> </li> <li> <code>masked_mean</code>             \u2013              <p>Compute mean of tensor with a masked values.</p> </li> <li> <code>masked_sum</code>             \u2013              <p>Compute sum of tensor with a masked values.</p> </li> <li> <code>replace_dataset_column</code>             \u2013              <p>Replace a column in the dataset with a new column.</p> </li> </ul>"},{"location":"reference/finetune/rl/utils/#tapeagents.finetune.rl.utils.calculate_advantage","title":"<code>calculate_advantage(row)</code>","text":"<p>Calculate advantage values for a row of data.</p> <p>Parameters:</p> <ul> <li> <code>row</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing rewards and statistics with keys:</p> <ul> <li>rewards: List of reward values</li> <li>reward_mean: Mean reward value</li> <li>reward_std: Standard deviation of rewards</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of advantage values calculated as (reward - mean)/(std + eps)   where eps=1e-4 is added for numerical stability</p> </li> </ul> Source code in <code>tapeagents/finetune/rl/utils.py</code> <pre><code>def calculate_advantage(row):\n    \"\"\"\n    Calculate advantage values for a row of data.\n\n    Args:\n        row (dict): Dictionary containing rewards and statistics with keys:\n\n            - rewards: List of reward values\n            - reward_mean: Mean reward value\n            - reward_std: Standard deviation of rewards\n\n    Returns:\n       (list[float]): List of advantage values calculated as (reward - mean)/(std + eps)\n            where eps=1e-4 is added for numerical stability\n    \"\"\"\n    rewards = row[\"rewards\"]\n    mean = row[\"reward_mean\"]\n    std = row[\"reward_std\"]\n    return [(reward - mean) / (np.nan_to_num(std) + 1e-4) for reward in rewards]\n</code></pre>"},{"location":"reference/finetune/rl/utils/#tapeagents.finetune.rl.utils.calculate_reward_with_implicit_kl","title":"<code>calculate_reward_with_implicit_kl(row, reward_minus_kl_coef)</code>","text":"<p>Calculate reward with implicit KL penalty.</p> <p>Parameters:</p> <ul> <li> <code>row</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing reward and log probability data with keys:</p> <ul> <li>reward: Base reward value</li> <li>old_logprobs: Log probabilities from old policy</li> <li>ref_logprobs: Reference log probabilities</li> </ul> </li> <li> <code>reward_minus_kl_coef</code>               (<code>float</code>)           \u2013            <p>Coefficient for implicit KL penalty term</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Reward value adjusted by implicit KL penalty, calculated as: reward - reward_minus_kl_coef * KL(ref||old) The KL divergence is approximated using the Schulman approximation: KL \u2248 exp(log_ratio) - log_ratio - 1 where log_ratio = ref_logprobs - old_logprobs</p> </li> </ul> Source code in <code>tapeagents/finetune/rl/utils.py</code> <pre><code>def calculate_reward_with_implicit_kl(row, reward_minus_kl_coef):\n    \"\"\"\n    Calculate reward with implicit KL penalty.\n\n    Args:\n        row (dict): Dictionary containing reward and log probability data with keys:\n\n            - reward: Base reward value\n            - old_logprobs: Log probabilities from old policy\n            - ref_logprobs: Reference log probabilities\n        reward_minus_kl_coef (float): Coefficient for implicit KL penalty term\n\n    Returns:\n        (float): Reward value adjusted by implicit KL penalty, calculated as:\n            reward - reward_minus_kl_coef * KL(ref||old)\n            The KL divergence is approximated using the Schulman approximation:\n            KL \u2248 exp(log_ratio) - log_ratio - 1\n            where log_ratio = ref_logprobs - old_logprobs\n    \"\"\"\n    reward = row[\"reward\"]\n    old_logprobs = row[\"old_logprobs\"]\n    ref_logprobs = row[\"ref_logprobs\"]\n    log_ratio_ref_old = ref_logprobs - old_logprobs\n    kl = (np.exp(log_ratio_ref_old) - log_ratio_ref_old - 1).sum()  # Schulman KL approx\n    return reward - reward_minus_kl_coef * kl\n</code></pre>"},{"location":"reference/finetune/rl/utils/#tapeagents.finetune.rl.utils.masked_mean","title":"<code>masked_mean(values, mask, axis=None)</code>","text":"<p>Compute mean of tensor with a masked values.</p> Source code in <code>tapeagents/finetune/rl/utils.py</code> <pre><code>def masked_mean(values: torch.Tensor, mask: torch.Tensor, axis: Optional[bool] = None) -&gt; torch.Tensor:\n    \"\"\"Compute mean of tensor with a masked values.\"\"\"\n    if axis is not None:\n        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)  # type: ignore\n    else:\n        return (values * mask).sum() / mask.sum()\n</code></pre>"},{"location":"reference/finetune/rl/utils/#tapeagents.finetune.rl.utils.masked_sum","title":"<code>masked_sum(values, mask, axis=None)</code>","text":"<p>Compute sum of tensor with a masked values.</p> Source code in <code>tapeagents/finetune/rl/utils.py</code> <pre><code>def masked_sum(values: torch.Tensor, mask: torch.Tensor, axis: Optional[bool] = None) -&gt; torch.Tensor:\n    \"\"\"Compute sum of tensor with a masked values.\"\"\"\n    if axis is not None:\n        return (values * mask).sum(axis=axis)  # type: ignore\n    else:\n        return (values * mask).sum()\n</code></pre>"},{"location":"reference/finetune/rl/utils/#tapeagents.finetune.rl.utils.replace_dataset_column","title":"<code>replace_dataset_column(dataset, column_name, new_column)</code>","text":"<p>Replace a column in the dataset with a new column.</p> Source code in <code>tapeagents/finetune/rl/utils.py</code> <pre><code>def replace_dataset_column(dataset: Dataset, column_name: str, new_column: List[List[float]]) -&gt; Dataset:\n    \"\"\"\n    Replace a column in the dataset with a new column.\n    \"\"\"\n    if column_name in dataset.features:\n        dataset = dataset.map(remove_columns=[column_name])\n    dataset = dataset.add_column(name=column_name, column=new_column)  # type: ignore\n\n    return dataset\n</code></pre>"},{"location":"reference/renderers/","title":"Renderers","text":"<p>Renderers that produce HTML from the tapes used by various GUI scripts.</p> <p>Modules:</p> <ul> <li> <code>basic</code>           \u2013            </li> <li> <code>camera_ready_renderer</code>           \u2013            </li> <li> <code>pretty</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>render_agent_tree</code>             \u2013              <p>Renders an ASCII tree representation of an agent's hierarchical structure.</p> </li> <li> <code>render_dialog_plain_text</code>             \u2013              <p>Renders a dialog tape into a plain text format.</p> </li> <li> <code>render_tape_with_prompts</code>             \u2013              <p>Renders a tape with prompts using the specified renderer.</p> </li> <li> <code>to_pretty_str</code>             \u2013              <p>Convert any Python object to a pretty formatted string representation.</p> </li> </ul>"},{"location":"reference/renderers/#tapeagents.renderers.render_agent_tree","title":"<code>render_agent_tree(agent, show_nodes=True, indent_increment=4)</code>","text":"<p>Renders an ASCII tree representation of an agent's hierarchical structure.</p> <p>This function creates a visual tree diagram showing the relationships between agents, their nodes, and subagents using ASCII characters. Each level is indented to show the hierarchy.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>Agent</code>)           \u2013            <p>The root agent object to render the tree from.</p> </li> <li> <code>show_nodes</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display the agent's nodes in the tree. Defaults to True.</p> </li> <li> <code>indent_increment</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of spaces to indent each level. Defaults to 4.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A string containing the ASCII tree representation.</p> </li> </ul> Example <pre><code>&gt; The Manager\n    .node1\n    .node2\n    &gt; His Assistant 1\n        .node1\n        .node2\n    &gt; His Helper 2\n        .node1\n        .node2\n</code></pre> Source code in <code>tapeagents/renderers/__init__.py</code> <pre><code>def render_agent_tree(agent: Agent, show_nodes: bool = True, indent_increment: int = 4) -&gt; str:\n    \"\"\"\n    Renders an ASCII tree representation of an agent's hierarchical structure.\n\n    This function creates a visual tree diagram showing the relationships between agents,\n    their nodes, and subagents using ASCII characters. Each level is indented to show\n    the hierarchy.\n\n    Args:\n        agent (Agent): The root agent object to render the tree from.\n        show_nodes (bool, optional): Whether to display the agent's nodes in the tree.\n            Defaults to True.\n        indent_increment (int, optional): Number of spaces to indent each level.\n            Defaults to 4.\n\n    Returns:\n        str: A string containing the ASCII tree representation.\n\n    Example:\n        ```\n        &gt; The Manager\n            .node1\n            .node2\n            &gt; His Assistant 1\n                .node1\n                .node2\n            &gt; His Helper 2\n                .node1\n                .node2\n        ```\n    \"\"\"\n\n    def render(agent: Agent, indent: int = 0) -&gt; str:\n        lines = [f\"{' ' * indent}&gt; {agent.name}\"]\n        if show_nodes:\n            lines.extend([f\"{' ' * (indent + indent_increment)}.{node.name}\" for node in agent.nodes])\n        for subagent in agent.subagents:\n            lines.append(render(subagent, indent + indent_increment))\n        return \"\\n\".join(lines)\n\n    return render(agent)\n</code></pre>"},{"location":"reference/renderers/#tapeagents.renderers.render_dialog_plain_text","title":"<code>render_dialog_plain_text(tape)</code>","text":"<p>Renders a dialog tape into a plain text format.</p> <p>Takes a DialogTape object containing conversation steps and formats them into human-readable text, with each dialog step on a new line prefixed by the speaker/action type.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Union[DialogTape, None]</code>)           \u2013            <p>A DialogTape object containing conversation steps, or None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A string containing the formatted dialog, with each step on a new line. Returns empty string if tape is None.</p> </li> </ul> Source code in <code>tapeagents/renderers/__init__.py</code> <pre><code>def render_dialog_plain_text(tape: DialogTape | None) -&gt; str:\n    \"\"\"\n    Renders a dialog tape into a plain text format.\n\n    Takes a DialogTape object containing conversation steps and formats them into human-readable text,\n    with each dialog step on a new line prefixed by the speaker/action type.\n\n    Args:\n        tape (Union[DialogTape, None]): A DialogTape object containing conversation steps, or None\n\n    Returns:\n        str: A string containing the formatted dialog, with each step on a new line.\n            Returns empty string if tape is None.\n    \"\"\"\n    if tape is None:\n        return \"\"\n    lines = []\n    for step in tape:\n        if isinstance(step, UserStep):\n            lines.append(f\"User: {step.content}\")\n        elif isinstance(step, AssistantStep):\n            lines.append(f\"Assistant: {step.content}\")\n        elif isinstance(step, ToolCalls):\n            for tc in step.tool_calls:\n                lines.append(f\"Tool calls: {tc.function}\")\n        elif isinstance(step, ToolResult):\n            lines.append(f\"Tool result: {step.content}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/renderers/#tapeagents.renderers.render_tape_with_prompts","title":"<code>render_tape_with_prompts(tape, renderer)</code>","text":"<p>Renders a tape with prompts using the specified renderer.</p> <p>This function combines the tape's LLM calls with the renderer's style and rendering to produce a complete rendered output of the tape's content.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object to be rendered</p> </li> <li> <code>renderer</code>               (<code>BasicRenderer</code>)           \u2013            <p>The renderer to use for rendering the tape</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The rendered tape content with applied styling, combining both  the renderer's style and the tape's content with LLM calls</p> </li> </ul> Source code in <code>tapeagents/renderers/__init__.py</code> <pre><code>def render_tape_with_prompts(tape: Tape, renderer: BasicRenderer):\n    \"\"\"\n    Renders a tape with prompts using the specified renderer.\n\n    This function combines the tape's LLM calls with the renderer's style and rendering\n    to produce a complete rendered output of the tape's content.\n\n    Args:\n        tape (Tape): The tape object to be rendered\n        renderer (BasicRenderer): The renderer to use for rendering the tape\n\n    Returns:\n        (str): The rendered tape content with applied styling, combining both\n             the renderer's style and the tape's content with LLM calls\n    \"\"\"\n    llm_calls = retrieve_tape_llm_calls(tape)\n    return renderer.style + renderer.render_tape(tape, llm_calls)\n</code></pre>"},{"location":"reference/renderers/#tapeagents.renderers.to_pretty_str","title":"<code>to_pretty_str(a, prefix='', indent=2)</code>","text":"<p>Convert any Python object to a pretty formatted string representation.</p> <p>This function recursively formats nested data structures (lists and dictionaries) with proper indentation and line breaks for improved readability.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>Any</code>)           \u2013            <p>The object to be formatted. Can be a list, dictionary, or any other type.</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>String to prepend to each line. Used for recursive indentation. Defaults to \"\".</p> </li> <li> <code>indent</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of spaces to use for each level of indentation. Defaults to 2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A formatted string representation of the input object.</p> </li> </ul> Example <pre><code>data = {\"foo\": [1, 2, {\"bar\": \"baz\"}]}\nprint(to_pretty_str(data))\n</code></pre> <p>foo:   - 1   - 2   - bar: baz</p> Source code in <code>tapeagents/renderers/__init__.py</code> <pre><code>def to_pretty_str(a: Any, prefix: str = \"\", indent: int = 2) -&gt; str:\n    \"\"\"Convert any Python object to a pretty formatted string representation.\n\n    This function recursively formats nested data structures (lists and dictionaries)\n    with proper indentation and line breaks for improved readability.\n\n    Args:\n        a (Any): The object to be formatted. Can be a list, dictionary, or any other type.\n        prefix (str, optional): String to prepend to each line. Used for recursive indentation. Defaults to \"\".\n        indent (int, optional): Number of spaces to use for each level of indentation. Defaults to 2.\n\n    Returns:\n        str: A formatted string representation of the input object.\n\n    Example:\n        ```python\n        data = {\"foo\": [1, 2, {\"bar\": \"baz\"}]}\n        print(to_pretty_str(data))\n        ```\n        foo:\n          - 1\n          - 2\n          - bar: baz\n    \"\"\"\n    view = \"\"\n    if isinstance(a, list) and len(a):\n        if len(str(a)) &lt; 80:\n            view = str(a)\n        else:\n            lines = []\n            for item in a:\n                value_view = to_pretty_str(item, prefix + \" \" * indent)\n                if \"\\n\" in value_view:\n                    value_view = f\"\\n{value_view}\"\n                lines.append(f\"{prefix}- \" + value_view)\n            view = \"\\n\".join(lines)\n    elif isinstance(a, dict) and len(a):\n        lines = []\n        for k, v in a.items():\n            value_view = to_pretty_str(v, prefix + \" \" * indent)\n            if \"\\n\" in value_view:\n                value_view = f\"\\n{value_view}\"\n            lines.append(f\"{prefix}{k}: {value_view}\")\n        view = \"\\n\".join(lines)\n    else:\n        view = str(a)\n    return view\n</code></pre>"},{"location":"reference/renderers/basic/","title":"Basic","text":"<p>Classes:</p> <ul> <li> <code>BasicRenderer</code>           \u2013            <p>A basic renderer for displaying tapes in HTML format.</p> </li> </ul>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer","title":"<code>BasicRenderer</code>","text":"<p>A basic renderer for displaying tapes in HTML format.</p> <p>This class provides functionality to render tapes and LLM calls in a structured HTML format with customizable styling and filtering options.</p> <p>Attributes:</p> <ul> <li> <code>metadata_header</code>               (<code>str</code>)           \u2013            <p>HTML header for metadata section</p> </li> <li> <code>context_header</code>               (<code>str</code>)           \u2013            <p>HTML header for context section</p> </li> <li> <code>steps_header</code>               (<code>str</code>)           \u2013            <p>HTML header for steps section</p> </li> <li> <code>agent_tape_header</code>               (<code>str</code>)           \u2013            <p>HTML header for agent tape section</p> </li> <li> <code>user_tape_header</code>               (<code>str</code>)           \u2013            <p>HTML header for user tapes section</p> </li> <li> <code>annotator_tape_header</code>               (<code>str</code>)           \u2013            <p>HTML header for annotator tapes section</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>filter_steps</code>               (<code>Optional[tuple[Type, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Types of steps to include in rendering. If None, all steps are rendered.</p> </li> <li> <code>render_llm_calls</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to render LLM calls. Defaults to True.</p> </li> <li> <code>render_agent_node</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to render agent node information. Defaults to False.</p> </li> </ul> Example <pre><code>renderer = BasicRenderer(render_llm_calls=True)\nhtml_output = renderer.render_tape(tape)\n</code></pre> <p>The renderer supports:</p> <ul> <li>Tape rendering with metadata, context, and steps</li> <li>Episode rendering with user, agent, and annotator columns</li> <li>LLM call rendering with prompts and outputs</li> <li>Custom styling through CSS</li> <li>Step filtering</li> <li>Collapsible sections using HTML details tags</li> </ul> <p>Methods:</p> <ul> <li> <code>render_episode</code>             \u2013              <p>Renders an episode into HTML format.</p> </li> <li> <code>render_llm_call</code>             \u2013              <p>Renders an LLM call into HTML format.</p> </li> <li> <code>render_step</code>             \u2013              <p>Renders a single step in the process.</p> </li> <li> <code>render_steps</code>             \u2013              <p>Renders a sequence of steps from a tape into an HTML string representation.</p> </li> <li> <code>render_tape</code>             \u2013              <p>Render a tape object into HTML representation.</p> </li> </ul> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>class BasicRenderer:\n    \"\"\"A basic renderer for displaying tapes in HTML format.\n\n    This class provides functionality to render tapes and LLM calls in a structured HTML format\n    with customizable styling and filtering options.\n\n    Attributes:\n        metadata_header (str): HTML header for metadata section\n        context_header (str): HTML header for context section\n        steps_header (str): HTML header for steps section\n        agent_tape_header (str): HTML header for agent tape section\n        user_tape_header (str): HTML header for user tapes section\n        annotator_tape_header (str): HTML header for annotator tapes section\n\n    Args:\n        filter_steps (Optional[tuple[Type, ...]]): Types of steps to include in rendering. If None, all steps are rendered.\n        render_llm_calls (bool): Whether to render LLM calls. Defaults to True.\n        render_agent_node (bool): Whether to render agent node information. Defaults to False.\n\n    Example:\n        ```python\n        renderer = BasicRenderer(render_llm_calls=True)\n        html_output = renderer.render_tape(tape)\n        ```\n\n    The renderer supports:\n\n    - Tape rendering with metadata, context, and steps\n    - Episode rendering with user, agent, and annotator columns\n    - LLM call rendering with prompts and outputs\n    - Custom styling through CSS\n    - Step filtering\n    - Collapsible sections using HTML details tags\n    \"\"\"\n\n    metadata_header = \"&lt;h3 style='margin: 2px'&gt;Metadata&lt;/h3&gt;\"\n    context_header = \"&lt;h3 style='margin: 2px'&gt;Context&lt;/h3&gt;\"\n    steps_header = \"&lt;h3 style='margin: 2px'&gt;Steps&lt;/h3&gt;\"\n    agent_tape_header = \"&lt;h1&gt; Agent Tape &lt;/h1&gt;\"\n    user_tape_header = \"&lt;h1&gt; User Tapes &lt;/h1&gt;\"\n    annotator_tape_header = \"&lt;h1&gt; Annotator Tapes &lt;/h1&gt;\"\n\n    def __init__(\n        self,\n        filter_steps: tuple[Type, ...] | None = None,\n        render_llm_calls: bool = True,\n        render_agent_node: bool = False,\n    ):\n        self.filter_steps = filter_steps\n        self.render_llm_calls = render_llm_calls\n        self.render_agent_node = render_agent_node\n\n    @property\n    def style(self) -&gt; str:\n        return (\n            \"&lt;style&gt;\"\n            \".basic-renderer-box { margin: 4px; padding: 2px; padding-left: 6px; background: lavender; white-space: pre-wrap; color: black;}\"\n            \".basic-prompt-box { margin: 4px; padding: 2px; padding-left: 6px; background: lavender; color: black;}\"\n            \".episode-row { display: flex; align-items: end; }\"\n            \".agent-column { width: 50%; }\"\n            \".user-column { width: 25%; }\"\n            \".annotator-column { width: 25%; }\"\n            \".inner-tape-container { display: flex }\"\n            \".inner-tape-indent { width: 10%; }\"\n            \".inner-tape { width: 90%; }\"\n            \".agent_node { text-align: center; position: relative; margin: 12px 0; }\"\n            \".agent_node hr { border: 1px solid #e1e1e1; margin: 0 !important;}\"\n            \".agent_node span { position: absolute; right: 0; top: -0.6em; background: white; color: grey !important; padding: 0 10px; }\"\n            \"&lt;/style&gt;\"\n        )\n\n    def render_as_box(self, data: Any):\n        if isinstance(data, dict):\n            str_ = yaml.dump(data, indent=2)\n        else:\n            str_ = str(data)\n        return f\"&lt;div class='basic-renderer-box'&gt;{str_}&lt;/div&gt;\"\n\n    def render_metadata(self, tape: Tape):\n        return f\"&lt;details&gt; &lt;summary&gt;id: {tape.metadata.id}&lt;/summary&gt; {self.render_as_box(tape.metadata.model_dump())} &lt;/details&gt;\"\n\n    def render_context(self, tape: Tape):\n        if isinstance(tape.context, Tape):\n            summary = f\"Tape of {len(tape.context)} steps.\" f\"&lt;div&gt;ID: {tape.context.metadata.id}&lt;/div&gt;\"\n            return (\n                \"&lt;div class=inner-tape-container&gt;\"\n                \"&lt;div class=inner-tape-indent&gt; &lt;/div&gt;\"\n                \"&lt;div class=inner-tape&gt; &lt;details&gt;\"\n                f\"&lt;summary&gt;{summary}&lt;/b&gt;&lt;/summary&gt;\"\n                f\"{self.render_tape(tape.context)}&lt;/details&gt;\"\n                \"&lt;/div&gt;&lt;/div&gt;\"\n            )\n        else:\n            context_str = tape.context.model_dump() if isinstance(tape.context, BaseModel) else tape.context\n            return self.render_as_box(context_str)\n\n    def render_step(self, step: Step, index: int, **kwargs) -&gt; str:\n        \"\"\"\n        Renders a single step in the process.\n\n        Args:\n            step (Step): The step object containing the data to be rendered\n            index (int): The index of the current step\n            **kwargs (dict, optional): Additional keyword arguments for rendering customization\n\n        Returns:\n            str: The rendered step as a formatted string in box format\n\n        Note:\n            The step is first converted to a dictionary using model_dump() before rendering.\n            The rendering is done using the render_as_box method.\n        \"\"\"\n        step_dict = step.model_dump()\n        return self.render_as_box(step_dict)\n\n    def render_steps(self, tape: Tape, llm_calls: dict[str, LLMCall] = {}) -&gt; str:\n        \"\"\"\n        Renders a sequence of steps from a tape into an HTML string representation.\n\n        This method processes each step in the tape and generates HTML chunks based on various rendering options.\n        It can show agent nodes, LLM calls, and individual steps based on configuration.\n\n        Args:\n            tape (Tape): The tape object containing the sequence of steps to render\n            llm_calls (dict[str, LLMCall], optional): Dictionary mapping prompt IDs to LLM calls. Defaults to {}.\n\n        Returns:\n            str: A concatenated HTML string containing all rendered chunks\n\n        Notes:\n            - If filter_steps is set, only steps matching those types will be rendered\n            - Agent nodes are rendered as dividers with agent/node names if render_agent_node is True\n            - LLM calls are rendered for each unique prompt_id if render_llm_calls is True\n            - Steps from UserStep and Observation are treated as \"Environment\" agent\n        \"\"\"\n        chunks = []\n        last_prompt_id = None\n        last_agent_node = None\n        for index, step in enumerate(tape):\n            if self.filter_steps and not isinstance(step, self.filter_steps):\n                continue\n            if self.render_agent_node:\n                if isinstance(step, UserStep) or isinstance(step, Observation):\n                    agent = \"Environment\"\n                    node = \"\"\n                else:\n                    agent = step.metadata.agent.split(\"/\")[-1]\n                    node = step.metadata.node\n                agent_node = agent + (f\".{node}\" if node else \"\")\n                if agent_node != last_agent_node:\n                    chunks.append(f\"\"\"&lt;div class=\"agent_node\"&gt;&lt;hr&gt;&lt;span&gt;{agent_node}&lt;/span&gt;&lt;/div&gt;\"\"\")\n                    last_agent_node = agent_node\n            if self.render_llm_calls:\n                if step.metadata.prompt_id != last_prompt_id:\n                    llm_call = llm_calls.get(step.metadata.prompt_id)\n                    if llm_call:\n                        chunks.append(self.render_llm_call(llm_call))\n                    last_prompt_id = step.metadata.prompt_id\n            chunks.append(self.render_step(step, index))\n        return \"\".join(chunks)\n\n    def render_tape(self, tape: Tape, llm_calls: dict[str, LLMCall] = {}) -&gt; str:\n        \"\"\"\n        Render a tape object into HTML representation.\n\n        Args:\n            tape (Tape): The tape object to render.\n            llm_calls (dict[str, LLMCall], optional): Dictionary of LLM calls associated with the tape. Defaults to {}.\n\n        Returns:\n            str: HTML representation of the tape including metadata, context (if present), and steps.\n\n        The rendered HTML includes:\n\n        - Metadata section with tape metadata\n        - Context section (if tape.context exists)\n        - Steps section with tape execution steps\n        \"\"\"\n        metadata_html = self.render_metadata(tape)\n        context_html = self.render_context(tape)\n        steps_html = self.render_steps(tape, llm_calls)\n        return (\n            f\"{self.metadata_header}{metadata_html}\"\n            + (f\"{self.context_header}{context_html}\" if tape.context is not None else \"\")\n            + f\"{self.steps_header}{steps_html}\"\n        )\n\n    def render_episode(self, episode: Episode) -&gt; str:\n        \"\"\"Renders an episode into HTML format.\n\n        Takes an Episode object and converts it into an HTML string representation with three columns:\n        user, agent, and annotator. The rendering includes headers, context, and sequential steps\n        organized in rows.\n\n        Args:\n            episode (Episode): Episode object containing the interaction sequence to be rendered\n\n        Returns:\n            str: HTML string representation of the episode with formatted columns and rows\n        \"\"\"\n        chunks = []\n\n        def wrap_agent(html: str) -&gt; str:\n            return f\"&lt;div class='agent-column'&gt;{html}&lt;/div&gt;\"\n\n        def wrap_user(html: str) -&gt; str:\n            return f\"&lt;div class='user-column'&gt;{html}&lt;/div&gt;\"\n\n        def wrap_annotator(html: str) -&gt; str:\n            return f\"&lt;div class='annotator-column'&gt;{html}&lt;/div&gt;\"\n\n        def row(user: str, agent: str, annotator: str):\n            return f\"&lt;div class='episode-row'&gt;{wrap_user(user)}{wrap_agent(agent)}{wrap_annotator(annotator)}&lt;/div&gt;\"\n\n        chunks.append(row(self.user_tape_header, self.agent_tape_header, self.annotator_tape_header))\n        chunks.append(row(\"\", self.context_header, \"\"))\n        chunks.append(row(\"\", self.render_context(episode.tape), \"\"))\n        chunks.append(row(\"\", self.steps_header, \"\"))\n        for index, (user_tape, step, annotator_tapes) in enumerate(episode.group_by_step()):\n            if user_tape:\n                user_html = f\"{self.steps_header}{self.render_steps(user_tape)}\"\n            else:\n                user_html = \"\"\n            agent_html = self.render_step(step, index)\n            annotations_html = \"\".join([f\"{self.steps_header}{self.render_steps(tape)}\" for tape in annotator_tapes])\n            chunks.append(row(user_html, agent_html, annotations_html))\n        return \"\".join(chunks)\n\n    def render_llm_call(self, llm_call: LLMCall | None) -&gt; str:\n        \"\"\"Renders an LLM call into HTML format.\n\n        This method generates HTML representation of an LLM call, including both the prompt\n        and output (if available). The HTML includes expandable details sections for both\n        the prompt and response.\n\n        Args:\n            llm_call (Union[LLMCall, None]): An LLM call object containing prompt and output information.\n                If None, returns an empty string.\n\n        Returns:\n            str: HTML string representation of the LLM call. The HTML contains:\n\n            - Prompt section with:\n                - Summary showing token/character count and cache status\n                - Expandable details with prompt messages\n            - Output section (if output exists) with:\n                - Summary showing token count\n                - Expandable details with LLM response\n\n        The rendered HTML uses collapsible details elements and basic styling for\n        readability, with a light yellow background color.\n        \"\"\"\n        if llm_call is None:\n            return \"\"\n        if llm_call.prompt.tools:\n            prompt_messages = [f\"tool_schemas: {json.dumps(llm_call.prompt.tools, indent=2)}\"]\n        else:\n            prompt_messages = []\n        for m in llm_call.prompt.messages:\n            role = f\"{m['role']} ({m['name']})\" if \"name\" in m else m[\"role\"]\n            prompt_messages.append(f\"{role}: {m['content'] if 'content' in m else m['tool_calls']}\")\n        prompt_text = \"\\n--\\n\".join(prompt_messages)\n        prompt_length_str = (\n            f\"{llm_call.prompt_length_tokens} tokens\"\n            if llm_call.prompt_length_tokens\n            else f\"{len(prompt_text)} characters\"\n        )\n        label = f\"Prompt {prompt_length_str} {', cached' if llm_call.cached else ''}\"\n        html = f\"\"\"&lt;div class='basic-prompt-box' style='background-color:#ffffba; margin: 0 4px;'&gt;\n        &lt;details&gt;\n            &lt;summary&gt;{label}&lt;/summary&gt;\n            &lt;pre style='font-size: 12px; white-space: pre-wrap;word-wrap: break-word;'&gt;{prompt_text.strip()}&lt;/pre&gt;\n        &lt;/details&gt;\n        &lt;/div&gt;\"\"\"\n        if llm_call.output:\n            html += f\"\"\"&lt;div class='basic-prompt-box' style='background-color:#ffffba; margin: 0 4px;'&gt;\n                &lt;details&gt;\n                    &lt;summary&gt;LLM Output {llm_call.output_length_tokens} tokens&lt;/summary&gt;\n                    &lt;pre style='font-size: 12px; white-space: pre-wrap; word-wrap: break-word;'&gt;{llm_call.output}&lt;/pre&gt;\n                &lt;/details&gt;\n                &lt;/div&gt;\"\"\"\n        return html\n</code></pre>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer.render_episode","title":"<code>render_episode(episode)</code>","text":"<p>Renders an episode into HTML format.</p> <p>Takes an Episode object and converts it into an HTML string representation with three columns: user, agent, and annotator. The rendering includes headers, context, and sequential steps organized in rows.</p> <p>Parameters:</p> <ul> <li> <code>episode</code>               (<code>Episode</code>)           \u2013            <p>Episode object containing the interaction sequence to be rendered</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>HTML string representation of the episode with formatted columns and rows</p> </li> </ul> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>def render_episode(self, episode: Episode) -&gt; str:\n    \"\"\"Renders an episode into HTML format.\n\n    Takes an Episode object and converts it into an HTML string representation with three columns:\n    user, agent, and annotator. The rendering includes headers, context, and sequential steps\n    organized in rows.\n\n    Args:\n        episode (Episode): Episode object containing the interaction sequence to be rendered\n\n    Returns:\n        str: HTML string representation of the episode with formatted columns and rows\n    \"\"\"\n    chunks = []\n\n    def wrap_agent(html: str) -&gt; str:\n        return f\"&lt;div class='agent-column'&gt;{html}&lt;/div&gt;\"\n\n    def wrap_user(html: str) -&gt; str:\n        return f\"&lt;div class='user-column'&gt;{html}&lt;/div&gt;\"\n\n    def wrap_annotator(html: str) -&gt; str:\n        return f\"&lt;div class='annotator-column'&gt;{html}&lt;/div&gt;\"\n\n    def row(user: str, agent: str, annotator: str):\n        return f\"&lt;div class='episode-row'&gt;{wrap_user(user)}{wrap_agent(agent)}{wrap_annotator(annotator)}&lt;/div&gt;\"\n\n    chunks.append(row(self.user_tape_header, self.agent_tape_header, self.annotator_tape_header))\n    chunks.append(row(\"\", self.context_header, \"\"))\n    chunks.append(row(\"\", self.render_context(episode.tape), \"\"))\n    chunks.append(row(\"\", self.steps_header, \"\"))\n    for index, (user_tape, step, annotator_tapes) in enumerate(episode.group_by_step()):\n        if user_tape:\n            user_html = f\"{self.steps_header}{self.render_steps(user_tape)}\"\n        else:\n            user_html = \"\"\n        agent_html = self.render_step(step, index)\n        annotations_html = \"\".join([f\"{self.steps_header}{self.render_steps(tape)}\" for tape in annotator_tapes])\n        chunks.append(row(user_html, agent_html, annotations_html))\n    return \"\".join(chunks)\n</code></pre>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer.render_llm_call","title":"<code>render_llm_call(llm_call)</code>","text":"<p>Renders an LLM call into HTML format.</p> <p>This method generates HTML representation of an LLM call, including both the prompt and output (if available). The HTML includes expandable details sections for both the prompt and response.</p> <p>Parameters:</p> <ul> <li> <code>llm_call</code>               (<code>Union[LLMCall, None]</code>)           \u2013            <p>An LLM call object containing prompt and output information. If None, returns an empty string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>HTML string representation of the LLM call. The HTML contains:</p> </li> <li> <code>str</code>           \u2013            <ul> <li>Prompt section with:</li> <li>Summary showing token/character count and cache status</li> <li>Expandable details with prompt messages</li> </ul> </li> <li> <code>str</code>           \u2013            <ul> <li>Output section (if output exists) with:</li> <li>Summary showing token count</li> <li>Expandable details with LLM response</li> </ul> </li> </ul> <p>The rendered HTML uses collapsible details elements and basic styling for readability, with a light yellow background color.</p> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>def render_llm_call(self, llm_call: LLMCall | None) -&gt; str:\n    \"\"\"Renders an LLM call into HTML format.\n\n    This method generates HTML representation of an LLM call, including both the prompt\n    and output (if available). The HTML includes expandable details sections for both\n    the prompt and response.\n\n    Args:\n        llm_call (Union[LLMCall, None]): An LLM call object containing prompt and output information.\n            If None, returns an empty string.\n\n    Returns:\n        str: HTML string representation of the LLM call. The HTML contains:\n\n        - Prompt section with:\n            - Summary showing token/character count and cache status\n            - Expandable details with prompt messages\n        - Output section (if output exists) with:\n            - Summary showing token count\n            - Expandable details with LLM response\n\n    The rendered HTML uses collapsible details elements and basic styling for\n    readability, with a light yellow background color.\n    \"\"\"\n    if llm_call is None:\n        return \"\"\n    if llm_call.prompt.tools:\n        prompt_messages = [f\"tool_schemas: {json.dumps(llm_call.prompt.tools, indent=2)}\"]\n    else:\n        prompt_messages = []\n    for m in llm_call.prompt.messages:\n        role = f\"{m['role']} ({m['name']})\" if \"name\" in m else m[\"role\"]\n        prompt_messages.append(f\"{role}: {m['content'] if 'content' in m else m['tool_calls']}\")\n    prompt_text = \"\\n--\\n\".join(prompt_messages)\n    prompt_length_str = (\n        f\"{llm_call.prompt_length_tokens} tokens\"\n        if llm_call.prompt_length_tokens\n        else f\"{len(prompt_text)} characters\"\n    )\n    label = f\"Prompt {prompt_length_str} {', cached' if llm_call.cached else ''}\"\n    html = f\"\"\"&lt;div class='basic-prompt-box' style='background-color:#ffffba; margin: 0 4px;'&gt;\n    &lt;details&gt;\n        &lt;summary&gt;{label}&lt;/summary&gt;\n        &lt;pre style='font-size: 12px; white-space: pre-wrap;word-wrap: break-word;'&gt;{prompt_text.strip()}&lt;/pre&gt;\n    &lt;/details&gt;\n    &lt;/div&gt;\"\"\"\n    if llm_call.output:\n        html += f\"\"\"&lt;div class='basic-prompt-box' style='background-color:#ffffba; margin: 0 4px;'&gt;\n            &lt;details&gt;\n                &lt;summary&gt;LLM Output {llm_call.output_length_tokens} tokens&lt;/summary&gt;\n                &lt;pre style='font-size: 12px; white-space: pre-wrap; word-wrap: break-word;'&gt;{llm_call.output}&lt;/pre&gt;\n            &lt;/details&gt;\n            &lt;/div&gt;\"\"\"\n    return html\n</code></pre>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer.render_step","title":"<code>render_step(step, index, **kwargs)</code>","text":"<p>Renders a single step in the process.</p> <p>Parameters:</p> <ul> <li> <code>step</code>               (<code>Step</code>)           \u2013            <p>The step object containing the data to be rendered</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index of the current step</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for rendering customization</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The rendered step as a formatted string in box format</p> </li> </ul> Note <p>The step is first converted to a dictionary using model_dump() before rendering. The rendering is done using the render_as_box method.</p> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>def render_step(self, step: Step, index: int, **kwargs) -&gt; str:\n    \"\"\"\n    Renders a single step in the process.\n\n    Args:\n        step (Step): The step object containing the data to be rendered\n        index (int): The index of the current step\n        **kwargs (dict, optional): Additional keyword arguments for rendering customization\n\n    Returns:\n        str: The rendered step as a formatted string in box format\n\n    Note:\n        The step is first converted to a dictionary using model_dump() before rendering.\n        The rendering is done using the render_as_box method.\n    \"\"\"\n    step_dict = step.model_dump()\n    return self.render_as_box(step_dict)\n</code></pre>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer.render_steps","title":"<code>render_steps(tape, llm_calls={})</code>","text":"<p>Renders a sequence of steps from a tape into an HTML string representation.</p> <p>This method processes each step in the tape and generates HTML chunks based on various rendering options. It can show agent nodes, LLM calls, and individual steps based on configuration.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object containing the sequence of steps to render</p> </li> <li> <code>llm_calls</code>               (<code>dict[str, LLMCall]</code>, default:                   <code>{}</code> )           \u2013            <p>Dictionary mapping prompt IDs to LLM calls. Defaults to {}.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A concatenated HTML string containing all rendered chunks</p> </li> </ul> Notes <ul> <li>If filter_steps is set, only steps matching those types will be rendered</li> <li>Agent nodes are rendered as dividers with agent/node names if render_agent_node is True</li> <li>LLM calls are rendered for each unique prompt_id if render_llm_calls is True</li> <li>Steps from UserStep and Observation are treated as \"Environment\" agent</li> </ul> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>def render_steps(self, tape: Tape, llm_calls: dict[str, LLMCall] = {}) -&gt; str:\n    \"\"\"\n    Renders a sequence of steps from a tape into an HTML string representation.\n\n    This method processes each step in the tape and generates HTML chunks based on various rendering options.\n    It can show agent nodes, LLM calls, and individual steps based on configuration.\n\n    Args:\n        tape (Tape): The tape object containing the sequence of steps to render\n        llm_calls (dict[str, LLMCall], optional): Dictionary mapping prompt IDs to LLM calls. Defaults to {}.\n\n    Returns:\n        str: A concatenated HTML string containing all rendered chunks\n\n    Notes:\n        - If filter_steps is set, only steps matching those types will be rendered\n        - Agent nodes are rendered as dividers with agent/node names if render_agent_node is True\n        - LLM calls are rendered for each unique prompt_id if render_llm_calls is True\n        - Steps from UserStep and Observation are treated as \"Environment\" agent\n    \"\"\"\n    chunks = []\n    last_prompt_id = None\n    last_agent_node = None\n    for index, step in enumerate(tape):\n        if self.filter_steps and not isinstance(step, self.filter_steps):\n            continue\n        if self.render_agent_node:\n            if isinstance(step, UserStep) or isinstance(step, Observation):\n                agent = \"Environment\"\n                node = \"\"\n            else:\n                agent = step.metadata.agent.split(\"/\")[-1]\n                node = step.metadata.node\n            agent_node = agent + (f\".{node}\" if node else \"\")\n            if agent_node != last_agent_node:\n                chunks.append(f\"\"\"&lt;div class=\"agent_node\"&gt;&lt;hr&gt;&lt;span&gt;{agent_node}&lt;/span&gt;&lt;/div&gt;\"\"\")\n                last_agent_node = agent_node\n        if self.render_llm_calls:\n            if step.metadata.prompt_id != last_prompt_id:\n                llm_call = llm_calls.get(step.metadata.prompt_id)\n                if llm_call:\n                    chunks.append(self.render_llm_call(llm_call))\n                last_prompt_id = step.metadata.prompt_id\n        chunks.append(self.render_step(step, index))\n    return \"\".join(chunks)\n</code></pre>"},{"location":"reference/renderers/basic/#tapeagents.renderers.basic.BasicRenderer.render_tape","title":"<code>render_tape(tape, llm_calls={})</code>","text":"<p>Render a tape object into HTML representation.</p> <p>Parameters:</p> <ul> <li> <code>tape</code>               (<code>Tape</code>)           \u2013            <p>The tape object to render.</p> </li> <li> <code>llm_calls</code>               (<code>dict[str, LLMCall]</code>, default:                   <code>{}</code> )           \u2013            <p>Dictionary of LLM calls associated with the tape. Defaults to {}.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>HTML representation of the tape including metadata, context (if present), and steps.</p> </li> </ul> <p>The rendered HTML includes:</p> <ul> <li>Metadata section with tape metadata</li> <li>Context section (if tape.context exists)</li> <li>Steps section with tape execution steps</li> </ul> Source code in <code>tapeagents/renderers/basic.py</code> <pre><code>def render_tape(self, tape: Tape, llm_calls: dict[str, LLMCall] = {}) -&gt; str:\n    \"\"\"\n    Render a tape object into HTML representation.\n\n    Args:\n        tape (Tape): The tape object to render.\n        llm_calls (dict[str, LLMCall], optional): Dictionary of LLM calls associated with the tape. Defaults to {}.\n\n    Returns:\n        str: HTML representation of the tape including metadata, context (if present), and steps.\n\n    The rendered HTML includes:\n\n    - Metadata section with tape metadata\n    - Context section (if tape.context exists)\n    - Steps section with tape execution steps\n    \"\"\"\n    metadata_html = self.render_metadata(tape)\n    context_html = self.render_context(tape)\n    steps_html = self.render_steps(tape, llm_calls)\n    return (\n        f\"{self.metadata_header}{metadata_html}\"\n        + (f\"{self.context_header}{context_html}\" if tape.context is not None else \"\")\n        + f\"{self.steps_header}{steps_html}\"\n    )\n</code></pre>"},{"location":"reference/renderers/camera_ready_renderer/","title":"Camera Ready Renderer","text":"<p>Functions:</p> <ul> <li> <code>dict_to_params</code>             \u2013              <p>Transform a dictionary into a function parameters string.</p> </li> <li> <code>str_to_dict</code>             \u2013              <p>Convert a string representation of a dictionary into an actual dictionary.</p> </li> </ul>"},{"location":"reference/renderers/camera_ready_renderer/#tapeagents.renderers.camera_ready_renderer.dict_to_params","title":"<code>dict_to_params(arguments)</code>","text":"<p>Transform a dictionary into a function parameters string. Example: {'a': 1, 'b': 2} -&gt; 'a=1, b=2'</p> Source code in <code>tapeagents/renderers/camera_ready_renderer.py</code> <pre><code>def dict_to_params(arguments: str | dict) -&gt; str:\n    \"\"\"\n    Transform a dictionary into a function parameters string.\n    Example: {'a': 1, 'b': 2} -&gt; 'a=1, b=2'\n    \"\"\"\n    if isinstance(arguments, str):\n        arguments = str_to_dict(arguments)\n    return \", \".join(f\"{key}={value!r}\" for key, value in arguments.items())\n</code></pre>"},{"location":"reference/renderers/camera_ready_renderer/#tapeagents.renderers.camera_ready_renderer.str_to_dict","title":"<code>str_to_dict(s)</code>","text":"<p>Convert a string representation of a dictionary into an actual dictionary. Example: \"{'a': 1, 'b': 2}\" -&gt; {'a': 1, 'b': 2}</p> Source code in <code>tapeagents/renderers/camera_ready_renderer.py</code> <pre><code>def str_to_dict(s: str) -&gt; dict:\n    \"\"\"\n    Convert a string representation of a dictionary into an actual dictionary.\n    Example: \"{'a': 1, 'b': 2}\" -&gt; {'a': 1, 'b': 2}\n    \"\"\"\n    try:\n        return ast.literal_eval(s)\n    except (ValueError, SyntaxError):\n        raise ValueError(\"Invalid string representation of a dictionary\")\n</code></pre>"},{"location":"reference/renderers/pretty/","title":"Pretty","text":"<p>Classes:</p> <ul> <li> <code>PrettyRenderer</code>           \u2013            <p>Rendering enhancements for a handful of known steps.</p> </li> </ul>"},{"location":"reference/renderers/pretty/#tapeagents.renderers.pretty.PrettyRenderer","title":"<code>PrettyRenderer</code>","text":"<p>               Bases: <code>BasicRenderer</code></p> <p>Rendering enhancements for a handful of known steps.</p> Source code in <code>tapeagents/renderers/pretty.py</code> <pre><code>class PrettyRenderer(BasicRenderer):\n    \"\"\"Rendering enhancements for a handful of known steps.\"\"\"\n\n    def __init__(self, show_metadata=False, **kwargs):\n        self.show_metadata = show_metadata\n        super().__init__(**kwargs)\n\n    @property\n    def style(self):\n        return super().style + (\n            \"&lt;style&gt;\"\n            \".observation { background-color: #baffc9;; }\"\n            \".error_observation { background-color: #dd0000; }\"\n            \".action { background-color: #cccccc; }\"\n            \".thought { background-color: #ffffdb; }\"\n            \".call { background-color: #ffffff; }\"\n            \".respond { background-color: #ffffff; }\"\n            \".step-header { margin: 2pt 2pt 2pt 0 !important; }\"\n            \".step-text { font-size: 12px; white-space: pre-wrap; word-wrap: break-word;}\"\n            \"&lt;/style&gt;\"\n        )\n\n    def render_step(self, step: Step, index: int, **kwargs):\n        title = type(step).__name__\n        if isinstance(step, UserStep):\n            role = \"User\"\n            title = \"\"\n            class_ = \"observation\"\n        elif isinstance(step, SystemStep):\n            role = \"System\"\n            title = \"\"\n            class_ = \"observation\"\n        elif isinstance(step, AssistantStep):\n            role = \"Assistant\"\n            title = \"\"\n            class_ = \"action\"\n        elif isinstance(step, DialogContext):\n            role = \"\"\n            class_ = \"observation\"\n        elif isinstance(step, Call):\n            role = \"\"\n            title = f\"{step.metadata.agent.split('/')[-1]} calls {step.agent_name}\"\n            class_ = \"call\"\n        elif isinstance(step, Respond):\n            role = \"\"\n            parts = step.metadata.agent.split(\"/\")\n            title = f\"{parts[-1]} responds to {parts[-2]}\" if len(parts) &gt; 1 else f\"{step.metadata.agent} responds\"\n            class_ = \"respond\"\n        elif isinstance(step, Thought):\n            role = \"Thought\"\n            class_ = \"thought\"\n        elif isinstance(step, Action):\n            role = \"Action\"\n            class_ = \"action\"\n        elif isinstance(step, CodeExecutionResult):\n            role = \"Observation\"\n            class_ = \"error_observation\" if step.result.exit_code != 0 else \"observation\"\n        elif isinstance(step, Observation):\n            role = \"Observation\"\n            class_ = \"observation\"\n        else:\n            raise ValueError(f\"Unknown object type: {type(step)}\")\n\n        dump = step.model_dump()\n        if not self.show_metadata:\n            dump.pop(\"metadata\", None)\n\n        def pretty_yaml(d: dict):\n            return yaml.dump(d, sort_keys=False, indent=2) if d else \"\"\n\n        def maybe_fold(content: Any):\n            content = str(content)\n            summary = f\"{len(content)} characters ...\"\n            if len(content) &gt; 1000:\n                return f\"&lt;details&gt;&lt;summary&gt;{summary}&lt;/summary&gt;{content}&lt;/details&gt;\"\n            return content\n\n        if (content := getattr(step, \"content\", None)) is not None:\n            # TODO: also show metadata here\n            del dump[\"content\"]\n            text = pretty_yaml(dump) + (\"\\n\" + maybe_fold(content) if content else \"\")\n        elif isinstance(step, ExecuteCode):\n            del dump[\"code\"]\n\n            def format_code_block(block: CodeBlock) -&gt; str:\n                return f\"```{block.language}\\n{block.code}\\n```\"\n\n            code_blocks = \"\\n\".join([format_code_block(block) for block in step.code])\n            text = pretty_yaml(dump) + \"\\n\" + maybe_fold(code_blocks)\n        elif isinstance(step, CodeExecutionResult):\n            del dump[\"result\"][\"output\"]\n            text = pretty_yaml(dump) + \"\\n\" + maybe_fold(step.result.output)\n        else:\n            text = pretty_yaml(dump)\n\n        index_str = f\"[{index}]\"\n        header_text = title if not role else (role if not title else f\"{role}: {title}\")\n        header = f\"{index_str} {header_text}\"\n\n        return (\n            f\"&lt;div class='basic-renderer-box {class_}'&gt;\"\n            f\"&lt;h4 class='step-header'&gt;{header}&lt;/h4&gt;\"\n            f\"&lt;pre class='step-text'&gt;{text}&lt;/pre&gt;\"\n            f\"&lt;/div&gt;\"\n        )\n</code></pre>"},{"location":"reference/tools/","title":"Tools","text":"<p>Tools for the agents to use.</p> <p>Modules:</p> <ul> <li> <code>calculator</code>           \u2013            </li> <li> <code>container_executor</code>           \u2013            </li> <li> <code>document_converters</code>           \u2013            </li> <li> <code>gym_browser</code>           \u2013            </li> <li> <code>python_interpreter</code>           \u2013            </li> <li> <code>simple_browser</code>           \u2013            </li> <li> <code>stock</code>           \u2013            </li> </ul>"},{"location":"reference/tools/calculator/","title":"Calculator","text":"<p>Classes:</p> <ul> <li> <code>NumericStringParser</code>           \u2013            <p>Most of this code comes from the fourFn.py pyparsing example</p> </li> </ul>"},{"location":"reference/tools/calculator/#tapeagents.tools.calculator.NumericStringParser","title":"<code>NumericStringParser</code>","text":"<p>               Bases: <code>object</code></p> <p>Most of this code comes from the fourFn.py pyparsing example</p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>expop   :: '^'</p> </li> </ul> Source code in <code>tapeagents/tools/calculator.py</code> <pre><code>class NumericStringParser(object):\n    \"\"\"\n    Most of this code comes from the fourFn.py pyparsing example\n\n    \"\"\"\n\n    def pushFirst(self, strg, loc, toks):\n        self.exprStack.append(toks[0])\n\n    def pushUMinus(self, strg, loc, toks):\n        if toks and toks[0] == \"-\":\n            self.exprStack.append(\"unary -\")\n\n    def __init__(self):\n        \"\"\"\n        expop   :: '^'\n        multop  :: '*' | '/'\n        addop   :: '+' | '-'\n        integer :: ['+' | '-'] '0'..'9'+\n        atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')'\n        factor  :: atom [ expop factor ]*\n        term    :: factor [ multop factor ]*\n        expr    :: term [ addop term ]*\n        \"\"\"\n        point = Literal(\".\")\n        e = CaselessLiteral(\"E\")\n        fnumber = Combine(\n            Word(\"+-\" + nums, nums) + Optional(point + Optional(Word(nums))) + Optional(e + Word(\"+-\" + nums, nums))\n        )\n        ident = Word(alphas, alphas + nums + \"_$\")\n        plus = Literal(\"+\")\n        minus = Literal(\"-\")\n        mult = Literal(\"*\")\n        div = Literal(\"/\")\n        lpar = Literal(\"(\").suppress()\n        rpar = Literal(\")\").suppress()\n        addop = plus | minus\n        multop = mult | div\n        expop = Literal(\"^\")\n        pi = CaselessLiteral(\"PI\")\n        expr = Forward()\n        atom = (\n            (Optional(oneOf(\"- +\")) + (ident + lpar + expr + rpar | pi | e | fnumber).setParseAction(self.pushFirst))\n            | Optional(oneOf(\"- +\")) + Group(lpar + expr + rpar)\n        ).setParseAction(self.pushUMinus)\n        # by defining exponentiation as \"atom [ ^ factor ]...\" instead of\n        # \"atom [ ^ atom ]...\", we get right-to-left exponents, instead of left-to-right\n        # that is, 2^3^2 = 2^(3^2), not (2^3)^2.\n        factor = Forward()\n        factor &lt;&lt; atom + ZeroOrMore((expop + factor).setParseAction(self.pushFirst))\n        term = factor + ZeroOrMore((multop + factor).setParseAction(self.pushFirst))\n        expr &lt;&lt; term + ZeroOrMore((addop + term).setParseAction(self.pushFirst))\n        # addop_term = ( addop + term ).setParseAction( self.pushFirst )\n        # general_term = term + ZeroOrMore( addop_term ) | OneOrMore( addop_term)\n        # expr &lt;&lt;  general_term\n        self.bnf = expr\n        # map operator symbols to corresponding arithmetic operations\n        epsilon = 1e-12\n        self.opn = {\"+\": operator.add, \"-\": operator.sub, \"*\": operator.mul, \"/\": operator.truediv, \"^\": operator.pow}\n        self.fn = {\n            \"sin\": math.sin,\n            \"cos\": math.cos,\n            \"tan\": math.tan,\n            \"exp\": math.exp,\n            \"abs\": abs,\n            \"trunc\": lambda a: int(a),\n            \"round\": round,\n            \"sgn\": lambda a: abs(a) &gt; epsilon and cmp(a, 0) or 0,\n        }\n\n    def evaluateStack(self, s):\n        op = s.pop()\n        if op == \"unary -\":\n            return -self.evaluateStack(s)\n        if op in \"+-*/^\":\n            op2 = self.evaluateStack(s)\n            op1 = self.evaluateStack(s)\n            return self.opn[op](op1, op2)\n        elif op == \"PI\":\n            return math.pi  # 3.1415926535\n        elif op == \"E\":\n            return math.e  # 2.718281828\n        elif op in self.fn:\n            return self.fn[op](self.evaluateStack(s))\n        elif op[0].isalpha():\n            return 0\n        else:\n            return float(op)\n\n    def eval(self, num_string, parseAll=True):\n        self.exprStack = []\n        self.bnf.parseString(num_string, parseAll)\n        val = self.evaluateStack(self.exprStack[:])\n        return val\n</code></pre>"},{"location":"reference/tools/calculator/#tapeagents.tools.calculator.NumericStringParser.__init__","title":"<code>__init__()</code>","text":"<p>expop   :: '^' multop  :: '' | '/' addop   :: '+' | '-' integer :: ['+' | '-'] '0'..'9'+ atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')' factor  :: atom [ expop factor ] term    :: factor [ multop factor ] expr    :: term [ addop term ]</p> Source code in <code>tapeagents/tools/calculator.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    expop   :: '^'\n    multop  :: '*' | '/'\n    addop   :: '+' | '-'\n    integer :: ['+' | '-'] '0'..'9'+\n    atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')'\n    factor  :: atom [ expop factor ]*\n    term    :: factor [ multop factor ]*\n    expr    :: term [ addop term ]*\n    \"\"\"\n    point = Literal(\".\")\n    e = CaselessLiteral(\"E\")\n    fnumber = Combine(\n        Word(\"+-\" + nums, nums) + Optional(point + Optional(Word(nums))) + Optional(e + Word(\"+-\" + nums, nums))\n    )\n    ident = Word(alphas, alphas + nums + \"_$\")\n    plus = Literal(\"+\")\n    minus = Literal(\"-\")\n    mult = Literal(\"*\")\n    div = Literal(\"/\")\n    lpar = Literal(\"(\").suppress()\n    rpar = Literal(\")\").suppress()\n    addop = plus | minus\n    multop = mult | div\n    expop = Literal(\"^\")\n    pi = CaselessLiteral(\"PI\")\n    expr = Forward()\n    atom = (\n        (Optional(oneOf(\"- +\")) + (ident + lpar + expr + rpar | pi | e | fnumber).setParseAction(self.pushFirst))\n        | Optional(oneOf(\"- +\")) + Group(lpar + expr + rpar)\n    ).setParseAction(self.pushUMinus)\n    # by defining exponentiation as \"atom [ ^ factor ]...\" instead of\n    # \"atom [ ^ atom ]...\", we get right-to-left exponents, instead of left-to-right\n    # that is, 2^3^2 = 2^(3^2), not (2^3)^2.\n    factor = Forward()\n    factor &lt;&lt; atom + ZeroOrMore((expop + factor).setParseAction(self.pushFirst))\n    term = factor + ZeroOrMore((multop + factor).setParseAction(self.pushFirst))\n    expr &lt;&lt; term + ZeroOrMore((addop + term).setParseAction(self.pushFirst))\n    # addop_term = ( addop + term ).setParseAction( self.pushFirst )\n    # general_term = term + ZeroOrMore( addop_term ) | OneOrMore( addop_term)\n    # expr &lt;&lt;  general_term\n    self.bnf = expr\n    # map operator symbols to corresponding arithmetic operations\n    epsilon = 1e-12\n    self.opn = {\"+\": operator.add, \"-\": operator.sub, \"*\": operator.mul, \"/\": operator.truediv, \"^\": operator.pow}\n    self.fn = {\n        \"sin\": math.sin,\n        \"cos\": math.cos,\n        \"tan\": math.tan,\n        \"exp\": math.exp,\n        \"abs\": abs,\n        \"trunc\": lambda a: int(a),\n        \"round\": round,\n        \"sgn\": lambda a: abs(a) &gt; epsilon and cmp(a, 0) or 0,\n    }\n</code></pre>"},{"location":"reference/tools/container_executor/","title":"Container Executor","text":"<p>Classes:</p> <ul> <li> <code>ContainerExecutor</code>           \u2013            </li> </ul>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.CodeBlock","title":"<code>CodeBlock</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class that represents a code block.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>class CodeBlock(BaseModel):\n    \"\"\"A class that represents a code block.\"\"\"\n\n    code: str = Field(description=\"The code to execute.\")\n    language: str = Field(description=\"The language of the code.\")\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.CodeResult","title":"<code>CodeResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class that represents the result of a code execution.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>class CodeResult(BaseModel):\n    \"\"\"A class that represents the result of a code execution.\"\"\"\n\n    exit_code: int = Field(description=\"The exit code of the code execution.\")\n    output: str = Field(description=\"The output of the code execution.\")\n    output_files: list[str] = Field(default=None, description=\"The output files of the code execution.\")\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.CommandLineCodeResult","title":"<code>CommandLineCodeResult</code>","text":"<p>               Bases: <code>CodeResult</code></p> <p>(Experimental) A code result class for command line code executor.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>class CommandLineCodeResult(CodeResult):\n    \"\"\"(Experimental) A code result class for command line code executor.\"\"\"\n\n    code_files: list[str] = Field(\n        default=None,\n        description=\"The file that the executed code block was saved to.\",\n    )\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor","title":"<code>ContainerExecutor</code>","text":"<p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>(Experimental) A code executor class that executes code through</p> </li> <li> <code>execute_code_blocks</code>             \u2013              <p>(Experimental) Execute the code blocks and return the result.</p> </li> <li> <code>restart</code>             \u2013              <p>(Experimental) Restart the code executor.</p> </li> <li> <code>stop</code>             \u2013              <p>(Experimental) Stop the code executor.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>bind_dir</code>               (<code>Path</code>)           \u2013            <p>(Experimental) The binding directory for the code execution container.</p> </li> <li> <code>timeout</code>               (<code>int</code>)           \u2013            <p>(Experimental) The timeout for code execution.</p> </li> <li> <code>work_dir</code>               (<code>Path</code>)           \u2013            <p>(Experimental) The working directory for the code execution.</p> </li> </ul> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>class ContainerExecutor:\n    DEFAULT_EXECUTION_POLICY: ClassVar[Dict[str, bool]] = {\n        \"bash\": True,\n        \"shell\": True,\n        \"sh\": True,\n        \"p-sh\": True,\n        \"powershell\": True,\n        \"ps1\": True,\n        \"python\": True,\n        \"javascript\": False,\n        \"html\": False,\n        \"css\": False,\n    }\n    LANGUAGE_ALIASES: ClassVar[Dict[str, str]] = {\"py\": \"python\", \"js\": \"javascript\"}\n\n    def __init__(\n        self,\n        image: str = \"python:3-slim\",\n        container_name: Optional[str] = None,\n        timeout: int = 60,\n        work_dir: Union[Path, str] = Path(\".\"),\n        bind_dir: Optional[Union[Path, str]] = None,\n        auto_remove: bool = True,\n        stop_container: bool = True,\n        execution_policies: Optional[Dict[str, bool]] = None,\n    ):\n        \"\"\"(Experimental) A code executor class that executes code through\n        a command line environment in a Docker container.\n\n        The executor first saves each code block in a file in the working\n        directory, and then executes the code file in the container.\n        The executor executes the code blocks in the order they are received.\n        Currently, the executor only supports Python and shell scripts.\n        For Python code, use the language \"python\" for the code block.\n        For shell scripts, use the language \"bash\", \"shell\", or \"sh\" for the code\n        block.\n\n        Args:\n            image (_type_, optional): Docker image to use for code execution.\n                Defaults to \"python:3-slim\".\n            container_name (Optional[str], optional): Name of the Docker container\n                which is created. If None, will autogenerate a name. Defaults to None.\n            timeout (int, optional): The timeout for code execution. Defaults to 60.\n            work_dir (Union[Path, str], optional): The working directory for the code\n                execution. Defaults to Path(\".\").\n            bind_dir (Union[Path, str], optional): The directory that will be bound\n                to the code executor container. Useful for cases where you want to spawn\n                the container from within a container. Defaults to work_dir.\n            auto_remove (bool, optional): If true, will automatically remove the Docker\n                container when it is stopped. Defaults to True.\n            stop_container (bool, optional): If true, will automatically stop the\n                container when stop is called, when the context manager exits or when\n                the Python process exits with atext. Defaults to True.\n\n        Raises:\n            ValueError: On argument error, or if the container fails to start.\n        \"\"\"\n        if timeout &lt; 1:\n            raise ValueError(\"Timeout must be greater than or equal to 1.\")\n\n        if isinstance(work_dir, str):\n            work_dir = Path(work_dir)\n        work_dir.mkdir(parents=True, exist_ok=True)\n\n        if bind_dir is None:\n            bind_dir = work_dir\n        elif isinstance(bind_dir, str):\n            bind_dir = Path(bind_dir)\n\n        import podman as docker\n\n        client = docker.from_env()\n        # Check if the image exists\n        try:\n            client.images.get(image)\n        except docker.errors.ImageNotFound:\n            logging.info(f\"Pulling image {image}...\")\n            # Let the docker exception escape if this fails.\n            client.images.pull(image)\n\n        if container_name is None:\n            container_name = f\"autogen-code-exec-{uuid.uuid4()}\"\n\n        # Start a container from the image, read to exec commands later\n        host_path = str(bind_dir.resolve())\n        mounts = [\n            {\n                \"type\": \"bind\",\n                \"source\": host_path,\n                \"target\": \"/workspace\",\n            }\n        ]\n        self._container = client.containers.create(\n            image,\n            name=container_name,\n            # Note this change: was needed for Podman\n            # entrypoint=\"/bin/sh\",\n            entrypoint=[\"/bin/sh\"],\n            tty=True,\n            auto_remove=auto_remove,\n            # volumes={str(bind_dir.resolve()): {\"bind\": \"/workspace\", \"mode\": \"rw\"}},\n            mounts=mounts,\n            working_dir=\"/workspace\",\n        )\n        self._container.start()\n\n        _wait_for_ready(self._container)\n\n        def cleanup() -&gt; None:\n            try:\n                container = client.containers.get(container_name)\n                container.stop()\n            except docker.errors.NotFound:\n                pass\n            atexit.unregister(cleanup)\n\n        if stop_container:\n            atexit.register(cleanup)\n\n        self._cleanup = cleanup\n\n        # Check if the container is running\n        if self._container.status != \"running\":\n            raise ValueError(f\"Failed to start container from image {image}. Logs: {self._container.logs()}\")\n\n        self._timeout = timeout\n        self._work_dir: Path = work_dir\n        self._bind_dir: Path = bind_dir\n        self.execution_policies = self.DEFAULT_EXECUTION_POLICY.copy()\n        if execution_policies is not None:\n            self.execution_policies.update(execution_policies)\n\n    @property\n    def timeout(self) -&gt; int:\n        \"\"\"(Experimental) The timeout for code execution.\"\"\"\n        return self._timeout\n\n    @property\n    def work_dir(self) -&gt; Path:\n        \"\"\"(Experimental) The working directory for the code execution.\"\"\"\n        return self._work_dir\n\n    @property\n    def bind_dir(self) -&gt; Path:\n        \"\"\"(Experimental) The binding directory for the code execution container.\"\"\"\n        return self._bind_dir\n\n    def execute_code_blocks(self, code_blocks: List[CodeBlock]) -&gt; CommandLineCodeResult:\n        \"\"\"(Experimental) Execute the code blocks and return the result.\n\n        Args:\n            code_blocks (List[CodeBlock]): The code blocks to execute.\n\n        Returns:\n            CommandlineCodeResult: The result of the code execution.\"\"\"\n\n        if len(code_blocks) == 0:\n            raise ValueError(\"No code blocks to execute.\")\n\n        outputs = []\n        output_files = []\n        files: list[Path] = []\n        last_exit_code = 0\n        for code_block in code_blocks:\n            lang = self.LANGUAGE_ALIASES.get(code_block.language.lower(), code_block.language.lower())\n            if lang not in self.DEFAULT_EXECUTION_POLICY:\n                outputs.append(f\"Unsupported language {lang}\\n\")\n                last_exit_code = 1\n                break\n\n            execute_code = self.execution_policies.get(lang, False)\n            code = silence_pip(code_block.code, lang)\n\n            # Check if there is a filename comment\n            try:\n                filename = _get_file_name_from_content(code, self._work_dir)\n            except ValueError:\n                outputs.append(\"Filename is not in the workspace\")\n                last_exit_code = 1\n                break\n\n            if not filename:\n                filename = f\"tmp_code_{md5(code.encode()).hexdigest()}.{lang}\"\n\n            code_path = self._work_dir / filename\n            with code_path.open(\"w\", encoding=\"utf-8\") as fout:\n                fout.write(code)\n            files.append(code_path)\n\n            if not execute_code:\n                outputs.append(f\"Code saved to {str(code_path)}\\n\")\n                continue\n\n            command = [\"timeout\", str(self._timeout), _cmd(lang), filename]\n            # result = self._container.exec_run(command)\n            # exit_code = result.exit_code\n            # output = result.output.decode(\"utf-8\")\n            exit_code, output = self._container.exec_run(command, tty=True)\n            logger.info(f\"Command: {command}, Exit code: {exit_code}\\n Output: {output}\")\n            assert isinstance(output, bytes)\n            output = output.decode(\"utf-8\")\n            if exit_code == 124:\n                output += \"\\n\" + \"Timeout\"\n            outputs.append(output)\n            if file_output := _get_file_name_from_output(output, self._work_dir):\n                output_files.append(file_output)\n\n            last_exit_code = exit_code\n            if exit_code != 0:\n                break\n\n        return CommandLineCodeResult(\n            exit_code=last_exit_code,\n            output=\"\".join(outputs),\n            output_files=output_files,\n            code_files=[str(file) for file in files],\n        )\n\n    def restart(self) -&gt; None:\n        \"\"\"(Experimental) Restart the code executor.\"\"\"\n        self._container.restart()\n        if self._container.status != \"running\":\n            raise ValueError(f\"Failed to restart container. Logs: {self._container.logs()}\")\n\n    def stop(self) -&gt; None:\n        \"\"\"(Experimental) Stop the code executor.\"\"\"\n        self._cleanup()\n\n    def __enter__(self) -&gt; Self:\n        return self\n\n    def __exit__(\n        self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]\n    ) -&gt; None:\n        self.stop()\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.bind_dir","title":"<code>bind_dir: Path</code>  <code>property</code>","text":"<p>(Experimental) The binding directory for the code execution container.</p>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.timeout","title":"<code>timeout: int</code>  <code>property</code>","text":"<p>(Experimental) The timeout for code execution.</p>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.work_dir","title":"<code>work_dir: Path</code>  <code>property</code>","text":"<p>(Experimental) The working directory for the code execution.</p>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.__init__","title":"<code>__init__(image='python:3-slim', container_name=None, timeout=60, work_dir=Path('.'), bind_dir=None, auto_remove=True, stop_container=True, execution_policies=None)</code>","text":"<p>(Experimental) A code executor class that executes code through a command line environment in a Docker container.</p> <p>The executor first saves each code block in a file in the working directory, and then executes the code file in the container. The executor executes the code blocks in the order they are received. Currently, the executor only supports Python and shell scripts. For Python code, use the language \"python\" for the code block. For shell scripts, use the language \"bash\", \"shell\", or \"sh\" for the code block.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>_type_</code>, default:                   <code>'python:3-slim'</code> )           \u2013            <p>Docker image to use for code execution. Defaults to \"python:3-slim\".</p> </li> <li> <code>container_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the Docker container which is created. If None, will autogenerate a name. Defaults to None.</p> </li> <li> <code>timeout</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>The timeout for code execution. Defaults to 60.</p> </li> <li> <code>work_dir</code>               (<code>Union[Path, str]</code>, default:                   <code>Path('.')</code> )           \u2013            <p>The working directory for the code execution. Defaults to Path(\".\").</p> </li> <li> <code>bind_dir</code>               (<code>Union[Path, str]</code>, default:                   <code>None</code> )           \u2013            <p>The directory that will be bound to the code executor container. Useful for cases where you want to spawn the container from within a container. Defaults to work_dir.</p> </li> <li> <code>auto_remove</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, will automatically remove the Docker container when it is stopped. Defaults to True.</p> </li> <li> <code>stop_container</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, will automatically stop the container when stop is called, when the context manager exits or when the Python process exits with atext. Defaults to True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>On argument error, or if the container fails to start.</p> </li> </ul> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def __init__(\n    self,\n    image: str = \"python:3-slim\",\n    container_name: Optional[str] = None,\n    timeout: int = 60,\n    work_dir: Union[Path, str] = Path(\".\"),\n    bind_dir: Optional[Union[Path, str]] = None,\n    auto_remove: bool = True,\n    stop_container: bool = True,\n    execution_policies: Optional[Dict[str, bool]] = None,\n):\n    \"\"\"(Experimental) A code executor class that executes code through\n    a command line environment in a Docker container.\n\n    The executor first saves each code block in a file in the working\n    directory, and then executes the code file in the container.\n    The executor executes the code blocks in the order they are received.\n    Currently, the executor only supports Python and shell scripts.\n    For Python code, use the language \"python\" for the code block.\n    For shell scripts, use the language \"bash\", \"shell\", or \"sh\" for the code\n    block.\n\n    Args:\n        image (_type_, optional): Docker image to use for code execution.\n            Defaults to \"python:3-slim\".\n        container_name (Optional[str], optional): Name of the Docker container\n            which is created. If None, will autogenerate a name. Defaults to None.\n        timeout (int, optional): The timeout for code execution. Defaults to 60.\n        work_dir (Union[Path, str], optional): The working directory for the code\n            execution. Defaults to Path(\".\").\n        bind_dir (Union[Path, str], optional): The directory that will be bound\n            to the code executor container. Useful for cases where you want to spawn\n            the container from within a container. Defaults to work_dir.\n        auto_remove (bool, optional): If true, will automatically remove the Docker\n            container when it is stopped. Defaults to True.\n        stop_container (bool, optional): If true, will automatically stop the\n            container when stop is called, when the context manager exits or when\n            the Python process exits with atext. Defaults to True.\n\n    Raises:\n        ValueError: On argument error, or if the container fails to start.\n    \"\"\"\n    if timeout &lt; 1:\n        raise ValueError(\"Timeout must be greater than or equal to 1.\")\n\n    if isinstance(work_dir, str):\n        work_dir = Path(work_dir)\n    work_dir.mkdir(parents=True, exist_ok=True)\n\n    if bind_dir is None:\n        bind_dir = work_dir\n    elif isinstance(bind_dir, str):\n        bind_dir = Path(bind_dir)\n\n    import podman as docker\n\n    client = docker.from_env()\n    # Check if the image exists\n    try:\n        client.images.get(image)\n    except docker.errors.ImageNotFound:\n        logging.info(f\"Pulling image {image}...\")\n        # Let the docker exception escape if this fails.\n        client.images.pull(image)\n\n    if container_name is None:\n        container_name = f\"autogen-code-exec-{uuid.uuid4()}\"\n\n    # Start a container from the image, read to exec commands later\n    host_path = str(bind_dir.resolve())\n    mounts = [\n        {\n            \"type\": \"bind\",\n            \"source\": host_path,\n            \"target\": \"/workspace\",\n        }\n    ]\n    self._container = client.containers.create(\n        image,\n        name=container_name,\n        # Note this change: was needed for Podman\n        # entrypoint=\"/bin/sh\",\n        entrypoint=[\"/bin/sh\"],\n        tty=True,\n        auto_remove=auto_remove,\n        # volumes={str(bind_dir.resolve()): {\"bind\": \"/workspace\", \"mode\": \"rw\"}},\n        mounts=mounts,\n        working_dir=\"/workspace\",\n    )\n    self._container.start()\n\n    _wait_for_ready(self._container)\n\n    def cleanup() -&gt; None:\n        try:\n            container = client.containers.get(container_name)\n            container.stop()\n        except docker.errors.NotFound:\n            pass\n        atexit.unregister(cleanup)\n\n    if stop_container:\n        atexit.register(cleanup)\n\n    self._cleanup = cleanup\n\n    # Check if the container is running\n    if self._container.status != \"running\":\n        raise ValueError(f\"Failed to start container from image {image}. Logs: {self._container.logs()}\")\n\n    self._timeout = timeout\n    self._work_dir: Path = work_dir\n    self._bind_dir: Path = bind_dir\n    self.execution_policies = self.DEFAULT_EXECUTION_POLICY.copy()\n    if execution_policies is not None:\n        self.execution_policies.update(execution_policies)\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.execute_code_blocks","title":"<code>execute_code_blocks(code_blocks)</code>","text":"<p>(Experimental) Execute the code blocks and return the result.</p> <p>Parameters:</p> <ul> <li> <code>code_blocks</code>               (<code>List[CodeBlock]</code>)           \u2013            <p>The code blocks to execute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CommandlineCodeResult</code> (              <code>CommandLineCodeResult</code> )          \u2013            <p>The result of the code execution.</p> </li> </ul> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def execute_code_blocks(self, code_blocks: List[CodeBlock]) -&gt; CommandLineCodeResult:\n    \"\"\"(Experimental) Execute the code blocks and return the result.\n\n    Args:\n        code_blocks (List[CodeBlock]): The code blocks to execute.\n\n    Returns:\n        CommandlineCodeResult: The result of the code execution.\"\"\"\n\n    if len(code_blocks) == 0:\n        raise ValueError(\"No code blocks to execute.\")\n\n    outputs = []\n    output_files = []\n    files: list[Path] = []\n    last_exit_code = 0\n    for code_block in code_blocks:\n        lang = self.LANGUAGE_ALIASES.get(code_block.language.lower(), code_block.language.lower())\n        if lang not in self.DEFAULT_EXECUTION_POLICY:\n            outputs.append(f\"Unsupported language {lang}\\n\")\n            last_exit_code = 1\n            break\n\n        execute_code = self.execution_policies.get(lang, False)\n        code = silence_pip(code_block.code, lang)\n\n        # Check if there is a filename comment\n        try:\n            filename = _get_file_name_from_content(code, self._work_dir)\n        except ValueError:\n            outputs.append(\"Filename is not in the workspace\")\n            last_exit_code = 1\n            break\n\n        if not filename:\n            filename = f\"tmp_code_{md5(code.encode()).hexdigest()}.{lang}\"\n\n        code_path = self._work_dir / filename\n        with code_path.open(\"w\", encoding=\"utf-8\") as fout:\n            fout.write(code)\n        files.append(code_path)\n\n        if not execute_code:\n            outputs.append(f\"Code saved to {str(code_path)}\\n\")\n            continue\n\n        command = [\"timeout\", str(self._timeout), _cmd(lang), filename]\n        # result = self._container.exec_run(command)\n        # exit_code = result.exit_code\n        # output = result.output.decode(\"utf-8\")\n        exit_code, output = self._container.exec_run(command, tty=True)\n        logger.info(f\"Command: {command}, Exit code: {exit_code}\\n Output: {output}\")\n        assert isinstance(output, bytes)\n        output = output.decode(\"utf-8\")\n        if exit_code == 124:\n            output += \"\\n\" + \"Timeout\"\n        outputs.append(output)\n        if file_output := _get_file_name_from_output(output, self._work_dir):\n            output_files.append(file_output)\n\n        last_exit_code = exit_code\n        if exit_code != 0:\n            break\n\n    return CommandLineCodeResult(\n        exit_code=last_exit_code,\n        output=\"\".join(outputs),\n        output_files=output_files,\n        code_files=[str(file) for file in files],\n    )\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.restart","title":"<code>restart()</code>","text":"<p>(Experimental) Restart the code executor.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def restart(self) -&gt; None:\n    \"\"\"(Experimental) Restart the code executor.\"\"\"\n    self._container.restart()\n    if self._container.status != \"running\":\n        raise ValueError(f\"Failed to restart container. Logs: {self._container.logs()}\")\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.ContainerExecutor.stop","title":"<code>stop()</code>","text":"<p>(Experimental) Stop the code executor.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"(Experimental) Stop the code executor.\"\"\"\n    self._cleanup()\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.extract_code_blocks","title":"<code>extract_code_blocks(message)</code>","text":"<p>(Experimental) Extract code blocks from a message. If no code blocks are found, return an empty list.</p> <p>Parameters:</p> <ul> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>The message to extract code blocks from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[CodeBlock]</code>           \u2013            <p>List[CodeBlock]: The extracted code blocks or an empty list.</p> </li> </ul> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def extract_code_blocks(message: str) -&gt; List[CodeBlock]:\n    \"\"\"(Experimental) Extract code blocks from a message. If no code blocks are found,\n    return an empty list.\n\n    Args:\n        message (str): The message to extract code blocks from.\n\n    Returns:\n        List[CodeBlock]: The extracted code blocks or an empty list.\n    \"\"\"\n\n    text = message\n    match = re.findall(CODE_BLOCK_PATTERN, text, flags=re.DOTALL)\n    if not match:\n        return []\n    code_blocks = []\n    for lang, code in match:\n        if lang == \"\":\n            lang = infer_lang(code)\n        if lang == UNKNOWN:\n            lang = \"\"\n        code_blocks.append(CodeBlock(code=code, language=lang))\n    return code_blocks\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.infer_lang","title":"<code>infer_lang(code)</code>","text":"<p>infer the language for the code. TODO: make it robust.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def infer_lang(code: str) -&gt; str:\n    \"\"\"infer the language for the code.\n    TODO: make it robust.\n    \"\"\"\n    if code.startswith(\"python \") or code.startswith(\"pip\") or code.startswith(\"python3 \"):\n        return \"sh\"\n\n    # check if code is a valid python code\n    try:\n        compile(code, \"test\", \"exec\")\n        return \"python\"\n    except SyntaxError:\n        # not a valid python code\n        return UNKNOWN\n</code></pre>"},{"location":"reference/tools/container_executor/#tapeagents.tools.container_executor.silence_pip","title":"<code>silence_pip(code, lang)</code>","text":"<p>Apply -qqq flag to pip install commands.</p> Source code in <code>tapeagents/tools/container_executor.py</code> <pre><code>def silence_pip(code: str, lang: str) -&gt; str:\n    \"\"\"Apply -qqq flag to pip install commands.\"\"\"\n    if lang == \"python\":\n        regex = r\"^! ?pip install\"\n    elif lang in [\"bash\", \"shell\", \"sh\", \"pwsh\", \"powershell\", \"ps1\"]:\n        regex = r\"^pip install\"\n    else:\n        return code\n\n    # Find lines that start with pip install and make sure \"-qqq\" flag is added.\n    lines = code.split(\"\\n\")\n    for i, line in enumerate(lines):\n        # use regex to find lines that start with pip install.\n        match = re.search(regex, line)\n        if match is not None:\n            if \"-qqq\" not in line:\n                lines[i] = line.replace(match.group(0), match.group(0) + \" -qqq\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tools/document_converters/","title":"Document Converters","text":"<p>Classes:</p> <ul> <li> <code>DocumentConverterResult</code>           \u2013            <p>The result of converting a document to text.</p> </li> <li> <code>FileConverter</code>           \u2013            <p>(In preview) An extremely simple text-based document reader, suitable for LLM use.</p> </li> <li> <code>HtmlConverter</code>           \u2013            <p>Anything with content type text/html</p> </li> <li> <code>PlainTextConverter</code>           \u2013            <p>Anything with content type text/plain</p> </li> <li> <code>WikipediaConverter</code>           \u2013            <p>Handle Wikipedia pages separately, focusing only on the main document content.</p> </li> <li> <code>YouTubeConverter</code>           \u2013            <p>Handle YouTube specially, focusing on the video title, description, and transcript.</p> </li> </ul>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.DocumentConverterResult","title":"<code>DocumentConverterResult</code>","text":"<p>The result of converting a document to text.</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class DocumentConverterResult:\n    \"\"\"The result of converting a document to text.\"\"\"\n\n    def __init__(self, title: Union[str, None] = None, text_content: str = \"\"):\n        self.title = title\n        self.text_content = text_content\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.FileConverter","title":"<code>FileConverter</code>","text":"<p>(In preview) An extremely simple text-based document reader, suitable for LLM use. This reader will convert common file-types or webpages to Markdown.</p> <p>Methods:</p> <ul> <li> <code>convert</code>             \u2013              <p>Args:</p> </li> <li> <code>register_page_converter</code>             \u2013              <p>Register a page text converter.</p> </li> </ul> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class FileConverter:\n    \"\"\"(In preview) An extremely simple text-based document reader, suitable for LLM use.\n    This reader will convert common file-types or webpages to Markdown.\"\"\"\n\n    def __init__(\n        self,\n        requests_session: Optional[requests.Session] = None,\n        mlm_client: Optional[Any] = None,\n    ):\n        if requests_session is None:\n            self._requests_session = requests.Session()\n        else:\n            self._requests_session = requests_session\n\n        self._mlm_client = mlm_client\n\n        self._page_converters: List[DocumentConverter] = []\n\n        # Register converters for successful browsing operations\n        # Later registrations are tried first / take higher priority than earlier registrations\n        # To this end, the most specific converters should appear below the most generic converters\n        self.register_page_converter(PlainTextConverter())\n        self.register_page_converter(HtmlConverter())\n        self.register_page_converter(WikipediaConverter())\n        self.register_page_converter(YouTubeConverter())\n        self.register_page_converter(DocxConverter())\n        self.register_page_converter(XlsxConverter())\n        self.register_page_converter(PptxConverter())\n        self.register_page_converter(WavConverter())\n        self.register_page_converter(Mp3Converter())\n        self.register_page_converter(ImageConverter())\n\n        if IS_PDF_CAPABLE:\n            self.register_page_converter(PdfConverter())\n\n    def convert(self, source, **kwargs):\n        \"\"\"\n        Args:\n            source (str): can be a string representing a path or url, or a requests.response object\n            **kwargs (dict, optional): additional options to pass to the converters\n        \"\"\"\n\n        # Local path or url\n        if isinstance(source, str):\n            if source.startswith(\"http://\") or source.startswith(\"https://\") or source.startswith(\"file://\"):\n                return self.convert_url(source, **kwargs)\n            else:\n                return self.convert_local(source, **kwargs)\n        # Request response\n        elif isinstance(source, requests.Response):\n            return self.convert_response(source, **kwargs)\n\n    def convert_local(self, path, **kwargs):\n        # Prepare a list of extensions to try (in order of priority)\n        ext = kwargs.get(\"file_extension\")\n        extensions = [ext] if ext is not None else []\n\n        # Get extension alternatives from the path and puremagic\n        base, ext = os.path.splitext(path)\n        self._append_ext(extensions, ext)\n        self._append_ext(extensions, self._guess_ext_magic(path))\n\n        # Convert\n        return self._convert(path, extensions, **kwargs)\n\n    def convert_url(self, url, **kwargs):\n        # Send a HTTP request to the URL\n        response = self._requests_session.get(url, stream=True)\n        response.raise_for_status()\n        return self.convert_response(response, **kwargs)\n\n    def convert_response(self, response, **kwargs):\n        # Prepare a list of extensions to try (in order of priority)\n        ext = kwargs.get(\"file_extension\")\n        extensions = [ext] if ext is not None else []\n\n        # Guess from the mimetype\n        content_type = response.headers.get(\"content-type\", \"\").split(\";\")[0]\n        self._append_ext(extensions, mimetypes.guess_extension(content_type))\n\n        # Read the content disposition if there is one\n        content_disposition = response.headers.get(\"content-disposition\", \"\")\n        m = re.search(r\"filename=([^;]+)\", content_disposition)\n        if m:\n            base, ext = os.path.splitext(m.group(1).strip(\"\\\"'\"))\n            self._append_ext(extensions, ext)\n\n        # Read from the extension from the path\n        base, ext = os.path.splitext(urlparse(response.url).path)\n        self._append_ext(extensions, ext)\n\n        # Save the file locally to a temporary file. It will be deleted before this method exits\n        handle, temp_path = tempfile.mkstemp()\n        fh = os.fdopen(handle, \"wb\")\n        result = None\n        try:\n            # Download the file\n            for chunk in response.iter_content(chunk_size=512):\n                fh.write(chunk)\n            fh.close()\n\n            # Use puremagic to check for more extension options\n            self._append_ext(extensions, self._guess_ext_magic(temp_path))\n\n            # Convert\n            result = self._convert(temp_path, extensions, url=response.url, **kwargs)\n\n        # Clean up\n        finally:\n            try:\n                fh.close()\n            except Exception:\n                pass\n            os.unlink(temp_path)\n\n        return result\n\n    def _convert(self, local_path, extensions, **kwargs):\n        error_trace = \"\"\n        for ext in extensions:\n            for converter in self._page_converters:\n                _kwargs = copy.deepcopy(kwargs)\n                _kwargs.update({\"file_extension\": ext})\n\n                # Copy any additional global options\n                if \"mlm_client\" not in _kwargs and self._mlm_client is not None:\n                    _kwargs[\"mlm_client\"] = self._mlm_client\n\n                # If we hit an error log it and keep trying\n                try:\n                    res = converter.convert(local_path, **_kwargs)\n                except Exception:\n                    error_trace = (\"\\n\\n\" + traceback.format_exc()).strip()\n\n                if res is not None:\n                    # Normalize the content\n                    res.text_content = \"\\n\".join([line.rstrip() for line in re.split(r\"\\r?\\n\", res.text_content)])\n                    res.text_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", res.text_content)\n\n                    # Todo\n                    return res\n\n        # If we got this far without success, report any exceptions\n        if len(error_trace) &gt; 0:\n            raise FileConversionException(\n                f\"Could not convert '{local_path}' to Markdown. File type was recognized as {extensions}. While converting the file, the following error was encountered:\\n\\n{error_trace}\"\n            )\n\n        # Nothing can handle it!\n        raise UnsupportedFormatException(\n            f\"Could not convert '{local_path}' to Markdown. The formats {extensions} are not supported.\"\n        )\n\n    def _append_ext(self, extensions, ext):\n        \"\"\"Append a unique non-None, non-empty extension to a list of extensions.\"\"\"\n        if ext is None:\n            return\n        ext = ext.strip()\n        if ext == \"\":\n            return\n        if ext not in extensions:\n            extensions.append(ext)\n\n    def _guess_ext_magic(self, path):\n        \"\"\"Use puremagic (a Python implementation of libmagic) to guess a file's extension based on the first few bytes.\"\"\"\n        # Use puremagic to guess\n        try:\n            guesses = puremagic.magic_file(path)\n            if len(guesses) &gt; 0:\n                ext = guesses[0].extension.strip()\n                if len(ext) &gt; 0:\n                    return ext\n        except FileNotFoundError:\n            pass\n        except IsADirectoryError:\n            pass\n        except PermissionError:\n            pass\n        return None\n\n    def register_page_converter(self, converter: DocumentConverter) -&gt; None:\n        \"\"\"Register a page text converter.\"\"\"\n        self._page_converters.insert(0, converter)\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.FileConverter.convert","title":"<code>convert(source, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>source</code>               (<code>str</code>)           \u2013            <p>can be a string representing a path or url, or a requests.response object</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>additional options to pass to the converters</p> </li> </ul> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>def convert(self, source, **kwargs):\n    \"\"\"\n    Args:\n        source (str): can be a string representing a path or url, or a requests.response object\n        **kwargs (dict, optional): additional options to pass to the converters\n    \"\"\"\n\n    # Local path or url\n    if isinstance(source, str):\n        if source.startswith(\"http://\") or source.startswith(\"https://\") or source.startswith(\"file://\"):\n            return self.convert_url(source, **kwargs)\n        else:\n            return self.convert_local(source, **kwargs)\n    # Request response\n    elif isinstance(source, requests.Response):\n        return self.convert_response(source, **kwargs)\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.FileConverter.register_page_converter","title":"<code>register_page_converter(converter)</code>","text":"<p>Register a page text converter.</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>def register_page_converter(self, converter: DocumentConverter) -&gt; None:\n    \"\"\"Register a page text converter.\"\"\"\n    self._page_converters.insert(0, converter)\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.HtmlConverter","title":"<code>HtmlConverter</code>","text":"<p>               Bases: <code>DocumentConverter</code></p> <p>Anything with content type text/html</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class HtmlConverter(DocumentConverter):\n    \"\"\"Anything with content type text/html\"\"\"\n\n    def convert(self, local_path, **kwargs) -&gt; Union[None, DocumentConverterResult]:\n        # Bail if not html\n        extension = kwargs.get(\"file_extension\", \"\")\n        if extension.lower() not in [\".html\", \".htm\"]:\n            return None\n\n        result = None\n        readability = kwargs.get(\"readability\", False)\n        strip_links = kwargs.get(\"strip_links\", False)\n        strip_images = kwargs.get(\"strip_images\", False)\n        with open(local_path, \"rt\") as fh:\n            result = self._convert(fh.read(), readability, strip_links, strip_images)\n\n        return result\n\n    def _convert(\n        self, html_content, readability: bool = False, strip_links: bool = False, strip_images: bool = False\n    ) -&gt; Union[None, DocumentConverterResult]:\n        \"\"\"Helper function that converts and HTML string.\"\"\"\n\n        # Parse the string\n        soup = BeautifulSoup(html_content, \"html.parser\")\n\n        # Remove javascript and style blocks\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        # Print only the main content\n        body_elm = soup.find(\"body\")\n        strip = []\n        if strip_links:\n            strip.append(\"a\")\n        if strip_images:\n            strip.append(\"img\")\n        webpage_text = \"\"\n        if body_elm:\n            webpage_text = markdownify.MarkdownConverter(strip=strip).convert_soup(body_elm)\n        else:\n            webpage_text = markdownify.MarkdownConverter(strip=strip).convert_soup(soup)\n        if readability and len(webpage_text) &gt; 4000:\n            doc = Document(soup.prettify())\n            clean_html = doc.summary()\n            webpage_text = markdownify.markdownify(clean_html, strip=strip)\n\n        return DocumentConverterResult(\n            title=None if soup.title is None else soup.title.string, text_content=webpage_text\n        )\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.PlainTextConverter","title":"<code>PlainTextConverter</code>","text":"<p>               Bases: <code>DocumentConverter</code></p> <p>Anything with content type text/plain</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class PlainTextConverter(DocumentConverter):\n    \"\"\"Anything with content type text/plain\"\"\"\n\n    def convert(self, local_path, **kwargs) -&gt; Union[None, DocumentConverterResult]:\n        extension = kwargs.get(\"file_extension\", \"\")\n        if extension == \"\":\n            return None\n\n        content_type, encoding = mimetypes.guess_type(\"__placeholder\" + extension)\n        if content_type is None:\n            return None\n\n        if \"text/\" not in content_type.lower() and extension.lower() not in [\".txt\", \".xml\", \".jsonld\", \".pdb\"]:\n            return None\n\n        text_content = \"\"\n        with open(local_path, \"rt\") as fh:\n            text_content = fh.read()\n\n        return DocumentConverterResult(\n            title=None,\n            text_content=text_content,\n        )\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.WikipediaConverter","title":"<code>WikipediaConverter</code>","text":"<p>               Bases: <code>DocumentConverter</code></p> <p>Handle Wikipedia pages separately, focusing only on the main document content.</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class WikipediaConverter(DocumentConverter):\n    \"\"\"Handle Wikipedia pages separately, focusing only on the main document content.\"\"\"\n\n    def convert(self, local_path, **kwargs) -&gt; Union[None, DocumentConverterResult]:\n        # Bail if not Wikipedia\n        extension = kwargs.get(\"file_extension\", \"\")\n        if extension.lower() not in [\".html\", \".htm\"]:\n            return None\n        url = kwargs.get(\"url\", \"\")\n        if not re.search(r\"^https?:\\/\\/[a-zA-Z]{2,3}\\.wikipedia.org\\/\", url):\n            return None\n\n        # Parse the file\n        soup = None\n        with open(local_path, \"rt\") as fh:\n            soup = BeautifulSoup(fh.read(), \"html.parser\")\n\n        # Remove javascript and style blocks\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        # Print only the main content\n        body_elm = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n        title_elm = soup.find(\"span\", {\"class\": \"mw-page-title-main\"})\n\n        webpage_text = \"\"\n        if body_elm:\n            # What's the title\n            main_title = soup.title.string\n            if title_elm and len(title_elm) &gt; 0:\n                main_title = title_elm.string\n\n            # Convert the page\n            webpage_text = \"# \" + main_title + \"\\n\\n\" + markdownify.MarkdownConverter().convert_soup(body_elm)\n        else:\n            webpage_text = markdownify.MarkdownConverter().convert_soup(soup)\n\n        return DocumentConverterResult(\n            title=soup.title.string,\n            text_content=webpage_text,\n        )\n</code></pre>"},{"location":"reference/tools/document_converters/#tapeagents.tools.document_converters.YouTubeConverter","title":"<code>YouTubeConverter</code>","text":"<p>               Bases: <code>DocumentConverter</code></p> <p>Handle YouTube specially, focusing on the video title, description, and transcript.</p> Source code in <code>tapeagents/tools/document_converters.py</code> <pre><code>class YouTubeConverter(DocumentConverter):\n    \"\"\"Handle YouTube specially, focusing on the video title, description, and transcript.\"\"\"\n\n    def convert(self, local_path, **kwargs) -&gt; Union[None, DocumentConverterResult]:\n        # Bail if not YouTube\n        extension = kwargs.get(\"file_extension\", \"\")\n        if extension.lower() not in [\".html\", \".htm\"]:\n            return None\n        url = kwargs.get(\"url\", \"\")\n        if not url.startswith(\"https://www.youtube.com/watch?\"):\n            return None\n\n        # Parse the file\n        soup = None\n        with open(local_path, \"rt\") as fh:\n            soup = BeautifulSoup(fh.read(), \"html.parser\")\n\n        # Read the meta tags\n        metadata = {\"title\": soup.title.string}\n        for meta in soup([\"meta\"]):\n            for a in meta.attrs:\n                if a in [\"itemprop\", \"property\", \"name\"]:\n                    metadata[meta[a]] = meta.get(\"content\", \"\")\n                    break\n\n        # We can also try to read the full description. This is more prone to breaking, since it reaches into the page implementation\n        try:\n            for script in soup([\"script\"]):\n                content = script.text\n                if \"ytInitialData\" in content:\n                    lines = re.split(r\"\\r?\\n\", content)\n                    obj_start = lines[0].find(\"{\")\n                    obj_end = lines[0].rfind(\"}\")\n                    if obj_start &gt;= 0 and obj_end &gt;= 0:\n                        data = json.loads(lines[0][obj_start : obj_end + 1])\n                        attrdesc = self._findKey(data, \"attributedDescriptionBodyText\")\n                        if attrdesc:\n                            metadata[\"description\"] = attrdesc[\"content\"]\n                    break\n        except Exception:\n            pass\n\n        # Start preparing the page\n        webpage_text = \"# YouTube\\n\"\n\n        title = self._get(metadata, [\"title\", \"og:title\", \"name\"])\n        if title:\n            webpage_text += f\"\\n## {title}\\n\"\n\n        stats = \"\"\n        views = self._get(metadata, [\"interactionCount\"])\n        if views:\n            stats += f\"- **Views:** {views}\\n\"\n\n        keywords = self._get(metadata, [\"keywords\"])\n        if keywords:\n            stats += f\"- **Keywords:** {keywords}\\n\"\n\n        runtime = self._get(metadata, [\"duration\"])\n        if runtime:\n            stats += f\"- **Runtime:** {runtime}\\n\"\n\n        if len(stats) &gt; 0:\n            webpage_text += f\"\\n### Video Metadata\\n{stats}\\n\"\n\n        description = self._get(metadata, [\"description\", \"og:description\"])\n        if description:\n            webpage_text += f\"\\n### Description\\n{description}\\n\"\n\n        if IS_YOUTUBE_TRANSCRIPT_CAPABLE:\n            transcript_text = \"\"\n            parsed_url = urlparse(url)\n            params = parse_qs(parsed_url.query)\n            if \"v\" in params:\n                video_id = params[\"v\"][0]\n                try:\n                    # Must be a single transcript.\n                    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n                    transcript_text = \" \".join([part[\"text\"] for part in transcript])\n                    # Alternative formatting:\n                    # formatter = TextFormatter()\n                    # formatter.format_transcript(transcript)\n                except Exception:\n                    pass\n            if transcript_text:\n                webpage_text += f\"\\n### Transcript\\n{transcript_text}\\n\"\n\n        return DocumentConverterResult(\n            title=title if title else soup.title.string,\n            text_content=webpage_text,\n        )\n\n    def _get(self, json, keys, default=None):\n        for k in keys:\n            if k in json:\n                return json[k]\n        return default\n\n    def _findKey(self, json, key):\n        if isinstance(json, list):\n            for elm in json:\n                ret = self._findKey(elm, key)\n                if ret is not None:\n                    return ret\n        elif isinstance(json, dict):\n            for k in json:\n                if k == key:\n                    return json[k]\n                else:\n                    ret = self._findKey(json[k], key)\n                    if ret is not None:\n                        return ret\n        return None\n</code></pre>"},{"location":"reference/tools/gym_browser/","title":"Gym Browser","text":"<p>Functions:</p> <ul> <li> <code>flatten_axtree</code>             \u2013              <p>Formats the accessibility tree into a string text</p> </li> </ul>"},{"location":"reference/tools/gym_browser/#tapeagents.tools.gym_browser.flatten_axtree","title":"<code>flatten_axtree(AX_tree, extra_properties=None, with_visible=False, with_clickable=False, with_center_coords=False, with_bounding_box_coords=False, with_som=False, filter_visible_only=True, filter_with_bid_only=False, filter_som_only=False, coord_decimals=0, ignored_roles=IGNORED_ROLES, ignored_properties=IGNORED_AXTREE_PROPERTIES, ignore_navigation=False, hide_bid_if_invisible=False, hide_all_children=False, nodes_with_bid=NODES_WITH_BID)</code>","text":"<p>Formats the accessibility tree into a string text</p> Source code in <code>tapeagents/tools/gym_browser.py</code> <pre><code>def flatten_axtree(\n    AX_tree,\n    extra_properties: dict | None = None,\n    with_visible: bool = False,\n    with_clickable: bool = False,\n    with_center_coords: bool = False,\n    with_bounding_box_coords: bool = False,\n    with_som: bool = False,\n    filter_visible_only: bool = True,\n    filter_with_bid_only: bool = False,\n    filter_som_only: bool = False,\n    coord_decimals: int = 0,\n    ignored_roles=IGNORED_ROLES,\n    ignored_properties=IGNORED_AXTREE_PROPERTIES,\n    ignore_navigation: bool = False,\n    hide_bid_if_invisible: bool = False,\n    hide_all_children: bool = False,\n    nodes_with_bid: list[str] = NODES_WITH_BID,\n) -&gt; str:\n    \"\"\"Formats the accessibility tree into a string text\"\"\"\n    if ignore_navigation:\n        ignored_roles += NAVIGATION_ROLES\n    extra_properties = extra_properties or {}\n    node_id_to_idx = {}\n    for idx, node in enumerate(AX_tree[\"nodes\"]):\n        node_id_to_idx[node[\"nodeId\"]] = idx\n\n    def dfs(node_idx: int, depth: int, parent_node_filtered: bool) -&gt; str:\n        tree_str = \"\"\n        node = AX_tree[\"nodes\"][node_idx]\n        indent = \"  \" * depth\n        skip_node = False\n        filter_node = False\n        node_role = node[\"role\"][\"value\"]\n\n        if node_role in ignored_roles:\n            return tree_str\n        elif \"name\" not in node:\n            skip_node = True\n            pass\n        else:\n            node_name = node[\"name\"][\"value\"]\n            if \"value\" in node and \"value\" in node[\"value\"]:\n                node_value = node[\"value\"][\"value\"]\n            else:\n                node_value = None\n\n            attributes = []\n            bid = node.get(\"browsergym_id\", None)\n            for property in node.get(\"properties\", []):\n                if \"value\" not in property:\n                    continue\n                if \"value\" not in property[\"value\"]:\n                    continue\n\n                prop_name = property[\"name\"]\n                prop_value = property[\"value\"][\"value\"]\n\n                if prop_name == \"browsergym_id\":\n                    bid = prop_value\n                elif prop_name in ignored_properties:\n                    continue\n                elif prop_name in (\"required\", \"focused\", \"atomic\"):\n                    if prop_value:\n                        attributes.append(prop_name)\n                else:\n                    attributes.append(f\"{prop_name}={repr(prop_value)}\")\n\n            if node_role == \"generic\" and not attributes:\n                skip_node = True\n            elif node_role != \"StaticText\":\n                filter_node, extra_attributes_to_print = _process_bid(\n                    bid,\n                    extra_properties=extra_properties,\n                    with_visible=with_visible,\n                    with_clickable=with_clickable,\n                    with_center_coords=with_center_coords,\n                    with_bounding_box_coords=with_bounding_box_coords,\n                    with_som=with_som,\n                    filter_visible_only=filter_visible_only,\n                    filter_with_bid_only=filter_with_bid_only,\n                    filter_som_only=filter_som_only,\n                    coord_decimals=coord_decimals,\n                )\n\n                # if either is True, skip the node\n                skip_node = skip_node or filter_node or (hide_all_children and parent_node_filtered)\n\n                # insert extra attributes before regular attributes\n                attributes = extra_attributes_to_print + attributes\n\n            # actually print the node string\n            if not skip_node:\n                if node_role == \"paragraph\":\n                    node_str = \"\"\n                elif node_role == \"StaticText\":\n                    node_str = node_name.strip()\n                else:\n                    node_repr = node_name.strip()\n                    if node_repr and node_role != \"checkbox\":\n                        node_str = f\"{node_role} {node_repr}\"\n                    else:\n                        node_str = \"-\" if node_role == \"listitem\" else node_role\n                    if (\n                        not (\n                            bid is None\n                            or (hide_bid_if_invisible and extra_properties.get(bid, {}).get(\"visibility\", 0) &lt; 0.5)\n                        )\n                        and node_role in nodes_with_bid\n                    ):\n                        node_str = f\"BID:{bid} \" + node_str\n\n                if node_value is not None:\n                    node_str += f' value={repr(node[\"value\"][\"value\"])}'\n\n                if attributes:\n                    node_str += \", \".join([\"\"] + attributes)\n\n                if \"'Advertisement'\" in node_str:\n                    return tree_str\n                tree_str += f\"{indent}{node_str}\"\n\n        for child_node_id in node[\"childIds\"]:\n            if child_node_id not in node_id_to_idx or child_node_id == node[\"nodeId\"]:\n                continue\n            # mark this to save some tokens\n            child_depth = depth if skip_node else (depth + 1)\n            child_str = dfs(node_id_to_idx[child_node_id], child_depth, parent_node_filtered=filter_node or skip_node)\n            if child_str and node_role != \"link\":\n                if tree_str:\n                    tree_str += \"\\n\"\n                tree_str += child_str\n\n        return tree_str\n\n    return dfs(0, 0, False)\n</code></pre>"},{"location":"reference/tools/python_interpreter/","title":"Python Interpreter","text":"<p>Classes:</p> <ul> <li> <code>InterpreterError</code>           \u2013            <p>An error raised when the interpretor cannot evaluate a Python expression, due to syntax error or unsupported</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>evaluate_ast</code>             \u2013              <p>Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given</p> </li> <li> <code>evaluate_python_code</code>             \u2013              <p>Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set</p> </li> </ul>"},{"location":"reference/tools/python_interpreter/#tapeagents.tools.python_interpreter.InterpreterError","title":"<code>InterpreterError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>An error raised when the interpretor cannot evaluate a Python expression, due to syntax error or unsupported operations.</p> Source code in <code>tapeagents/tools/python_interpreter.py</code> <pre><code>class InterpreterError(ValueError):\n    \"\"\"\n    An error raised when the interpretor cannot evaluate a Python expression, due to syntax error or unsupported\n    operations.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/tools/python_interpreter/#tapeagents.tools.python_interpreter.evaluate_ast","title":"<code>evaluate_ast(expression, state, tools, authorized_imports=LIST_SAFE_MODULES)</code>","text":"<p>Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given set of functions.</p> <p>This function will recurse trough the nodes of the tree provided.</p> <p>Parameters:</p> <ul> <li> <code>expression</code>               (<code>`ast.AST`</code>)           \u2013            <p>The code to evaluate, as an abstract syntax tree.</p> </li> <li> <code>state</code>               (<code>`Dict[str, Any]`</code>)           \u2013            <p>A dictionary mapping variable names to values. The <code>state</code> is updated if need be when the evaluation encounters assignements.</p> </li> <li> <code>tools</code>               (<code>`Dict[str, Callable]`</code>)           \u2013            <p>The functions that may be called during the evaluation. Any call to another function will fail with an <code>InterpreterError</code>.</p> </li> <li> <code>authorized_imports</code>               (<code>`List[str]`</code>, default:                   <code>LIST_SAFE_MODULES</code> )           \u2013            <p>The list of modules that can be imported by the code. By default, only a few safe modules are allowed. Add more at your own risk!</p> </li> </ul> Source code in <code>tapeagents/tools/python_interpreter.py</code> <pre><code>def evaluate_ast(\n    expression: ast.AST,\n    state: Dict[str, Any],\n    tools: Dict[str, Callable],\n    authorized_imports: List[str] = LIST_SAFE_MODULES,\n):\n    \"\"\"\n    Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given\n    set of functions.\n\n    This function will recurse trough the nodes of the tree provided.\n\n    Args:\n        expression (`ast.AST`):\n            The code to evaluate, as an abstract syntax tree.\n        state (`Dict[str, Any]`):\n            A dictionary mapping variable names to values. The `state` is updated if need be when the evaluation\n            encounters assignements.\n        tools (`Dict[str, Callable]`):\n            The functions that may be called during the evaluation. Any call to another function will fail with an\n            `InterpreterError`.\n        authorized_imports (`List[str]`):\n            The list of modules that can be imported by the code. By default, only a few safe modules are allowed.\n            Add more at your own risk!\n    \"\"\"\n    if isinstance(expression, ast.Assign):\n        # Assignement -&gt; we evaluate the assignment which should update the state\n        # We return the variable assigned as it may be used to determine the final result.\n        return evaluate_assign(expression, state, tools)\n    elif isinstance(expression, ast.AugAssign):\n        return evaluate_augassign(expression, state, tools)\n    elif isinstance(expression, ast.Call):\n        # Function call -&gt; we return the value of the function call\n        return evaluate_call(expression, state, tools)\n    elif isinstance(expression, ast.Constant):\n        # Constant -&gt; just return the value\n        return expression.value\n    elif isinstance(expression, ast.Tuple):\n        return tuple(evaluate_ast(elt, state, tools) for elt in expression.elts)\n    elif isinstance(expression, ast.ListComp):\n        return evaluate_listcomp(expression, state, tools)\n    elif isinstance(expression, ast.UnaryOp):\n        return evaluate_unaryop(expression, state, tools)\n    elif isinstance(expression, ast.BoolOp):\n        # Boolean operation -&gt; evaluate the operation\n        return evaluate_boolop(expression, state, tools)\n    elif isinstance(expression, ast.Break):\n        raise BreakException()\n    elif isinstance(expression, ast.Continue):\n        raise ContinueException()\n    elif isinstance(expression, ast.BinOp):\n        # Binary operation -&gt; execute operation\n        return evaluate_binop(expression, state, tools)\n    elif isinstance(expression, ast.Compare):\n        # Comparison -&gt; evaluate the comparison\n        return evaluate_condition(expression, state, tools)\n    elif isinstance(expression, ast.Lambda):\n        return evaluate_lambda(expression, state, tools)\n    elif isinstance(expression, ast.FunctionDef):\n        return evaluate_function_def(expression, state, tools)\n    elif isinstance(expression, ast.Dict):\n        # Dict -&gt; evaluate all keys and values\n        keys = [evaluate_ast(k, state, tools) for k in expression.keys]\n        values = [evaluate_ast(v, state, tools) for v in expression.values]\n        return dict(zip(keys, values))\n    elif isinstance(expression, ast.Expr):\n        # Expression -&gt; evaluate the content\n        return evaluate_ast(expression.value, state, tools)\n    elif isinstance(expression, ast.For):\n        # For loop -&gt; execute the loop\n        return evaluate_for(expression, state, tools)\n    elif isinstance(expression, ast.FormattedValue):\n        # Formatted value (part of f-string) -&gt; evaluate the content and return\n        return evaluate_ast(expression.value, state, tools)\n    elif isinstance(expression, ast.If):\n        # If -&gt; execute the right branch\n        return evaluate_if(expression, state, tools)\n    elif hasattr(ast, \"Index\") and isinstance(expression, ast.Index):\n        return evaluate_ast(expression.value, state, tools)\n    elif isinstance(expression, ast.JoinedStr):\n        return \"\".join([str(evaluate_ast(v, state, tools)) for v in expression.values])\n    elif isinstance(expression, ast.List):\n        # List -&gt; evaluate all elements\n        return [evaluate_ast(elt, state, tools) for elt in expression.elts]\n    elif isinstance(expression, ast.Name):\n        # Name -&gt; pick up the value in the state\n        return evaluate_name(expression, state, tools)\n    elif isinstance(expression, ast.Subscript):\n        # Subscript -&gt; return the value of the indexing\n        return evaluate_subscript(expression, state, tools)\n    elif isinstance(expression, ast.IfExp):\n        test_val = evaluate_ast(expression.test, state, tools)\n        if test_val:\n            return evaluate_ast(expression.body, state, tools)\n        else:\n            return evaluate_ast(expression.orelse, state, tools)\n    elif isinstance(expression, ast.Attribute):\n        obj = evaluate_ast(expression.value, state, tools)\n        return getattr(obj, expression.attr)\n    elif isinstance(expression, ast.Slice):\n        return slice(\n            evaluate_ast(expression.lower, state, tools) if expression.lower is not None else None,\n            evaluate_ast(expression.upper, state, tools) if expression.upper is not None else None,\n            evaluate_ast(expression.step, state, tools) if expression.step is not None else None,\n        )\n    elif isinstance(expression, ast.ListComp) or isinstance(expression, ast.GeneratorExp):\n        result = []\n        vars = {}\n        for generator in expression.generators:\n            var_name = generator.target.id\n            iter_value = evaluate_ast(generator.iter, state, tools)\n            for value in iter_value:\n                vars[var_name] = value\n                if all(evaluate_ast(if_clause, {**state, **vars}, tools) for if_clause in generator.ifs):\n                    elem = evaluate_ast(expression.elt, {**state, **vars}, tools)\n                    result.append(elem)\n        return result\n    elif isinstance(expression, ast.DictComp):\n        result = {}\n        for gen in expression.generators:\n            for container in get_iterable(evaluate_ast(gen.iter, state, tools)):\n                state[gen.target.id] = container\n                key = evaluate_ast(expression.key, state, tools)\n                value = evaluate_ast(expression.value, state, tools)\n                result[key] = value\n        return result\n    elif isinstance(expression, ast.Import):\n        for alias in expression.names:\n            if alias.name in authorized_imports:\n                module = __import__(alias.name)\n                state[alias.asname or alias.name] = module\n            else:\n                raise InterpreterError(f\"Import of {alias.name} is not allowed.\")\n        return None\n    elif isinstance(expression, ast.While):\n        return evaluate_while(expression, state, tools)\n    elif isinstance(expression, ast.ImportFrom):\n        if expression.module in authorized_imports:\n            module = __import__(expression.module)\n            for alias in expression.names:\n                state[alias.asname or alias.name] = getattr(module, alias.name)\n        else:\n            raise InterpreterError(f\"Import from {expression.module} is not allowed.\")\n        return None\n    elif isinstance(expression, ast.ClassDef):\n        return evaluate_class_def(expression, state, tools)\n    elif isinstance(expression, ast.Try):\n        return evaluate_try(expression, state, tools)\n    elif isinstance(expression, ast.Raise):\n        return evaluate_raise(expression, state, tools)\n    elif isinstance(expression, ast.Assert):\n        return evaluate_assert(expression, state, tools)\n    elif isinstance(expression, ast.With):\n        return evaluate_with(expression, state, tools)\n    elif isinstance(expression, ast.Set):\n        return {evaluate_ast(elt, state, tools) for elt in expression.elts}\n    elif isinstance(expression, ast.Return):\n        raise ReturnException(evaluate_ast(expression.value, state, tools) if expression.value else None)\n    else:\n        # For now we refuse anything else. Let's add things as we need them.\n        raise InterpreterError(f\"{expression.__class__.__name__} is not supported.\")\n</code></pre>"},{"location":"reference/tools/python_interpreter/#tapeagents.tools.python_interpreter.evaluate_python_code","title":"<code>evaluate_python_code(code, tools={}, state=None, authorized_imports=AUTHORIZED_IMPORTS)</code>","text":"<p>Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set of functions.</p> <p>This function will recurse through the nodes of the tree provided.</p> <p>Parameters:</p> <ul> <li> <code>code</code>               (<code>`str`</code>)           \u2013            <p>The code to evaluate.</p> </li> <li> <code>tools</code>               (<code>`Dict[str, Callable]`</code>, default:                   <code>{}</code> )           \u2013            <p>The functions that may be called during the evaluation. Any call to another function will fail with an <code>InterpreterError</code>.</p> </li> <li> <code>state</code>               (<code>`Dict[str, Any]`</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary mapping variable names to values. The <code>state</code> should contain the initial inputs but will be updated by this function to contain all variables as they are evaluated. The print outputs will be stored in the state under the key 'print_outputs'.</p> </li> </ul> Source code in <code>tapeagents/tools/python_interpreter.py</code> <pre><code>def evaluate_python_code(\n    code: str,\n    tools: Optional[Dict[str, Callable]] = {},\n    state=None,\n    authorized_imports: List[str] = AUTHORIZED_IMPORTS,\n):\n    \"\"\"\n    Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set\n    of functions.\n\n    This function will recurse through the nodes of the tree provided.\n\n    Args:\n        code (`str`):\n            The code to evaluate.\n        tools (`Dict[str, Callable]`):\n            The functions that may be called during the evaluation. Any call to another function will fail with an\n            `InterpreterError`.\n        state (`Dict[str, Any]`):\n            A dictionary mapping variable names to values. The `state` should contain the initial inputs but will be\n            updated by this function to contain all variables as they are evaluated.\n            The print outputs will be stored in the state under the key 'print_outputs'.\n    \"\"\"\n    try:\n        expression = ast.parse(code)\n    except SyntaxError as e:\n        raise SyntaxError(f\"The code generated by the agent is not valid.\\n{e}\")\n    if state is None:\n        state = {}\n    result = None\n    global PRINT_OUTPUTS\n    PRINT_OUTPUTS = \"\"\n    for node in expression.body:\n        try:\n            result = evaluate_ast(node, state, tools, authorized_imports)\n        except InterpreterError as e:\n            msg = f\"Evaluation stopped at line '{ast.get_source_segment(code, node)}' because of the following error:\\n{e}\"\n            if len(PRINT_OUTPUTS) &gt; 0:\n                msg += f\"Executing code yielded these outputs:\\n{PRINT_OUTPUTS}\\n====\\n\"\n            raise InterpreterError(msg)\n        finally:\n            state[\"print_outputs\"] = PRINT_OUTPUTS\n\n    return result\n</code></pre>"},{"location":"reference/tools/simple_browser/","title":"Simple Browser","text":"<p>Classes:</p> <ul> <li> <code>SimpleTextBrowser</code>           \u2013            <p>An extremely simple text-based web browser suitable for Agentic use.</p> </li> </ul>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser","title":"<code>SimpleTextBrowser</code>","text":"<p>An extremely simple text-based web browser suitable for Agentic use.</p> <p>Methods:</p> <ul> <li> <code>find_next</code>             \u2013              <p>Scroll to the next viewport that matches the query</p> </li> <li> <code>find_on_page</code>             \u2013              <p>Searches for the query from the current viewport forward, looping back to the start if necessary.</p> </li> <li> <code>get_next_page</code>             \u2013              <p>Load next page of the document and return the current content of the viewport, current page number and total number of pages.</p> </li> <li> <code>get_page</code>             \u2013              <p>Load web page and return content of its first viewport (first screen), current page number and total number of pages.</p> </li> <li> <code>get_search_results</code>             \u2013              <p>Get search results for the query.</p> </li> <li> <code>set_address</code>             \u2013              <p>Update the address, visit the page, and set the content of the viewport.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>address</code>               (<code>str</code>)           \u2013            <p>Return the address of the current page.</p> </li> <li> <code>page_content</code>               (<code>str</code>)           \u2013            <p>Return the full contents of the current page.</p> </li> <li> <code>viewport</code>               (<code>str</code>)           \u2013            <p>Return the content of the current viewport.</p> </li> </ul> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>class SimpleTextBrowser:\n    \"\"\"An extremely simple text-based web browser suitable for Agentic use.\"\"\"\n\n    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n\n    def __init__(\n        self,\n        start_page: Optional[str] = None,\n        viewport_size: Optional[int] = 32000,\n        downloads_folder: str = \"/tmp/agent_browser_downloads\",\n        use_tavily: bool = False,\n        use_web_cache: bool = True,\n        only_cached_webpages: bool = False,\n        vision_lm: LLM | None = None,\n        request_kwargs: Optional[Union[Dict[str, Any], None]] = None,\n        converter_kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        self.start_page: str = start_page if start_page else \"about:blank\"\n        self.viewport_size = viewport_size  # Applies only to the standard uri types\n        self.downloads_folder = downloads_folder\n        os.makedirs(self.downloads_folder, exist_ok=True)\n        self.history: List[Tuple[str, float]] = list()\n        self.page_title: Optional[str] = None\n        self.viewport_current_page = 0\n        self.viewport_pages: List[Tuple[int, int]] = list()\n        self.set_address(self.start_page)\n        self.request_kwargs = request_kwargs or {\"headers\": {\"User-Agent\": self.user_agent}}\n        self.request_kwargs[\"headers\"] = self.request_kwargs.get(\"headers\", {})\n\n        def img2text(messages: list[dict]) -&gt; str:\n            assert vision_lm\n            for event in vision_lm.generate(Prompt(messages=messages)):\n                if event.output and event.output.content:\n                    logger.debug(\"Image caption\", event.output.content)\n                    return event.output.content\n            raise Exception(\"No answer from vision model\")\n\n        mlm_client = img2text if vision_lm else None\n        self._mdconvert = FileConverter(mlm_client=mlm_client)\n\n        self._page_content: str = \"\"\n        self._page_error: int = 0\n        self.tavily = TavilyClient(api_key=get_tavily_key()) if use_tavily else None\n        self.converter_kwargs = converter_kwargs or {}\n\n        self._find_on_page_query: Union[str, None] = None\n        self._find_on_page_last_result: Union[int, None] = None  # Location of the last result\n\n        self.use_web_cache = use_web_cache\n        self.only_cached_webpages = only_cached_webpages\n        self._cache = {}\n        self._log = {}\n        self._cache_writes = 0\n        self._cache_filename = \"web_cache.json\"\n        if _FORCE_CACHE_PATH:\n            self._cache_filename = _FORCE_CACHE_PATH\n            self.only_cached_webpages = True\n            assert os.path.exists(self._cache_filename), \"Forced cache file not found\"\n        if os.path.exists(self._cache_filename):\n            with open(self._cache_filename) as f:\n                self._cache = json.load(f)\n            logger.info(f\"Loaded {len(self._cache)} web results from cache\")\n\n    @property\n    def address(self) -&gt; str:\n        \"\"\"Return the address of the current page.\"\"\"\n        return self.history[-1][0]\n\n    def set_address(self, uri_or_path: str) -&gt; None:\n        \"\"\"Update the address, visit the page, and set the content of the viewport.\"\"\"\n        self.history.append((uri_or_path, time.time()))\n\n        # Handle special URIs\n        if uri_or_path == \"about:blank\":\n            self._set_page_content(\"\")\n        else:\n            if (\n                not uri_or_path.startswith(\"http:\")\n                and not uri_or_path.startswith(\"https:\")\n                and not uri_or_path.startswith(\"file:\")\n            ):\n                if len(self.history) &gt; 1:\n                    prior_address = self.history[-2][0]\n                    uri_or_path = urljoin(prior_address, uri_or_path)\n                    # Update the address with the fully-qualified path\n                    self.history[-1] = (uri_or_path, self.history[-1][1])\n            self._fetch_page(uri_or_path)\n\n        self.viewport_current_page = 0\n        self.find_on_page_query = None\n        self.find_on_page_viewport = None\n\n    @property\n    def viewport(self) -&gt; str:\n        \"\"\"Return the content of the current viewport.\"\"\"\n        bounds = self.viewport_pages[self.viewport_current_page]\n        return self.page_content[bounds[0] : bounds[1]]\n\n    @property\n    def page_content(self) -&gt; str:\n        \"\"\"Return the full contents of the current page.\"\"\"\n        return self._page_content\n\n    def _set_page_content(self, content: str) -&gt; None:\n        \"\"\"Sets the text content of the current page.\"\"\"\n        self._page_content = content\n        self._split_pages()\n        if self.viewport_current_page &gt;= len(self.viewport_pages):\n            self.viewport_current_page = len(self.viewport_pages) - 1\n\n    def page_down(self) -&gt; None:\n        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)\n\n    def page_up(self) -&gt; None:\n        self.viewport_current_page = max(self.viewport_current_page - 1, 0)\n\n    def find_on_page(self, query: str) -&gt; Union[str, None]:\n        \"\"\"Searches for the query from the current viewport forward, looping back to the start if necessary.\"\"\"\n\n        # Did we get here via a previous find_on_page search with the same query?\n        # If so, map to find_next\n        if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:\n            return self.find_next()\n\n        # Ok it's a new search start from the current viewport\n        self._find_on_page_query = query\n        viewport_match = self._find_next_viewport(query, self.viewport_current_page)\n        if viewport_match is None:\n            self._find_on_page_last_result = None\n            return None\n        else:\n            self.viewport_current_page = viewport_match\n            self._find_on_page_last_result = viewport_match\n            return self.viewport\n\n    def find_next(self) -&gt; str | None:\n        \"\"\"Scroll to the next viewport that matches the query\"\"\"\n\n        if self._find_on_page_query is None:\n            return None\n\n        starting_viewport = self._find_on_page_last_result\n        if starting_viewport is None:\n            starting_viewport = 0\n        else:\n            starting_viewport += 1\n            if starting_viewport &gt;= len(self.viewport_pages):\n                starting_viewport = 0\n\n        viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)\n        if viewport_match is None:\n            self._find_on_page_last_result = None\n            return None\n        else:\n            self.viewport_current_page = viewport_match\n            self._find_on_page_last_result = viewport_match\n            return self.viewport\n\n    def _find_next_viewport(self, query: str, starting_viewport: int) -&gt; Union[int, None]:\n        \"\"\"Search for matches between the starting viewport looping when reaching the end.\"\"\"\n\n        if query is None:\n            return None\n\n        # Normalize the query, and convert to a regular expression\n        nquery = re.sub(r\"\\*\", \"__STAR__\", query)\n        nquery = \" \" + (\" \".join(re.split(r\"\\W+\", nquery))).strip() + \" \"\n        nquery = nquery.replace(\" __STAR__ \", \"__STAR__ \")  # Merge isolated stars with prior word\n        nquery = nquery.replace(\"__STAR__\", \".*\").lower()\n\n        if nquery.strip() == \"\":\n            return None\n\n        idxs = list()\n        idxs.extend(range(starting_viewport, len(self.viewport_pages)))\n        idxs.extend(range(0, starting_viewport))\n\n        for i in idxs:\n            bounds = self.viewport_pages[i]\n            content = self.page_content[bounds[0] : bounds[1]]\n\n            # TODO: Remove markdown links and images\n            ncontent = \" \" + (\" \".join(re.split(r\"\\W+\", content))).strip().lower() + \" \"\n            if re.search(nquery, ncontent):\n                return i\n\n        return None\n\n    def _split_pages(self) -&gt; None:\n        # Do not split search results\n        if self.address.startswith(\"search:\"):\n            self.viewport_pages = [(0, len(self._page_content))]\n            return\n\n        # Handle empty pages\n        if len(self._page_content) == 0:\n            self.viewport_pages = [(0, 0)]\n            return\n\n        # Break the viewport into pages\n        self.viewport_pages = []\n        start_idx = 0\n        while start_idx &lt; len(self._page_content):\n            end_idx = min(start_idx + self.viewport_size, len(self._page_content))  # type: ignore[operator]\n            # Adjust to end on a space\n            while end_idx &lt; len(self._page_content) and self._page_content[end_idx - 1] not in [\" \", \"\\t\", \"\\r\", \"\\n\"]:\n                end_idx += 1\n            self.viewport_pages.append((start_idx, end_idx))\n            start_idx = end_idx\n\n    def get_search_results(self, query: str, max_results: int = 5) -&gt; list[dict]:\n        \"\"\"Get search results for the query.\n\n        Return list of dictionaries with keys 'title', 'url', and 'content'.\n\n        \"\"\"\n        key = query.lower().strip()\n        if self.use_web_cache and (key in self._cache or query in self._cache):\n            if query in self._cache:\n                key = query\n            logger.info(colored(f\"Cache hit for search {query}\", \"green\"))\n            self._log[query] = self._cache[key]\n            return self._cache[key][:max_results]\n        if self.only_cached_webpages:\n            ratios = [(k, ratio(key, k, score_cutoff=0.5)) for k in self._cache.keys()]\n            if not len(ratios):\n                raise FatalError(f'No cache for \"{query}\"')\n            closest, score = sorted(ratios, key=lambda x: x[1], reverse=True)[0]\n            raise FatalError(f'No cache for \"{query}\". Closest with score {score}:\\n\"{closest}\"')\n        if self.tavily is not None:\n            serp = self.tavily.search(query=query, search_depth=\"basic\", max_results=max_results) or {\"results\": []}\n            results = [{\"title\": r[\"title\"], \"url\": r[\"url\"], \"content\": r[\"content\"][:200]} for r in serp[\"results\"]]\n        else:\n            with search_lock:\n                results = [\n                    {\"title\": r.title, \"url\": r.url, \"content\": r.description}\n                    for r in search(query, advanced=True, num_results=max_results)\n                ]\n                time.sleep(2)  # Avoid rate limiting of the search engine\n        self._add_to_cache(key, results)\n        return results[:max_results]\n\n    def _fetch_page(self, url: str) -&gt; None:\n        download_path = \"\"\n        response = None\n        try:\n            if url.startswith(\"file://\"):\n                download_path = os.path.normcase(os.path.normpath(unquote(url[7:])))\n                res = self._mdconvert.convert_local(download_path, **self.converter_kwargs)\n                self.page_title = res.title\n                self._set_page_content(res.text_content)\n            else:\n                # Prepare the request parameters\n                request_kwargs = self.request_kwargs.copy() if self.request_kwargs is not None else {}\n                request_kwargs[\"stream\"] = True\n\n                response = requests.get(url, **request_kwargs)\n                response.raise_for_status()\n\n                content_type = response.headers.get(\"content-type\", \"\")\n                # Text or HTML\n                if \"text/\" in content_type.lower():\n                    res = self._mdconvert.convert_response(response, **self.converter_kwargs)\n                    self.page_title = res.title\n                    self._set_page_content(res.text_content)\n                # A download\n                else:\n                    # Try producing a safe filename\n                    fname = None\n                    download_path = None\n                    try:\n                        fname = pathvalidate.sanitize_filename(os.path.basename(urlparse(url).path)).strip()\n                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))\n\n                        suffix = 0\n                        while os.path.exists(download_path) and suffix &lt; 1000:\n                            suffix += 1\n                            base, ext = os.path.splitext(fname)\n                            new_fname = f\"{base}__{suffix}{ext}\"\n                            download_path = os.path.abspath(os.path.join(self.downloads_folder, new_fname))\n                    except NameError:\n                        pass\n\n                    # No suitable name, so make one\n                    if fname is None:\n                        extension = mimetypes.guess_extension(content_type)\n                        if extension is None:\n                            extension = \".download\"\n                        fname = str(uuid.uuid4()) + extension\n                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))\n\n                    # Open a file for writing\n                    if not download_path:\n                        raise ValueError(\"Could not determine a suitable download path.\")\n\n                    with open(download_path, \"wb\") as fh:\n                        for chunk in response.iter_content(chunk_size=512):\n                            fh.write(chunk)\n\n                    # Render it\n                    local_uri = pathlib.Path(download_path).as_uri()\n                    self.set_address(local_uri)\n\n        except UnsupportedFormatException as e:\n            print(colored(f\"UnsupportedFormatException: {e}\", \"red\"))\n            self.page_title = \"Unsupported Format\"\n            self._set_page_content(f\"Unsupported Format File: {e}\")\n            self._page_error = 1\n        except FileConversionException as e:\n            print(colored(f\"FileConversionException: {e}\", \"red\"))\n            self.page_title = \"Failed to read file\"\n            self._set_page_content(f\"Error: {e}\")\n            self._page_error = 2\n        except FileNotFoundError:\n            self.page_title = \"Error 404\"\n            self._set_page_content(f\"## Error 404\\n\\nFile not found: {download_path}\")\n            self._page_error = 404\n        except requests.exceptions.RequestException as e:\n            if response is None:\n                self._set_page_content(f\"## Error {e}\")\n                self._page_error = 3\n            else:\n                self.page_title = f\"Error {response.status_code}\"\n                self._page_error = response.status_code\n                # If the error was rendered in HTML we might as well render it\n                content_type = response.headers.get(\"content-type\", \"\")\n                if content_type is not None and \"text/html\" in content_type.lower():\n                    res = self._mdconvert.convert(response, **self.converter_kwargs)\n                    self.page_title = f\"Error {response.status_code}\"\n                    self._set_page_content(f\"## Error {response.status_code}\\n\\n{res.text_content[:500]}\")\n                else:\n                    text = \"\"\n                    for chunk in response.iter_content(chunk_size=512, decode_unicode=True):\n                        text += chunk\n                    self.page_title = f\"Error {response.status_code}\"\n                    self._set_page_content(f\"## Error {response.status_code}\\n\\n{text[:500]}\")\n        except Exception as e:\n            self._page_error = -1\n            self.page_title = \"Failed to fetch page\"\n            self._set_page_content(str(e))\n\n    def page_with_title(self) -&gt; str:\n        if self._page_error:\n            header = (\n                f\"Failed to load page, Error {self._page_error}\\nTitle: {self.page_title}\\n=======================\\n\"\n            )\n        else:\n            header = f\"Title: {self.page_title}\\n=======================\\n\" if self.page_title else \"\"\n        return header + self.viewport.strip()\n\n    def set_web_cache(self, cache: dict) -&gt; None:\n        self._cache = cache\n\n    def _add_to_cache(self, k: str, value: Any) -&gt; None:\n        self._cache[k] = value\n        self._log[k] = value\n\n    def save_cache(self):\n        with open(self._cache_filename, \"w\") as f:\n            json.dump(self._cache, f, indent=2, ensure_ascii=False)\n\n    def get_page(self, url: str) -&gt; tuple[str, int, int]:\n        \"\"\"\n        Load web page and return content of its first viewport (first screen), current page number and total number of pages.\n        \"\"\"\n        self._page_error = 0\n        if url.startswith(\"/\"):\n            # in case of a local file\n            url = f\"file://{url}\"\n        if self.use_web_cache and url in self._cache:\n            logger.info(colored(f\"Cache hit {url}\", \"green\"))\n            self._log[url] = self._cache[url]\n            content, title = self._cache[url]\n            self.history.append((url, time.time()))\n            self.page_title = title\n            self._set_page_content(content)\n            self.viewport_current_page = 0\n        elif self.only_cached_webpages:\n            ratios = [(k, ratio(url, k, score_cutoff=0.7)) for k in self._cache.keys()]\n            closest, score = sorted(ratios, key=lambda x: x[1], reverse=True)[0]\n            if score &gt;= 0.7:\n                logger.warning(diff_strings(url, closest))\n                logger.warning(f\"Closest url score: {score:.3f}\")\n            raise FatalError(f\"Page {url} not in cache\")\n        else:\n            logger.info(colored(f\"Page {url} not in cache\", \"yellow\"))\n            self.page_title = \"\"\n            self.set_address(url)\n            self._add_to_cache(url, (self.page_content, self.page_title))\n        error = self._page_error\n        self._page_error = 0\n        return (self.page_with_title(), len(self.viewport_pages), error)\n\n    def get_next_page(self) -&gt; tuple[str, int, int]:\n        \"\"\"\n        Load next page of the document and return the current content of the viewport, current page number and total number of pages.\n        \"\"\"\n        if self.viewport_current_page + 1 == len(self.viewport_pages):\n            raise ValueError(\"No more pages to read.\")\n        self.page_down()\n        return (\n            self.page_with_title(),\n            self.viewport_current_page + 1,\n            len(self.viewport_pages),\n        )\n\n    def get_whole_document(self, url: str) -&gt; str:\n        try:\n            self.get_page(url)\n        except Exception as e:\n            raise Exception(f\"Failed to load page {url}.\\nError: {e}\")\n        return self.page_content\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.address","title":"<code>address: str</code>  <code>property</code>","text":"<p>Return the address of the current page.</p>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.page_content","title":"<code>page_content: str</code>  <code>property</code>","text":"<p>Return the full contents of the current page.</p>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.viewport","title":"<code>viewport: str</code>  <code>property</code>","text":"<p>Return the content of the current viewport.</p>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.find_next","title":"<code>find_next()</code>","text":"<p>Scroll to the next viewport that matches the query</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def find_next(self) -&gt; str | None:\n    \"\"\"Scroll to the next viewport that matches the query\"\"\"\n\n    if self._find_on_page_query is None:\n        return None\n\n    starting_viewport = self._find_on_page_last_result\n    if starting_viewport is None:\n        starting_viewport = 0\n    else:\n        starting_viewport += 1\n        if starting_viewport &gt;= len(self.viewport_pages):\n            starting_viewport = 0\n\n    viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)\n    if viewport_match is None:\n        self._find_on_page_last_result = None\n        return None\n    else:\n        self.viewport_current_page = viewport_match\n        self._find_on_page_last_result = viewport_match\n        return self.viewport\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.find_on_page","title":"<code>find_on_page(query)</code>","text":"<p>Searches for the query from the current viewport forward, looping back to the start if necessary.</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def find_on_page(self, query: str) -&gt; Union[str, None]:\n    \"\"\"Searches for the query from the current viewport forward, looping back to the start if necessary.\"\"\"\n\n    # Did we get here via a previous find_on_page search with the same query?\n    # If so, map to find_next\n    if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:\n        return self.find_next()\n\n    # Ok it's a new search start from the current viewport\n    self._find_on_page_query = query\n    viewport_match = self._find_next_viewport(query, self.viewport_current_page)\n    if viewport_match is None:\n        self._find_on_page_last_result = None\n        return None\n    else:\n        self.viewport_current_page = viewport_match\n        self._find_on_page_last_result = viewport_match\n        return self.viewport\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.get_next_page","title":"<code>get_next_page()</code>","text":"<p>Load next page of the document and return the current content of the viewport, current page number and total number of pages.</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def get_next_page(self) -&gt; tuple[str, int, int]:\n    \"\"\"\n    Load next page of the document and return the current content of the viewport, current page number and total number of pages.\n    \"\"\"\n    if self.viewport_current_page + 1 == len(self.viewport_pages):\n        raise ValueError(\"No more pages to read.\")\n    self.page_down()\n    return (\n        self.page_with_title(),\n        self.viewport_current_page + 1,\n        len(self.viewport_pages),\n    )\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.get_page","title":"<code>get_page(url)</code>","text":"<p>Load web page and return content of its first viewport (first screen), current page number and total number of pages.</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def get_page(self, url: str) -&gt; tuple[str, int, int]:\n    \"\"\"\n    Load web page and return content of its first viewport (first screen), current page number and total number of pages.\n    \"\"\"\n    self._page_error = 0\n    if url.startswith(\"/\"):\n        # in case of a local file\n        url = f\"file://{url}\"\n    if self.use_web_cache and url in self._cache:\n        logger.info(colored(f\"Cache hit {url}\", \"green\"))\n        self._log[url] = self._cache[url]\n        content, title = self._cache[url]\n        self.history.append((url, time.time()))\n        self.page_title = title\n        self._set_page_content(content)\n        self.viewport_current_page = 0\n    elif self.only_cached_webpages:\n        ratios = [(k, ratio(url, k, score_cutoff=0.7)) for k in self._cache.keys()]\n        closest, score = sorted(ratios, key=lambda x: x[1], reverse=True)[0]\n        if score &gt;= 0.7:\n            logger.warning(diff_strings(url, closest))\n            logger.warning(f\"Closest url score: {score:.3f}\")\n        raise FatalError(f\"Page {url} not in cache\")\n    else:\n        logger.info(colored(f\"Page {url} not in cache\", \"yellow\"))\n        self.page_title = \"\"\n        self.set_address(url)\n        self._add_to_cache(url, (self.page_content, self.page_title))\n    error = self._page_error\n    self._page_error = 0\n    return (self.page_with_title(), len(self.viewport_pages), error)\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.get_search_results","title":"<code>get_search_results(query, max_results=5)</code>","text":"<p>Get search results for the query.</p> <p>Return list of dictionaries with keys 'title', 'url', and 'content'.</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def get_search_results(self, query: str, max_results: int = 5) -&gt; list[dict]:\n    \"\"\"Get search results for the query.\n\n    Return list of dictionaries with keys 'title', 'url', and 'content'.\n\n    \"\"\"\n    key = query.lower().strip()\n    if self.use_web_cache and (key in self._cache or query in self._cache):\n        if query in self._cache:\n            key = query\n        logger.info(colored(f\"Cache hit for search {query}\", \"green\"))\n        self._log[query] = self._cache[key]\n        return self._cache[key][:max_results]\n    if self.only_cached_webpages:\n        ratios = [(k, ratio(key, k, score_cutoff=0.5)) for k in self._cache.keys()]\n        if not len(ratios):\n            raise FatalError(f'No cache for \"{query}\"')\n        closest, score = sorted(ratios, key=lambda x: x[1], reverse=True)[0]\n        raise FatalError(f'No cache for \"{query}\". Closest with score {score}:\\n\"{closest}\"')\n    if self.tavily is not None:\n        serp = self.tavily.search(query=query, search_depth=\"basic\", max_results=max_results) or {\"results\": []}\n        results = [{\"title\": r[\"title\"], \"url\": r[\"url\"], \"content\": r[\"content\"][:200]} for r in serp[\"results\"]]\n    else:\n        with search_lock:\n            results = [\n                {\"title\": r.title, \"url\": r.url, \"content\": r.description}\n                for r in search(query, advanced=True, num_results=max_results)\n            ]\n            time.sleep(2)  # Avoid rate limiting of the search engine\n    self._add_to_cache(key, results)\n    return results[:max_results]\n</code></pre>"},{"location":"reference/tools/simple_browser/#tapeagents.tools.simple_browser.SimpleTextBrowser.set_address","title":"<code>set_address(uri_or_path)</code>","text":"<p>Update the address, visit the page, and set the content of the viewport.</p> Source code in <code>tapeagents/tools/simple_browser.py</code> <pre><code>def set_address(self, uri_or_path: str) -&gt; None:\n    \"\"\"Update the address, visit the page, and set the content of the viewport.\"\"\"\n    self.history.append((uri_or_path, time.time()))\n\n    # Handle special URIs\n    if uri_or_path == \"about:blank\":\n        self._set_page_content(\"\")\n    else:\n        if (\n            not uri_or_path.startswith(\"http:\")\n            and not uri_or_path.startswith(\"https:\")\n            and not uri_or_path.startswith(\"file:\")\n        ):\n            if len(self.history) &gt; 1:\n                prior_address = self.history[-2][0]\n                uri_or_path = urljoin(prior_address, uri_or_path)\n                # Update the address with the fully-qualified path\n                self.history[-1] = (uri_or_path, self.history[-1][1])\n        self._fetch_page(uri_or_path)\n\n    self.viewport_current_page = 0\n    self.find_on_page_query = None\n    self.find_on_page_viewport = None\n</code></pre>"},{"location":"reference/tools/stock/","title":"Stock","text":"<p>Functions:</p> <ul> <li> <code>get_stock_data</code>             \u2013              <p>Get stock proces for a given symbol and date range.</p> </li> <li> <code>get_stock_ticker</code>             \u2013              <p>Get company stock ticker from its name.</p> </li> </ul>"},{"location":"reference/tools/stock/#tapeagents.tools.stock.get_stock_data","title":"<code>get_stock_data(symbol, start_date, end_date)</code>","text":"<p>Get stock proces for a given symbol and date range.</p> <p>Parameters:</p> <ul> <li> <code>symbol</code>               (<code>str</code>)           \u2013            <p>Stock symbol.</p> </li> <li> <code>start_date</code>               (<code>str</code>)           \u2013            <p>Start date in the format 'YYYY-MM-DD'.</p> </li> <li> <code>end_date</code>               (<code>str</code>)           \u2013            <p>End date in the format 'YYYY-MM-DD'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[tuple]</code>           \u2013            <p>List of tuples, each tuple contains a 'YYYY-MM-DD' date and the stock price.</p> </li> </ul> Source code in <code>tapeagents/tools/stock.py</code> <pre><code>def get_stock_data(symbol: str, start_date: str, end_date: str):\n    \"\"\"Get stock proces for a given symbol and date range.\n\n    Args:\n        symbol (str): Stock symbol.\n        start_date (str): Start date in the format 'YYYY-MM-DD'.\n        end_date (str): End date in the format 'YYYY-MM-DD'.\n\n    Returns:\n        (list[tuple]): List of tuples, each tuple contains a 'YYYY-MM-DD' date and the stock price.\n    \"\"\"\n    symbol = symbol.upper()\n    # parse timestamps using datetime\n    start_timestamp = int(datetime.datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n    end_timestamp = int(datetime.datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n\n    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}?period1={start_timestamp}&amp;period2={end_timestamp}&amp;interval=1d\"\n\n    try:\n        # make a request to Yahoo Finance API\n        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        response.raise_for_status()\n        data = response.json()\n\n        timestamps = data[\"chart\"][\"result\"][0][\"timestamp\"]\n        prices = data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"close\"]\n        while len(timestamps) &gt; 100:\n            timestamps = timestamps[::2]\n            prices = prices[::2]\n\n        return list(\n            zip(\n                [datetime.datetime.fromtimestamp(ts, datetime.timezone.utc).strftime(\"%Y-%m-%d\") for ts in timestamps],\n                prices,\n            )\n        )\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n</code></pre>"},{"location":"reference/tools/stock/#tapeagents.tools.stock.get_stock_ticker","title":"<code>get_stock_ticker(company_name)</code>","text":"<p>Get company stock ticker from its name.</p> Source code in <code>tapeagents/tools/stock.py</code> <pre><code>def get_stock_ticker(company_name: str):\n    \"\"\"Get company stock ticker from its name.\"\"\"\n    yfinance = \"https://query2.finance.yahoo.com/v1/finance/search\"\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n    params = {\"q\": company_name, \"quotes_count\": 1, \"country\": \"United States\"}\n\n    res = requests.get(url=yfinance, params=params, headers={\"User-Agent\": user_agent})\n    data = res.json()\n\n    company_code = data[\"quotes\"][0][\"symbol\"]\n    return company_code\n</code></pre>"}]}