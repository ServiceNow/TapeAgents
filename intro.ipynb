{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to TapeAgents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TapeAgents** is a framework to build, debug, serve and optimize your AI agent. It takes a holistic view of the agent lifecycle and aims to support you at all stages. The main distinguishing feature of the framework is that by design a **TapeAgent** creates its **Tape**: a compherensive semantic log of the agent's session that greatly facilitates audit, debugging, finetuning, agent optimization, etc.\n",
    "\n",
    "In this tutorial you will learn:\n",
    "- how to create TapeAgents using the low-level API\n",
    "- run and resume TapeAgents\n",
    "- have one TapeAgent reuse another TapeAgent's tape as training data\n",
    "\n",
    "In upcoming versions of this tutorial you will also learn: \n",
    "- how to make a team TapeAgent with subagents\n",
    "- how to build TapeAgents using available high-level APIs\n",
    "- how to build a TapeAgent that streams partial steps\n",
    "\n",
    "Other tutorials and examples will cover:\n",
    "- code execution and browser use\n",
    "- finetuning\n",
    "- the TapeAgents apps (Studio and Browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Install conda\n",
    "https://conda.io/projects/conda/en/latest/user-guide/install/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. All commands assume execution from the tapeagents directory.\n",
    "```bash\n",
    "cd tapeagents/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Create and activate conda environment with Python 3.10:\n",
    "```bash\n",
    "conda create -y -n tapeagents python=3.10\n",
    "conda activate tapeagents\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set Jupyter Notebook kernel to your newly created `tapeagents` conda environment\n",
    "https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_create-or-open-a-jupyter-notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Install tapeagents and its dependencies\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Set your LLM API keys and other minor things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\", # put your https://platform.openai.com/ key here\n",
    "# os.environ[\"OPENAI_ORGANIZATION\"] = \"\", # optional\n",
    "# os.environ[\"TOGETHER_API_KEY\"] = \"\" # put your https://together.ai/ key here\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "today = \"2024-09-17\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Your first TapeAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will build the simplest possible \"hello world\" agent. We will then go through all the new concepts that you need to know to understand the code. This section is quite long, but with the solid foundation you acquire here other TapeAgent tutorials will be easy to process.\n",
    "\n",
    "Without further ado, here's the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.agent import Agent, Node\n",
    "from tapeagents.core import Jump, Prompt\n",
    "from tapeagents.dialog import AssistantStep, Dialog, UserStep\n",
    "from tapeagents.llms import LLMStream, LiteLLM\n",
    "\n",
    "llm = LiteLLM(model_name=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def make_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    return Prompt(messages=tape.as_prompt_messages())\n",
    "\n",
    "\n",
    "def generate_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    yield AssistantStep(content=llm_stream.get_text())\n",
    "    yield Jump(next_node=0)\n",
    "\n",
    "\n",
    "flow = [Node().with_prompt(make_prompt).with_generate_steps(generate_steps)]\n",
    "agent = Agent[Dialog].create(llm, flow=flow)\n",
    "start_tape = Dialog() + [UserStep(content=\"Tell me about Vulcan in 3 sentences\")]\n",
    "final_tape = agent.run(start_tape).get_final_tape()\n",
    "print(final_tape.model_dump_json(indent=2))\n",
    "\n",
    "# Please ignore \"Models won't be available and only tokenizers, configuration\" warning from Huggingface Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn about tapes, steps, prompts, llm streams, nodes and agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tape\n",
    "\n",
    "The core-most concept in TapeAgents is the `Tape`, a comprehensive semantic level log of the agent's session. A `Tape` contains a context and a sequence of `Step` objects. As you can see, a TapeAgent runs by adding steps (such as `UserStep` or `AssistantStep`) to the _tape_. This example uses the `Dialog` tape, which is a basic tape for user-assistant conversations. Let's see what are the possible steps in a `Dialog` tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Python generics to instantiate many different Tape types by\n",
    "# specifying different Context and Step types. In the output of this cell,\n",
    "# look at Union[UserStep, AssistantStep, ...]\n",
    "# for the list of possible step types in the Dialog tape.\n",
    "Dialog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these steps should be familiar to you. `UserStep`, `AssistantStep`, `SystemStep` and `ToolResult` correspond to `role=user`, `role=assistant`, `role=system` and `role=tool` LLM API messages respectively. `ToolCalls` and `AssistantThought` correspond to assistant messages where the LLM requests a tool call or produces an intermediate thought that is not meant to be shown to the user. `Jump` and `Pass` are TapeAgent's internal step to control which node it should run at the next iteration (more on this below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt format; LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the industry-standard \"chat.completions\" prompt format in TapeAgents: a list of user/assistant/system/tool messages plus tool schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost all classes in TapeAgents are Pydantic base models.\n",
    "# This allows easy validation, serialization and instrospection. For example,\n",
    "# here we area able to list all the fields in the Prompt model.\n",
    "Prompt.model_fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLMs in TapeAgent take `Prompt` and return an `LLMStream` object. The `LLMStream` object can be used both to fast-forward to the complete response text and to stream partial outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.llms import LLAMA\n",
    "\n",
    "llama = LLAMA(\n",
    "    base_url=\"https://api.together.xyz\",\n",
    "    model_name=\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    # you must use stream=True if you wish to have message chunks in your LLM events\n",
    "    stream=True,\n",
    ")\n",
    "# Streaming\n",
    "prompt = Prompt(messages=[UserStep(content=\"Write hello world in Java\")])\n",
    "for event in llama.generate(prompt):\n",
    "    print(event.chunk, end=\"\")\n",
    "# No streaming\n",
    "# (note: you can not use Prompt object for more than 1 LLM call in TapeAgents)\n",
    "prompt = Prompt(messages=[UserStep(content=\"Write hello world in C\")])\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(llama.generate(prompt).get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we use the easiest way to create a prompt from the tapes: `tape.as_prompt_messages()`. Under the hood this method uses `step.llm_dict()` method of all non-control steps in the tape to create the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((user := UserStep(content=\"hi AI!\")).llm_dict())\n",
    "print((assistant := AssistantStep(content=\"hello human\")).llm_dict())\n",
    "print(Dialog(steps=[user, assistant]).as_prompt_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key priority in TapeAgents is making use of the data that running the agent generates. To make this possible, some TapeAgent LLMs know how to make their finetuning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import LLMMessage\n",
    "\n",
    "\n",
    "prompt = Prompt(messages=[UserStep(content=\"Say bla 3 times and foo 2 times\")])\n",
    "text = llama.make_training_text(\n",
    "    prompt=prompt, completion=LLMMessage(content=\"Write a Python program that says bla 3 times and foo 2 times\")\n",
    ")\n",
    "print(\"--- ALL TEXT ---\")\n",
    "print(text.text)\n",
    "print(\"--- PREDICTED CHARACTERS ---\")\n",
    "print(text.completion_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node represents an uninterruptible atom of TapeAgent's computation. When TapeAgents runs a node, it uses its two main functions: `make_prompt` and `generate_steps`. To build a node, you can replace the default implementations of these functions using `with_prompt` and `with_generate_steps` methods. Or you can subclass `Node` and override these functions. Hote that `generate_steps` must be a generator, a design choice we made to make TapeAgents a streaming-friendly framework. \n",
    "\n",
    "Let's see what the node from the above example can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.llms import LLMEvent\n",
    "\n",
    "\n",
    "def make_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    return Prompt(messages=tape.as_prompt_messages())\n",
    "\n",
    "\n",
    "def generate_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    yield AssistantStep(content=llm_stream.get_text())\n",
    "    yield Jump(next_node=0)\n",
    "\n",
    "\n",
    "node = Node().with_prompt(make_prompt).with_generate_steps(generate_steps)\n",
    "\n",
    "# Let's run \"make_prompt\" in isolation.\n",
    "print(\"\")\n",
    "print(node.make_prompt(agent=None, tape=Dialog(steps=[UserStep(content=\"Hi, AI!\")])))\n",
    "\n",
    "\n",
    "# Now, let's run \"generate_steps\" in isolation.\n",
    "# We need to construct a fake LLMStream to do that.\n",
    "def _generator():\n",
    "    yield LLMEvent(completion=LLMMessage(content=\"Hello, human!\"))\n",
    "\n",
    "\n",
    "stream = LLMStream(_generator(), Prompt())\n",
    "print(list(node.generate_steps(agent=None, tape=Dialog(), llm_stream=stream)))\n",
    "\n",
    "# When the agent runs a node, it is a equivalent to the following three steps:\n",
    "# Step 1: make a prompt\n",
    "start_tape = Dialog(steps=[UserStep(content=\"Hi, AI!\")])\n",
    "prompt = node.make_prompt(agent, start_tape)\n",
    "# Step 2: construct the LLMStream from the prompt (happens inside the agent)\n",
    "stream = llama.generate(prompt)\n",
    "# Step 3: generate steps that the agent will then add to the tape\n",
    "for step in node.generate_steps(agent, start_tape, stream):\n",
    "    print(step.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and its flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TapeAgent agent iteratively runs the nodes from its **flow** and appends the steps generated by each node to the tape. To select which next node to run, internally a TapeAgent computes the **tape view** object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.view import TapeViewStack\n",
    "\n",
    "# The \"top\" view in the tape view stack is the view of current agent. Initially `top.next_node` is 0\".\n",
    "tape1 = Dialog(steps=[UserStep(content=\"Hi, AI!\")])\n",
    "print(TapeViewStack.compute(tape1).top.next_node)\n",
    "# When the agent computes the view, it bumps up `top.next_node` every time it encounters a step with a new `prompt_id``.\n",
    "# The new prompt_id on the tape signals to the agent the current node has run.\n",
    "tape2 = Dialog(steps=[UserStep(content=\"Hi, AI!\"), AssistantStep(prompt_id=\"123\", content=\"AI here, how I can help?\")])\n",
    "print(TapeViewStack.compute(tape2).top.next_node)\n",
    "# The Jump step on the tape changes `top.next_node` to the value of the `next_node` field in the Jump step.\n",
    "tape3 = Dialog(\n",
    "    steps=[\n",
    "        UserStep(content=\"Hi, AI!\"),\n",
    "        AssistantStep(prompt_id=\"123\", content=\"AI here, how I can help?\"),\n",
    "        Jump(next_node=0),\n",
    "    ]\n",
    ")\n",
    "print(TapeViewStack.compute(tape3).top.next_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the agent stop after the last node has produced an `Action` step. The action steps are the step by which the agent requests information from the environment. For example, `AssistantStep` is an `Action` as it indicates the agent awaits the user response, `ToolCalls` is an action requesting tool call results. Let's look at all possible steps in `Dialog` tape and see which of them are actions, observations and thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import Action, Pass, Thought, Observation\n",
    "from tapeagents.dialog import AssistantThought, ToolCalls, ToolResult\n",
    "\n",
    "assert all([issubclass(step_class, Action) for step_class in [AssistantStep, ToolCalls]])\n",
    "assert all([issubclass(step_class, Thought) for step_class in [AssistantThought, Jump, Pass]])\n",
    "assert all([issubclass(step_class, Observation) for step_class in [UserStep, ToolResult]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to look at a simplified summary of the corner-stone `agent.run` algorithm.\n",
    "\n",
    "1. Compute the new tape view\n",
    "2. Choose the active agent (more on multi-agent TapeAgents later)\n",
    "3. Choose the active node\n",
    "4. Run the node and add steps on the tape\n",
    "5. If the last node yielded an action, then stop, else repeat.\n",
    "\n",
    "`agent.run` returns an `AgentStream` object which allows iterating through the agent's steps (or partial steps when streaming) and fast-forwardin to the complete new tape with `get_final_tape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converse with a TapeAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape_to_continue = final_tape + [UserStep(content=\"No, I mean Vulcan the company\")]\n",
    "continued_tape = agent.run(tape_to_continue).get_final_tape()\n",
    "print(continued_tape.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the agent is able to continue talking to you thanks for `Jump(next_node=0)` step that `generate_steps` produced. If you try to remove this step as an exercise, the agent will crash cause there is only node in its flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tape rendering\n",
    "\n",
    "LLM agents create a lot of data that can be overwhelming to process. In TapeAgents we render the tape with the associated prompts and completions into a more readable HTML for you. To make this work, we store prompts and completions in an SQLite database every time you call `agent.run()`.\n",
    "\n",
    "Here's how to use tape rendering in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.rendering import PrettyRenderer, render_tape_with_prompts\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(render_tape_with_prompts(continued_tape, PrettyRenderer()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Your TapeAgent with planning and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a TapeAgent that plans and acts. We will be using OpenAI function calling capabilities in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.core import Jump\n",
    "from tapeagents.dialog import SystemStep, AssistantThought, ToolCalls\n",
    "from tapeagents.environment import ToolEnvironment\n",
    "from tapeagents.runtime import main_loop\n",
    "from tapeagents.examples.intro_tools import get_stock_ticker, get_stock_data\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "You will help the user to learn about financials of companies.\n",
    "Use as many relevant tools as possible to include more details and facts in your responses.\n",
    "Today is {today}.\n",
    "\"\"\"\n",
    "system_message = SystemStep(content=system_instruction)\n",
    "\n",
    "env = ToolEnvironment([get_stock_ticker, get_stock_data])\n",
    "\n",
    "\n",
    "def make_planning_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    guidance = \"Write a natural language plan on how to use tools help the user. Output a list of numbered items, like 1., 2., 3., etc.\"\n",
    "    guidance_message = UserStep(content=guidance)\n",
    "    return Prompt(\n",
    "        messages=[system_message] + tape.as_prompt_messages() + [guidance_message], tools=env.get_tool_schema_dicts()\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_planning_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    if content := getattr(llm_stream.get_message(), \"content\", None):\n",
    "        yield AssistantThought(content=content)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "def make_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    guidance = \"Follow the plan you created to earlier. When you are done, respond to the user.\"\n",
    "    guidance_message = UserStep(content=guidance)\n",
    "    return Prompt(\n",
    "        messages=[system_message] + tape.as_prompt_messages() + [guidance_message], tools=env.get_tool_schema_dicts()\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    m = llm_stream.get_message()\n",
    "    if content := getattr(m, \"content\", None):\n",
    "        yield AssistantStep(content=content)\n",
    "        yield Jump(next_node=0)\n",
    "    elif tool_calls := getattr(m, \"tool_calls\", None):\n",
    "        yield ToolCalls(tool_calls=tool_calls)\n",
    "        yield Jump(next_node=1)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "agent1 = Agent.create(\n",
    "    LiteLLM(model_name=\"gpt-4o\", parameters={\"temperature\": 0.1}),\n",
    "    flow=[\n",
    "        Node().with_prompt(make_planning_prompt).with_generate_steps(generate_planning_steps),\n",
    "        Node().with_prompt(make_prompt).with_generate_steps(generate_steps),\n",
    "    ],\n",
    ")\n",
    "\n",
    "final_tape1 = None\n",
    "for event in main_loop(agent1, Dialog() + [UserStep(content=\"Tell me about Vulcan in 3 sentences\")], env):\n",
    "    if ae := event.agent_event:\n",
    "        if ae.step:\n",
    "            print(ae.step.model_dump_json(indent=2))\n",
    "        if ae.final_tape:\n",
    "            final_tape1 = ae.final_tape\n",
    "    if event.observation:\n",
    "        print(event.observation.model_dump_json(indent=2))\n",
    "assert final_tape1\n",
    "HTML(render_tape_with_prompts(final_tape1, PrettyRenderer()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main new thing in this example is the environment. In TapeAgents framework the environment responds to the agent `Action` steps with `Observation` steps. We expect you to use the environment to encapsulate tool use, retrieval, code execution: everything that is non-deterministic, non-stationary, or computationally heavy. On the contrary, we encourage you to implements the agent's deterministic decision-making in `make_prompt` and `generate_steps` methods.\n",
    "\n",
    "Here we use a pre-defined `main_loop` orchestrator to run the agent and the environment. `main_loop` is a generator of events that you can use as you wish. You are free to implement your own orchestration paradigm with a fine-grained control over what actions get to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Agent configuration, resumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a similar agent with an open-weights LLAMA3.1 70B models. Conveniently, [Together AI](together.ai) offers API endpoints. You can create an account and get API key with some free quota.\n",
    "\n",
    "We've found that LLAMA3 function-calling is not yet battle-ready. We will use the structured output approach to make it call tools instead. We are also making this agent trainable by adding `make_completion` methods to each node. `Node.make_completion` defines how a node can reconstruct the LLM completion message that would be required to make the steps from the given tape at the given index. You can think of `Node.make_completion` as the inverse of `Node.generate_steps`.\n",
    "\n",
    "When you run the code below, you might see a different behavior every time. Often the LLAMA-based agent gets stuck in a loop. We will look into how TapeAgents supports you in addressing this issue by\n",
    "- tuning the prompt and resuming the agent exactly where it got stuck \n",
    "- producing training text from a different agent's tape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tapeagents.core import Completion, Observation\n",
    "from tapeagents.llms import LLAMA\n",
    "from litellm.utils import ChatCompletionMessageToolCall\n",
    "from litellm.utils import Function\n",
    "\n",
    "env = ToolEnvironment([get_stock_ticker, get_stock_data])\n",
    "\n",
    "system_instruction = f\"\"\"\n",
    "You will help the user to learn about financials of companies.\n",
    "Use as many relevant tools as possible to include more details and facts in your responses.\n",
    "Today is {today}.\n",
    "\n",
    "You have access to the following tools: {env.get_tool_schema_dicts()}\"\"\"\n",
    "\n",
    "planning_guidance = \"Write a natural language plan on how to use tools help the user. Output a list of numbered items, like 1., 2., 3., etc.\"\n",
    "\n",
    "call_or_respond_guidance = \"\"\"\n",
    "Follow the plan you created earlier. When you are done, respond to the user.\n",
    "If you want to call a or several tools, output JSON like this\n",
    "{\"kind\": \"tool_call\", \"tool_name\": \"...\", \"parameters\": \"... unquoted parameters json ...\"}\n",
    "If you have called all the tools in the plan, respond to the user with the JSON of the form\n",
    "{\"kind\": \"response\", \"content\": \"... you response ... \"}.\n",
    "Output ONE JSON OBJECT ONLY PER LINE ONLY AND NOTHING ELSE.\n",
    "\"\"\"\n",
    "\n",
    "def make_planning_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    system_message = SystemStep(content=system_instruction)\n",
    "    guidance_message = UserStep(content=agent.templates[\"planning\"])\n",
    "    return Prompt(messages=[system_message] + tape.as_prompt_messages() + [guidance_message])\n",
    "\n",
    "\n",
    "def generate_planning_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    if content := getattr(llm_stream.get_message(), \"content\", None):\n",
    "        yield AssistantThought(content=content)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "def make_planning_completion(agent, tape: Dialog, index: int) -> Completion:\n",
    "    if not isinstance(current := tape[index], AssistantThought):\n",
    "        raise ValueError()\n",
    "    return Completion(role=\"assistant\", content=current.content)\n",
    "\n",
    "\n",
    "def _llm_message_content(step: AssistantStep | ToolCalls):\n",
    "    \"\"\"Helper function to make both the prompt and the target completion\"\"\"\n",
    "    match step:\n",
    "        case AssistantStep():\n",
    "            return json.dumps({\"kind\": \"response\", \"content\": step.content})\n",
    "        case ToolCalls():\n",
    "            content = \"\"\n",
    "            for tool_call in step.tool_calls:\n",
    "                if content:\n",
    "                    content += \"\\n\"\n",
    "                content += json.dumps(\n",
    "                    {\n",
    "                        \"kind\": \"tool_call\",\n",
    "                        \"tool_name\": tool_call.function.name,\n",
    "                        \"parameters\": json.loads(tool_call.function.arguments),\n",
    "                    }\n",
    "                )\n",
    "            return content\n",
    "        case _:\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "def make_prompt(agent, tape: Dialog) -> Prompt:\n",
    "    system_message = SystemStep(content=system_instruction)\n",
    "    guidance_message = UserStep(content=agent.templates[\"call_or_respond\"])\n",
    "    messages = [system_message]\n",
    "    for step in tape:\n",
    "        if isinstance(step, (ToolCalls)):\n",
    "            messages.append(AssistantStep(content=_llm_message_content(step)))\n",
    "        elif not isinstance(step, Jump):\n",
    "            messages.append(tape.step_to_message(step))\n",
    "    messages += [guidance_message]\n",
    "    return Prompt(messages=messages)\n",
    "\n",
    "\n",
    "def generate_steps(agent, dialog, llm_stream: LLMStream):\n",
    "    m = llm_stream.get_message()\n",
    "    try:\n",
    "        assert m.content\n",
    "        tool_calls = []\n",
    "        response = None\n",
    "        for line in m.content.split(\"\\n\"):\n",
    "            data = json.loads(line)\n",
    "            if data.get(\"kind\") == \"response\":\n",
    "                response = data[\"content\"]\n",
    "            elif data.get(\"kind\") == \"tool_call\":\n",
    "                tool_call = ChatCompletionMessageToolCall(\n",
    "                    function=Function(name=data[\"tool_name\"], arguments=json.dumps(data[\"parameters\"])),\n",
    "                    # tool call must be a unique string, it helps to make it something deterministic\n",
    "                    id=f\"tool_call_{len(tool_calls)}_node_starts_at_{len(dialog)}\",\n",
    "                )\n",
    "                tool_calls.append(tool_call)\n",
    "            else:\n",
    "                yield AssistantStep(content=\"Invalid LLM completion: kind field must be 'response' or 'tool_call'\")\n",
    "        if response and tool_calls:\n",
    "            yield AssistantStep(content=\"Invalid LLM completion: response and tool_call cannot be in the same message\")\n",
    "        if response:\n",
    "            yield AssistantStep(content=response)\n",
    "            yield Jump(next_node=0)\n",
    "        if tool_calls:\n",
    "            yield ToolCalls(tool_calls=tool_calls)\n",
    "            yield Jump(next_node=1)\n",
    "    except Exception as e:\n",
    "        yield AssistantStep(content=\"Invalid JSON object: \" + str(e))\n",
    "\n",
    "\n",
    "def make_completion(agent, dialog: Dialog, index: int) -> Completion:\n",
    "    if not isinstance(step := dialog[index], AssistantStep | ToolCalls):\n",
    "        raise ValueError()\n",
    "    content = _llm_message_content(step)\n",
    "    return Completion(role=\"assistant\", content=content)\n",
    "\n",
    "\n",
    "agent2 = Agent.create(\n",
    "    LLAMA(\n",
    "        base_url=\"https://api.together.xyz\",\n",
    "        model_name=\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
    "        tokenizer_name=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        parameters=dict(temperature=0.01),\n",
    "    ),\n",
    "    templates={\n",
    "        \"system\": system_instruction,\n",
    "        \"planning\": planning_guidance,\n",
    "        \"call_or_respond\": call_or_respond_guidance,\n",
    "    },\n",
    "    flow=[\n",
    "        Node()\n",
    "        .with_prompt(make_planning_prompt)\n",
    "        .with_generate_steps(generate_planning_steps)\n",
    "        .with_completion(make_planning_completion),\n",
    "        Node().with_prompt(make_prompt).with_generate_steps(generate_steps).with_completion(make_completion),\n",
    "    ],\n",
    ")\n",
    "\n",
    "final_tape2 = None\n",
    "for event in main_loop(\n",
    "    agent2, Dialog() + [UserStep(content=\"Tell me about Vulcan Materials in 3 sentences\")], env, max_loops=3\n",
    "):\n",
    "    if ae := event.agent_event:\n",
    "        if ae.step:\n",
    "            print(ae.step.model_dump_json(indent=2))\n",
    "        if ae.final_tape:\n",
    "            final_tape2 = ae.final_tape\n",
    "    if event.observation:\n",
    "        print(event.observation.model_dump_json(indent=2))\n",
    "assert final_tape2\n",
    "HTML(render_tape_with_prompts(final_tape2, PrettyRenderer()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you're likely seeing that the LLAMA-based agent is having trouble. Let's try to help it. For reproducibility, we'll use a pre-recorded failed tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"assets/failed_tape.json\") as src:\n",
    "    failed_tape = Dialog.model_validate(json.load(src))\n",
    "agent2b = agent2.model_copy(deep=True)\n",
    "agent2b.templates[\"call_or_respond\"] += (\n",
    "    \"REMEMBER: check what tool calls you have already made. Do not do the same call again!\"\n",
    ")\n",
    "resume_from_step8 = agent2b.run(failed_tape[:8]).get_final_tape()\n",
    "HTML(render_tape_with_prompts(resume_from_step8, PrettyRenderer()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this helpful hint often gets LLAMA-based agent unstuck. Note how easy it was to test it thanks to the ability of the agent to resume from step 8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tape reuse and training data\n",
    "\n",
    "Another way to help this agent (or one with an even smaller LLM) is to finetune the LLM. And the most important step towards finetuning is making the training data!\n",
    "\n",
    "There are two ways to make training data in TapeAgents:\n",
    "- the basic one: use the `LLMCall` structures that the agent created when it produced the tape. You can retrieve them from the SQLite storage and convert into training text.\n",
    "- the much powerful one: call `agent.reuse` to reconstructed the prompts and completions **and** to validate that with the reconstructed LLMCalls the agent would indeed create the given tape\n",
    "\n",
    "The big advantage of the second approach is that it allows you to use the tape from a teacher agent (think slower and more expensive) to train a student agent (think faster and cheaper). Or to train an agent on its own revised tapes.\n",
    "\n",
    "Of course, restrictions apply: the tape by agent A may not be reusable by agent B directly. You might have to add/remove some steps. But at least `agent.reuse` verifies if your tape modifications led to creation of a tape that the agent B can indeed produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training data from the past LLM calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tapeagents.observe import retrieve_tape_llm_calls\n",
    "\n",
    "llm_calls = retrieve_tape_llm_calls(final_tape2)\n",
    "print(f\"Retrieved {len(llm_calls)} LLM calls from the tape.\")\n",
    "# under the hood agent2 will route this request to its llm\n",
    "example_text = agent2.make_training_text(list(llm_calls.values())[0])\n",
    "print(f\"From the first retrieved LLM call the LLM will be trained to predict this text:\")\n",
    "print(\"---\")\n",
    "print(example_text.completion_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training data by reusing a tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `agent2` reuses the tape by `agent1`, even though they have very different prompt and output formats! \n",
    "\n",
    "You can inspect the reused tape below and see that the steps are the same as before, but the prompts and completions are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reused_tape, _ = agent2.reuse(final_tape1)\n",
    "HTML(render_tape_with_prompts(reused_tape, PrettyRenderer()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer a quick way to harness the tape reuse to make training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = agent2.make_training_data(final_tape1)\n",
    "print(training_data[0].completion_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be simpler than that?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tapes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
