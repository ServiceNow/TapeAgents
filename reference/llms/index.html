
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="TapeAgents is a framework that facilitates all stages of the LLM Agent development lifecycle">
      
      
      
      
        <link rel="prev" href="../llm_function/">
      
      
        <link rel="next" href="../nodes/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.46">
    
    
      
        <title>LLMs - TapeAgents Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tapeagents.llms" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="TapeAgents Documentation" class="md-header__button md-logo" aria-label="TapeAgents Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            TapeAgents Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLMs
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ServiceNow/TapeAgents" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ServiceNow/TapeAgents
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="TapeAgents Documentation" class="md-nav__button md-logo" aria-label="TapeAgents Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    TapeAgents Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ServiceNow/TapeAgents" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    ServiceNow/TapeAgents
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quickstart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../batch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Batch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../core/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Core
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../demo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dialog_tape/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dialog Tape
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../environment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Environment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../finetune/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Finetune
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            Finetune
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/checkpoints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Checkpoints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/context/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Context
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/eval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Eval
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/finetune/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Finetune
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/logging_/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logging
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/lora/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lora
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/optim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optim
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../finetune/rl/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    RL
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_8_9" id="__nav_3_8_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_8_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8_9">
            <span class="md-nav__icon md-icon"></span>
            RL
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/rl/utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Types
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IO
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llm_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LLMs
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LLMs
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-module"></code>&nbsp;llms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CachedLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CachedLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM.reindex_log" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;reindex_log
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.log_output" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMEvent" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLMEvent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLMStream
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LLMStream">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream.get_output" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream.get_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LiteLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LiteLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.MockLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MockLLM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ReplayLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ReplayLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.from_llm" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;from_llm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TrainableLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" TrainableLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs_chat_complete" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs_chat_complete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs_complete" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs_complete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.load_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;load_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.closest_prompt" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;closest_prompt
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.trainable_llm_make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;trainable_llm_make_training_text
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nodes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Nodes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../observe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Observe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../orchestrator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Orchestrator
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../parallel_processing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel Processing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../prompting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompting
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_17" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../renderers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Renderers
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_17" id="__nav_3_17_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_17">
            <span class="md-nav__icon md-icon"></span>
            Renderers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../renderers/basic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../renderers/camera_ready_renderer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Camera Ready Renderer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../renderers/pretty/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pretty
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../studio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Studio
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tape_browser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tape Browser
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../team/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Team
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_21" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../tools/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Tools
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_21" id="__nav_3_21_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_21">
            <span class="md-nav__icon md-icon"></span>
            Tools
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/calculator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Calculator
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/container_executor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Container Executor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/document_converters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Document Converters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/gym_browser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gym Browser
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/python_interpreter/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Interpreter
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/simple_browser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Browser
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tools/stock/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stock
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../view/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    View
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-module"></code>&nbsp;llms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CachedLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CachedLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.CachedLLM.reindex_log" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;reindex_log
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.log_output" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMEvent" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLMEvent
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LLMStream
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LLMStream">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream.get_output" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LLMStream.get_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LiteLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LiteLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.LiteLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.MockLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;MockLLM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ReplayLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ReplayLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.from_llm" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;from_llm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.generate" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;generate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.ReplayLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TrainableLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" TrainableLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.count_tokens" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;count_tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs_chat_complete" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs_chat_complete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.get_log_probs_complete" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;get_log_probs_complete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.load_tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;load_tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tapeagents.llms.TrainableLLM.make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;make_training_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.closest_prompt" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;closest_prompt
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tapeagents.llms.trainable_llm_make_training_text" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-function"></code>&nbsp;trainable_llm_make_training_text
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>LLMs</h1>

<div class="doc doc-object doc-module">



<a id="tapeagents.llms"></a>
    <div class="doc doc-contents first">

        <p>Wrapper for interacting with external and hosted large language models (LLMs).</p>







<p><span class="doc-section-title">Classes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.CachedLLM" href="#tapeagents.llms.CachedLLM">CachedLLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM" href="#tapeagents.llms.LLM">LLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>An abstract base class representing a Language Learning Model (LLM).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMEvent" href="#tapeagents.llms.LLMEvent">LLMEvent</a></code></b>
          –
          <div class="doc-md-description">
            <p>An event class representing either a chunk of LLM output or the final LLM output.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream" href="#tapeagents.llms.LLMStream">LLMStream</a></code></b>
          –
          <div class="doc-md-description">
            <p>A wrapper class for LLM generators that provides convenient iteration and output extraction.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LiteLLM" href="#tapeagents.llms.LiteLLM">LiteLLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>A LiteLLM implementation of the LLM interface.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.MockLLM" href="#tapeagents.llms.MockLLM">MockLLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>A mock LLM implementation for testing purposes.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM" href="#tapeagents.llms.ReplayLLM">ReplayLLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>Specialized LLM class that replays previously recorded LLM interactions.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM" href="#tapeagents.llms.TrainableLLM">TrainableLLM</a></code></b>
          –
          <div class="doc-md-description">
            <p>Class for interacting with trainable language models through OpenAI-compatible API endpoints.</p>
          </div>
        </li>
    </ul>




<p><span class="doc-section-title">Functions:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.closest_prompt" href="#tapeagents.llms.closest_prompt">closest_prompt</a></code></b>
            –
            <div class="doc-md-description">
              <p>Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.trainable_llm_make_training_text" href="#tapeagents.llms.trainable_llm_make_training_text">trainable_llm_make_training_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generates training text for LLM fine-tuning by combining prompt and output using tokenizer's chat template.</p>
            </div>
          </li>
    </ul>





  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.CachedLLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>CachedLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM" href="#tapeagents.llms.LLM">LLM</a></code></p>


        <p>A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.</p>
<p>This class implements caching functionality for LLM responses to avoid redundant API calls
and enable replay of previous interactions. It supports both file-based caching and SQLite-based
replay functionality for testing purposes.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.CachedLLM.use_cache">use_cache</span></code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Flag to enable/disable caching functionality. Defaults to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.CachedLLM.stream">stream</span></code></b>
              (<code>bool</code>)
          –
          <div class="doc-md-description">
            <p>Flag to enable/disable streaming responses. Defaults to False.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.CachedLLM._cache">_cache</span></code></b>
              (<code>dict</code>)
          –
          <div class="doc-md-description">
            <p>Internal cache storage mapping prompt keys to LLM responses.</p>
          </div>
        </li>
    </ul>
        <p>The cache can be initialized in two modes:
1. SQLite replay mode: Used for testing, enforces cache hits only
2. File-based cache mode: Stores responses in a jsonl file for persistence</p>
<p>Cache keys are generated based on the prompt content, excluding the prompt ID.
During testing (replay mode), exact text matching is used instead of hashing.</p>









<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.CachedLLM.generate" href="#tapeagents.llms.CachedLLM.generate">generate</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generate a response stream from the language model based on the given prompt.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.CachedLLM.reindex_log" href="#tapeagents.llms.CachedLLM.reindex_log">reindex_log</a></code></b>
            –
            <div class="doc-md-description">
              <p>Reindex the log data into cache.</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CachedLLM</span><span class="p">(</span><span class="n">LLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A caching wrapper for LLM implementations that stores and retrieves previous LLM responses.</span>

<span class="sd">    This class implements caching functionality for LLM responses to avoid redundant API calls</span>
<span class="sd">    and enable replay of previous interactions. It supports both file-based caching and SQLite-based</span>
<span class="sd">    replay functionality for testing purposes.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        use_cache (bool): Flag to enable/disable caching functionality. Defaults to False.</span>
<span class="sd">        stream (bool): Flag to enable/disable streaming responses. Defaults to False.</span>
<span class="sd">        _cache (dict): Internal cache storage mapping prompt keys to LLM responses.</span>

<span class="sd">    The cache can be initialized in two modes:</span>
<span class="sd">    1. SQLite replay mode: Used for testing, enforces cache hits only</span>
<span class="sd">    2. File-based cache mode: Stores responses in a jsonl file for persistence</span>

<span class="sd">    Cache keys are generated based on the prompt content, excluding the prompt ID.</span>
<span class="sd">    During testing (replay mode), exact text matching is used instead of hashing.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">_cache</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">model_post_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">__content</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_REPLAY_SQLITE</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">llm_calls</span> <span class="o">=</span> <span class="n">retrieve_all_llm_calls</span><span class="p">(</span><span class="n">_REPLAY_SQLITE</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">llm_call</span> <span class="ow">in</span> <span class="n">llm_calls</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt_key</span><span class="p">(</span><span class="n">llm_call</span><span class="o">.</span><span class="n">prompt</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">llm_call</span><span class="o">.</span><span class="n">output</span><span class="p">)]</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Enforced LLM cache from </span><span class="si">{</span><span class="n">_REPLAY_SQLITE</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span><span class="si">}</span><span class="s2"> entries&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Use LLM Cache&quot;</span><span class="p">)</span>
        <span class="n">param_hash</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">&quot;token&quot;</span><span class="p">}))</span>
        <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;__&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;llm_cache_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">param_hash</span><span class="si">}</span><span class="s2">.jsonl&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache_file</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">key</span><span class="p">,</span> <span class="n">event_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">event_dict</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded cache with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">)</span><span class="si">}</span><span class="s2"> keys&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Cache file not found&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reindex_log</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Reindex the log data into cache.</span>

<span class="sd">        This method iterates through the log entries, validates each prompt and output,</span>
<span class="sd">        and adds them to the cache using the prompt key as index. Each entry is converted</span>
<span class="sd">        to an LLMEvent model before caching.</span>

<span class="sd">        Side Effects:</span>
<span class="sd">            - Updates the internal cache with log data</span>
<span class="sd">            - Logs the total number of reindexed entries at INFO level</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">log_data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt_key</span><span class="p">(</span><span class="n">Prompt</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">log_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_to_cache</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">LLMOutput</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">log_data</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
            <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reindexed </span><span class="si">{</span><span class="n">cnt</span><span class="si">}</span><span class="s2"> log entries&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_add_to_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">event_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">event_dict</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache_file</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">((</span><span class="n">key</span><span class="p">,</span> <span class="n">event_dict</span><span class="p">),</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_prompt_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">}),</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_key</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_REPLAY_SQLITE</span><span class="p">:</span>
            <span class="c1"># use exact text as a key during testing</span>
            <span class="k">return</span> <span class="n">text</span>
        <span class="k">return</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate a response stream from the language model based on the given prompt.</span>

<span class="sd">        This method handles both cached and new responses, implementing a caching mechanism</span>
<span class="sd">        for LLM responses to avoid redundant API calls.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt object containing messages to send to the LLM.</span>
<span class="sd">            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            LLMStream: A stream of LLM events containing the model&#39;s response.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).</span>

<span class="sd">        Notes:</span>
<span class="sd">            - If caching is enabled and the prompt exists in cache, returns cached response</span>
<span class="sd">            - If generating new response, tokens are counted and added to total token count</span>
<span class="sd">            - All generated events are cached for future use if caching is enabled</span>
<span class="sd">            - Output is logged through the logging system</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_implementation</span><span class="p">():</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt_key</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;llm cache hit, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">}</span><span class="s2"> events&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">event_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                    <span class="n">event</span> <span class="o">=</span> <span class="n">LLMEvent</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">event_dict</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">log_output</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">yield</span> <span class="n">event</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_REPLAY_SQLITE</span><span class="p">:</span>
                    <span class="n">closest</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">closest_prompt</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;llm cache miss, closest in cache has score </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s2">DIFF:</span><span class="se">\n</span><span class="si">{</span><span class="n">diff_strings</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">closest</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;llm cache miss not allowed, prompt: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">token_count</span> <span class="o">+=</span> <span class="n">toks</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">toks</span><span class="si">}</span><span class="s2"> prompt tokens, total: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">token_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_add_to_cache</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
                    <span class="c1"># note: the underlying LLM will log the output</span>
                    <span class="k">yield</span> <span class="n">event</span>

        <span class="k">return</span> <span class="n">LLMStream</span><span class="p">(</span><span class="n">_implementation</span><span class="p">(),</span> <span class="n">prompt</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">LLMEvent</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.CachedLLM.generate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Generate a response stream from the language model based on the given prompt.</p>
<p>This method handles both cached and new responses, implementing a caching mechanism
for LLM responses to avoid redundant API calls.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt object containing messages to send to the LLM.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>**kwargs</code></b>
              (<code>dict</code>, default:
                  <code>{}</code>
)
          –
          <div class="doc-md-description">
            <p>Additional arguments to pass to the underlying LLM implementation.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>LLMStream</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream" href="#tapeagents.llms.LLMStream">LLMStream</a></code>
)          –
          <div class="doc-md-description">
            <p>A stream of LLM events containing the model's response.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>ValueError</code>
            –
          <div class="doc-md-description">
            <p>If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).</p>
          </div>
        </li>
    </ul>


<details class="notes" open>
  <summary>Notes</summary>
  <ul>
<li>If caching is enabled and the prompt exists in cache, returns cached response</li>
<li>If generating new response, tokens are counted and added to total token count</li>
<li>All generated events are cached for future use if caching is enabled</li>
<li>Output is logged through the logging system</li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate a response stream from the language model based on the given prompt.</span>

<span class="sd">    This method handles both cached and new responses, implementing a caching mechanism</span>
<span class="sd">    for LLM responses to avoid redundant API calls.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt object containing messages to send to the LLM.</span>
<span class="sd">        **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        LLMStream: A stream of LLM events containing the model&#39;s response.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).</span>

<span class="sd">    Notes:</span>
<span class="sd">        - If caching is enabled and the prompt exists in cache, returns cached response</span>
<span class="sd">        - If generating new response, tokens are counted and added to total token count</span>
<span class="sd">        - All generated events are cached for future use if caching is enabled</span>
<span class="sd">        - Output is logged through the logging system</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_implementation</span><span class="p">():</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt_key</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;llm cache hit, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">])</span><span class="si">}</span><span class="s2"> events&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">event_dict</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                <span class="n">event</span> <span class="o">=</span> <span class="n">LLMEvent</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">event_dict</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">log_output</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">event</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_REPLAY_SQLITE</span><span class="p">:</span>
                <span class="n">closest</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">closest_prompt</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;llm cache miss, closest in cache has score </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s2">DIFF:</span><span class="se">\n</span><span class="si">{</span><span class="n">diff_strings</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">closest</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;llm cache miss not allowed, prompt: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">token_count</span> <span class="o">+=</span> <span class="n">toks</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">toks</span><span class="si">}</span><span class="s2"> prompt tokens, total: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">token_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_add_to_cache</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">event</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
                <span class="c1"># note: the underlying LLM will log the output</span>
                <span class="k">yield</span> <span class="n">event</span>

    <span class="k">return</span> <span class="n">LLMStream</span><span class="p">(</span><span class="n">_implementation</span><span class="p">(),</span> <span class="n">prompt</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.CachedLLM.reindex_log" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">reindex_log</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Reindex the log data into cache.</p>
<p>This method iterates through the log entries, validates each prompt and output,
and adds them to the cache using the prompt key as index. Each entry is converted
to an LLMEvent model before caching.</p>


<details class="side-effects" open>
  <summary>Side Effects</summary>
  <ul>
<li>Updates the internal cache with log data</li>
<li>Logs the total number of reindexed entries at INFO level</li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reindex_log</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reindex the log data into cache.</span>

<span class="sd">    This method iterates through the log entries, validates each prompt and output,</span>
<span class="sd">    and adds them to the cache using the prompt key as index. Each entry is converted</span>
<span class="sd">    to an LLMEvent model before caching.</span>

<span class="sd">    Side Effects:</span>
<span class="sd">        - Updates the internal cache with log data</span>
<span class="sd">        - Logs the total number of reindexed entries at INFO level</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">log_data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prompt_key</span><span class="p">(</span><span class="n">Prompt</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">log_data</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_add_to_cache</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">LLMOutput</span><span class="o">.</span><span class="n">model_validate</span><span class="p">(</span><span class="n">log_data</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reindexed </span><span class="si">{</span><span class="n">cnt</span><span class="si">}</span><span class="s2"> log entries&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.LLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pydantic.BaseModel">BaseModel</span></code>, <code><span title="abc.ABC">ABC</span></code></p>


        <p>An abstract base class representing a Language Learning Model (LLM).</p>
<p>This class defines the interface for interacting with different LLM implementations.
It handles basic LLM functionality like token counting, generation, and logging.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.model_name">model_name</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>Name of the LLM model</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.parameters">parameters</span></code></b>
              (<code>dict</code>)
          –
          <div class="doc-md-description">
            <p>Model-specific parameters for generation</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.context_size">context_size</span></code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Maximum context size in tokens (default: 32000)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.tokenizer_name">tokenizer_name</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>Name of the tokenizer used</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.tokenizer">tokenizer</span></code></b>
              (<code><span title="typing.Any">Any</span></code>)
          –
          <div class="doc-md-description">
            <p>Tokenizer instance</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM.token_count">token_count</span></code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Running count of tokens processed</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLM._log">_log</span></code></b>
              (<code>list</code>)
          –
          <div class="doc-md-description">
            <p>Internal log of LLM calls</p>
          </div>
        </li>
    </ul>


<details class="note" open>
  <summary>Note</summary>
  <p>This is an abstract class and requires implementation of the abstract methods
in derived classes.</p>
</details>








<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM.count_tokens" href="#tapeagents.llms.LLM.count_tokens">count_tokens</a></code></b>
            –
            <div class="doc-md-description">
              <p>Count tokens in messages or text</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM.generate" href="#tapeagents.llms.LLM.generate">generate</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generate text from a given prompt</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM.log_output" href="#tapeagents.llms.LLM.log_output">log_output</a></code></b>
            –
            <div class="doc-md-description">
              <p>Logs the output of an LLM (Language Model) call along with its metadata.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM.make_training_text" href="#tapeagents.llms.LLM.make_training_text">make_training_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Create training text from prompt and output.</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An abstract base class representing a Language Learning Model (LLM).</span>

<span class="sd">    This class defines the interface for interacting with different LLM implementations.</span>
<span class="sd">    It handles basic LLM functionality like token counting, generation, and logging.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        model_name (str): Name of the LLM model</span>
<span class="sd">        parameters (dict): Model-specific parameters for generation</span>
<span class="sd">        context_size (int): Maximum context size in tokens (default: 32000)</span>
<span class="sd">        tokenizer_name (str): Name of the tokenizer used</span>
<span class="sd">        tokenizer (Any): Tokenizer instance</span>
<span class="sd">        token_count (int): Running count of tokens processed</span>
<span class="sd">        _log (list): Internal log of LLM calls</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an abstract class and requires implementation of the abstract methods</span>
<span class="sd">        in derived classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">context_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32000</span>
    <span class="n">tokenizer_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">token_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">_log</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate text from a given prompt</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt object containing messages to send to the LLM.</span>
<span class="sd">            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.</span>

<span class="sd">        Returns:</span>
<span class="sd">            LLMStream: A stream of LLM events containing the model&#39;s response.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count tokens in messages or text</span>

<span class="sd">        Args:</span>
<span class="sd">            messages (Union[List[Dict], str]): List of messages or text to count tokens in</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: Number of tokens in the messages or text</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create training text from prompt and output.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt object containing messages used to generate the output.</span>
<span class="sd">            output (LLMOutput): The output generated by the LLM.</span>

<span class="sd">        Returns:</span>
<span class="sd">            TrainingText: The training text object containing the prompt and output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">log_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">,</span> <span class="n">cached</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Logs the output of an LLM (Language Model) call along with its metadata.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt object containing the input messages for the LLM.</span>
<span class="sd">            message (LLMOutput): The output message generated by the LLM.</span>
<span class="sd">            cached (bool, optional): Indicates whether the output was retrieved from cache. Defaults to False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">llm_call</span> <span class="o">=</span> <span class="n">LLMCall</span><span class="p">(</span>
            <span class="n">timestamp</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">output</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">prompt_length_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">),</span>
            <span class="n">output_length_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="k">if</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">llm_call</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
        <span class="n">observe_llm_call</span><span class="p">(</span><span class="n">llm_call</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLM.count_tokens" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">count_tokens</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Count tokens in messages or text</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>messages</code></b>
              (<code>Union[List[Dict], str]</code>)
          –
          <div class="doc-md-description">
            <p>List of messages or text to count tokens in</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>int</code></b> (              <code>int</code>
)          –
          <div class="doc-md-description">
            <p>Number of tokens in the messages or text</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Count tokens in messages or text</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (Union[List[Dict], str]): List of messages or text to count tokens in</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Number of tokens in the messages or text</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLM.generate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Generate text from a given prompt</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt object containing messages to send to the LLM.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>**kwargs</code></b>
              (<code>dict</code>, default:
                  <code>{}</code>
)
          –
          <div class="doc-md-description">
            <p>Additional arguments to pass to the underlying LLM implementation.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>LLMStream</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream" href="#tapeagents.llms.LLMStream">LLMStream</a></code>
)          –
          <div class="doc-md-description">
            <p>A stream of LLM events containing the model's response.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate text from a given prompt</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt object containing messages to send to the LLM.</span>
<span class="sd">        **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        LLMStream: A stream of LLM events containing the model&#39;s response.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLM.log_output" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">log_output</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">message</span><span class="p">,</span> <span class="n">cached</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Logs the output of an LLM (Language Model) call along with its metadata.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt object containing the input messages for the LLM.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>message</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The output message generated by the LLM.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>cached</code></b>
              (<code>bool</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Indicates whether the output was retrieved from cache. Defaults to False.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">message</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">,</span> <span class="n">cached</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logs the output of an LLM (Language Model) call along with its metadata.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt object containing the input messages for the LLM.</span>
<span class="sd">        message (LLMOutput): The output message generated by the LLM.</span>
<span class="sd">        cached (bool, optional): Indicates whether the output was retrieved from cache. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">llm_call</span> <span class="o">=</span> <span class="n">LLMCall</span><span class="p">(</span>
        <span class="n">timestamp</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">output</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">prompt_length_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">),</span>
        <span class="n">output_length_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="k">if</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">cached</span><span class="o">=</span><span class="n">cached</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">llm_call</span><span class="o">.</span><span class="n">model_dump</span><span class="p">())</span>
    <span class="n">observe_llm_call</span><span class="p">(</span><span class="n">llm_call</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLM.make_training_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Create training text from prompt and output.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt object containing messages used to generate the output.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The output generated by the LLM.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>TrainingText</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.core.TrainingText" href="../core/#tapeagents.core.TrainingText">TrainingText</a></code>
)          –
          <div class="doc-md-description">
            <p>The training text object containing the prompt and output.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create training text from prompt and output.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt object containing messages used to generate the output.</span>
<span class="sd">        output (LLMOutput): The output generated by the LLM.</span>

<span class="sd">    Returns:</span>
<span class="sd">        TrainingText: The training text object containing the prompt and output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.LLMEvent" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LLMEvent</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pydantic.BaseModel">BaseModel</span></code></p>


        <p>An event class representing either a chunk of LLM output or the final LLM output.</p>
<p>This class encapsulates events that occur during LLM processing, handling both
intermediate chunks of output and the final complete output.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLMEvent.chunk">chunk</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>A partial text output from the LLM stream. None if this
event represents a complete output.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLMEvent.output">output</span></code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The complete output from the LLM. None if this
event represents a partial chunk.</p>
          </div>
        </li>
    </ul>










              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LLMEvent</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An event class representing either a chunk of LLM output or the final LLM output.</span>

<span class="sd">    This class encapsulates events that occur during LLM processing, handling both</span>
<span class="sd">    intermediate chunks of output and the final complete output.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        chunk (str, optional): A partial text output from the LLM stream. None if this</span>
<span class="sd">            event represents a complete output.</span>
<span class="sd">        output (LLMOutput, optional): The complete output from the LLM. None if this</span>
<span class="sd">            event represents a partial chunk.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">chunk</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.LLMStream" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LLMStream</code>


</h2>


    <div class="doc doc-contents ">


        <p>A wrapper class for LLM generators that provides convenient iteration and output extraction.</p>
<p>This class wraps a generator that yields LLMEvents and provides methods to:</p>
<ul>
<li>Iterate through events</li>
<li>Extract complete LLM output</li>
<li>Get the assistant's response text</li>
</ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLMStream.generator">generator</span></code></b>
          –
          <div class="doc-md-description">
            <p>Generator yielding LLMEvents or None if empty</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.LLMStream.prompt">prompt</span></code></b>
          –
          <div class="doc-md-description">
            <p>The prompt used to generate the LLM response:</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>ValueError</code>
            –
          <div class="doc-md-description">
            <p>When trying to iterate null stream, when no output is produced,
       or when output is not an assistant message with content</p>
          </div>
        </li>
    </ul>









<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream.get_output" href="#tapeagents.llms.LLMStream.get_output">get_output</a></code></b>
            –
            <div class="doc-md-description">
              <p>Returns first LLMOutput found in events</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream.get_text" href="#tapeagents.llms.LLMStream.get_text">get_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Returns content of first assistant message found</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LLMStream</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A wrapper class for LLM generators that provides convenient iteration and output extraction.</span>

<span class="sd">    This class wraps a generator that yields LLMEvents and provides methods to:</span>

<span class="sd">    - Iterate through events</span>
<span class="sd">    - Extract complete LLM output</span>
<span class="sd">    - Get the assistant&#39;s response text</span>

<span class="sd">    Attributes:</span>
<span class="sd">        generator: Generator yielding LLMEvents or None if empty</span>
<span class="sd">        prompt: The prompt used to generate the LLM response:</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: When trying to iterate null stream, when no output is produced,</span>
<span class="sd">                   or when output is not an assistant message with content</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">:</span> <span class="n">Generator</span><span class="p">[</span><span class="n">LLMEvent</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;can&#39;t iterate a null stream&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMEvent</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMOutput</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns first LLMOutput found in events&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;LLM did not produce an output&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns content of first assistant message found&quot;&quot;&quot;</span>
        <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">o</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;assistant&quot;</span> <span class="ow">or</span> <span class="n">o</span><span class="o">.</span><span class="n">content</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;LLM did not produce an assistant message&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">o</span><span class="o">.</span><span class="n">content</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLMStream.get_output" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">get_output</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Returns first LLMOutput found in events</p>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMOutput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns first LLMOutput found in events&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">event</span><span class="o">.</span><span class="n">output</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;LLM did not produce an output&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LLMStream.get_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">get_text</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Returns content of first assistant message found</p>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns content of first assistant message found&quot;&quot;&quot;</span>
    <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_output</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">o</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;assistant&quot;</span> <span class="ow">or</span> <span class="n">o</span><span class="o">.</span><span class="n">content</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;LLM did not produce an assistant message&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">o</span><span class="o">.</span><span class="n">content</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.LiteLLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>LiteLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="tapeagents.llms.CachedLLM" href="#tapeagents.llms.CachedLLM">CachedLLM</a></code></p>


        <p>A LiteLLM implementation of the LLM interface.</p>
<p>This class provides integration with the LiteLLM library for making LLM API calls.
It supports both streaming and non-streaming responses, token counting, and handles API timeouts with retries.
Streaming responses are handled by yielding chunks of text as they arrive.
Non-streaming responses return complete messages.</p>


<details class="note" open>
  <summary>Note</summary>
  <p>Function calling during streaming is not yet implemented and will raise NotImplementedError.</p>
</details>








<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LiteLLM.count_tokens" href="#tapeagents.llms.LiteLLM.count_tokens">count_tokens</a></code></b>
            –
            <div class="doc-md-description">
              <p>Count the number of tokens in a message or string.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.LiteLLM.make_training_text" href="#tapeagents.llms.LiteLLM.make_training_text">make_training_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generates the training text for the model.</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LiteLLM</span><span class="p">(</span><span class="n">CachedLLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A LiteLLM implementation of the LLM interface.</span>

<span class="sd">    This class provides integration with the LiteLLM library for making LLM API calls.</span>
<span class="sd">    It supports both streaming and non-streaming responses, token counting, and handles API timeouts with retries.</span>
<span class="sd">    Streaming responses are handled by yielding chunks of text as they arrive.</span>
<span class="sd">    Non-streaming responses return complete messages.</span>

<span class="sd">    Note:</span>
<span class="sd">        Function calling during streaming is not yet implemented and will raise NotImplementedError.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count the number of tokens in a message or string.</span>

<span class="sd">        Args:</span>
<span class="sd">            messages (Union[List[Dict], str]): List of messages or text to count tokens in.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The number of tokens in the messages or text.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">litellm</span><span class="o">.</span><span class="n">token_counter</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">litellm</span><span class="o">.</span><span class="n">token_counter</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">LLMEvent</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">litellm</span><span class="o">.</span><span class="n">completion</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                    <span class="n">messages</span><span class="o">=</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span>
                    <span class="n">tools</span><span class="o">=</span><span class="n">prompt</span><span class="o">.</span><span class="n">tools</span><span class="p">,</span>
                    <span class="n">stream</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stream</span><span class="p">,</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">break</span>
            <span class="k">except</span> <span class="n">openai</span><span class="o">.</span><span class="n">APITimeoutError</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;API Timeout, retrying in 1 sec&quot;</span><span class="p">)</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream</span><span class="p">:</span>
            <span class="n">buffer</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ModelResponse</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">litellm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">StreamingChoices</span><span class="p">):</span>
                    <span class="n">content_delta</span> <span class="o">=</span> <span class="n">part</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span>
                    <span class="k">if</span> <span class="n">content_delta</span><span class="p">:</span>
                        <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_delta</span><span class="p">)</span>
                        <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">chunk</span><span class="o">=</span><span class="n">content_delta</span><span class="p">)</span>
                    <span class="n">tool_delta</span> <span class="o">=</span> <span class="n">part</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">tool_calls</span>
                    <span class="k">if</span> <span class="n">tool_delta</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;TODO: streaming with function calls not implemented yet&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected response </span><span class="si">{</span><span class="n">part</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">buffer</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">litellm</span><span class="o">.</span><span class="n">ModelResponse</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">litellm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">Choices</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_output</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates the training text for the model.</span>

<span class="sd">        This method should be implemented by subclasses to provide the specific</span>
<span class="sd">        logic for creating the training text.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args (list): Variable length argument list.</span>
<span class="sd">            **kwargs (dict, optional): Arbitrary keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            TrainingText: The generated training text.</span>

<span class="sd">        Raises:</span>
<span class="sd">            NotImplementedError: If the method is not implemented by a subclass.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LiteLLM.count_tokens" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">count_tokens</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Count the number of tokens in a message or string.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>messages</code></b>
              (<code>Union[List[Dict], str]</code>)
          –
          <div class="doc-md-description">
            <p>List of messages or text to count tokens in.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>int</code></b> (              <code>int</code>
)          –
          <div class="doc-md-description">
            <p>The number of tokens in the messages or text.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Count the number of tokens in a message or string.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (Union[List[Dict], str]): List of messages or text to count tokens in.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The number of tokens in the messages or text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">litellm</span><span class="o">.</span><span class="n">token_counter</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">litellm</span><span class="o">.</span><span class="n">token_counter</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.LiteLLM.make_training_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">make_training_text</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Generates the training text for the model.</p>
<p>This method should be implemented by subclasses to provide the specific
logic for creating the training text.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>*args</code></b>
              (<code>list</code>, default:
                  <code>()</code>
)
          –
          <div class="doc-md-description">
            <p>Variable length argument list.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>**kwargs</code></b>
              (<code>dict</code>, default:
                  <code>{}</code>
)
          –
          <div class="doc-md-description">
            <p>Arbitrary keyword arguments.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>TrainingText</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.core.TrainingText" href="../core/#tapeagents.core.TrainingText">TrainingText</a></code>
)          –
          <div class="doc-md-description">
            <p>The generated training text.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>NotImplementedError</code>
            –
          <div class="doc-md-description">
            <p>If the method is not implemented by a subclass.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates the training text for the model.</span>

<span class="sd">    This method should be implemented by subclasses to provide the specific</span>
<span class="sd">    logic for creating the training text.</span>

<span class="sd">    Args:</span>
<span class="sd">        *args (list): Variable length argument list.</span>
<span class="sd">        **kwargs (dict, optional): Arbitrary keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        TrainingText: The generated training text.</span>

<span class="sd">    Raises:</span>
<span class="sd">        NotImplementedError: If the method is not implemented by a subclass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.MockLLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>MockLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM" href="#tapeagents.llms.LLM">LLM</a></code></p>


        <p>A mock LLM implementation for testing purposes.</p>
<p>This class simulates an LLM by returning predefined responses in a cyclic manner.
It tracks the prompts it receives and maintains a call counter.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.MockLLM.model_name">model_name</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>Name of the mock model, defaults to "mock"</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.MockLLM.call_number">call_number</span></code></b>
              (<code>int</code>)
          –
          <div class="doc-md-description">
            <p>Counter for number of calls made to generate, defaults to 0</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.MockLLM.mock_outputs">mock_outputs</span></code></b>
              (<code>list[str]</code>)
          –
          <div class="doc-md-description">
            <p>List of predefined responses to cycle through</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.MockLLM.prompts">prompts</span></code></b>
              (<code>list[<a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a>]</code>)
          –
          <div class="doc-md-description">
            <p>List of received prompts</p>
          </div>
        </li>
    </ul>










              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MockLLM</span><span class="p">(</span><span class="n">LLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A mock LLM implementation for testing purposes.</span>

<span class="sd">    This class simulates an LLM by returning predefined responses in a cyclic manner.</span>
<span class="sd">    It tracks the prompts it receives and maintains a call counter.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        model_name (str): Name of the mock model, defaults to &quot;mock&quot;</span>
<span class="sd">        call_number (int): Counter for number of calls made to generate, defaults to 0</span>
<span class="sd">        mock_outputs (list[str]): List of predefined responses to cycle through</span>
<span class="sd">        prompts (list[Prompt]): List of received prompts</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mock&quot;</span>
    <span class="n">call_number</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mock_outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Agent: I&#39;m good, thank you&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Agent: Sure, I worked at ServiceNow for 10 years&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Agent: I have 10 zillion parameters&quot;</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Prompt</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">_implementation</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mock_outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">call_number</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mock_outputs</span><span class="p">)]</span>
            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">output</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">call_number</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">LLMStream</span><span class="p">(</span><span class="n">_implementation</span><span class="p">(),</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">42</span>

    <span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">TrainingText</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;mock trace&quot;</span><span class="p">,</span> <span class="n">n_predicted</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.ReplayLLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>ReplayLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM" href="#tapeagents.llms.LLM">LLM</a></code></p>


        <p>Specialized LLM class that replays previously recorded LLM interactions.</p>
<p>Loads and replays model interactions from a SQLite database, allowing for
deterministic replay of previous LLM conversations without making new API calls.</p>
<p>The class is useful for:</p>
<ul>
<li>Testing and debugging LLM interactions</li>
<li>Reproducing specific model behaviors</li>
<li>Avoiding repeated API calls during development</li>
<li>Creating deterministic test scenarios</li>
</ul>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.ReplayLLM.outputs">outputs</span></code></b>
              (<code>dict[str, str]</code>)
          –
          <div class="doc-md-description">
            <p>Dictionary mapping prompt strings to their recorded outputs</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.ReplayLLM.llm_calls">llm_calls</span></code></b>
              (<code>list[<a class="autorefs autorefs-internal" title="tapeagents.observe.LLMCall" href="../core/#tapeagents.core.LLMCall">LLMCall</a>]</code>)
          –
          <div class="doc-md-description">
            <p>List of recorded LLM call objects</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.ReplayLLM.count_tokens_fn">count_tokens_fn</span></code></b>
              (<code><span title="typing.Callable">Callable</span></code>)
          –
          <div class="doc-md-description">
            <p>Function to count tokens in prompts/messages</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.ReplayLLM.make_training_text_fn">make_training_text_fn</span></code></b>
              (<code><span title="typing.Callable">Callable</span></code>)
          –
          <div class="doc-md-description">
            <p>Function to create training text from prompt/output pairs</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="tapeagents.utils.FatalError">FatalError</span></code>
            –
          <div class="doc-md-description">
            <p>When a prompt is not found in the recorded outputs</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
              <code>AssertionError</code>
            –
          <div class="doc-md-description">
            <p>When the specified SQLite database file doesn't exist</p>
          </div>
        </li>
    </ul>









<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM.count_tokens" href="#tapeagents.llms.ReplayLLM.count_tokens">count_tokens</a></code></b>
            –
            <div class="doc-md-description">
              <p>Counts the number of tokens in the given messages.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM.from_llm" href="#tapeagents.llms.ReplayLLM.from_llm">from_llm</a></code></b>
            –
            <div class="doc-md-description">
              <p>Create a ReplayLLM instance from an existing LLM and a SQLite database file.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM.generate" href="#tapeagents.llms.ReplayLLM.generate">generate</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generates an LLMStream based on the provided prompt.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM.make_training_text" href="#tapeagents.llms.ReplayLLM.make_training_text">make_training_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generates training text based on the provided prompt and output.</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ReplayLLM</span><span class="p">(</span><span class="n">LLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specialized LLM class that replays previously recorded LLM interactions.</span>

<span class="sd">    Loads and replays model interactions from a SQLite database, allowing for</span>
<span class="sd">    deterministic replay of previous LLM conversations without making new API calls.</span>

<span class="sd">    The class is useful for:</span>

<span class="sd">    - Testing and debugging LLM interactions</span>
<span class="sd">    - Reproducing specific model behaviors</span>
<span class="sd">    - Avoiding repeated API calls during development</span>
<span class="sd">    - Creating deterministic test scenarios</span>

<span class="sd">    Attributes:</span>
<span class="sd">        outputs (dict[str, str]): Dictionary mapping prompt strings to their recorded outputs</span>
<span class="sd">        llm_calls (list[LLMCall]): List of recorded LLM call objects</span>
<span class="sd">        count_tokens_fn (Callable): Function to count tokens in prompts/messages</span>
<span class="sd">        make_training_text_fn (Callable): Function to create training text from prompt/output pairs</span>

<span class="sd">    Raises:</span>
<span class="sd">        FatalError: When a prompt is not found in the recorded outputs</span>
<span class="sd">        AssertionError: When the specified SQLite database file doesn&#39;t exist</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">outputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
    <span class="n">llm_calls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">LLMCall</span><span class="p">]</span>
    <span class="n">count_tokens_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">0</span>
    <span class="n">make_training_text_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">TrainingText</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">n_predicted</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_llm</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">llm</span><span class="p">:</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">run_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompts_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DB_DEFAULT_FILENAME</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a ReplayLLM instance from an existing LLM and a SQLite database file.</span>

<span class="sd">        Args:</span>
<span class="sd">            cls (Type): The class to instantiate.</span>
<span class="sd">            llm (LLM): The original LLM instance.</span>
<span class="sd">            run_dir (str): The directory where the SQLite database file is located.</span>
<span class="sd">            prompts_file (str, optional): The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.</span>

<span class="sd">        Returns:</span>
<span class="sd">            (ReplayLLM): An instance of ReplayLLM initialized with the LLM calls from the SQLite database.</span>

<span class="sd">        Raises:</span>
<span class="sd">            AssertionError: If the SQLite database file does not exist at the specified path.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sqlite_fpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run_dir</span><span class="p">,</span> <span class="n">prompts_file</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sqlite_fpath</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Sqlite not found: </span><span class="si">{</span><span class="n">sqlite_fpath</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">llm_calls</span> <span class="o">=</span> <span class="n">retrieve_all_llm_calls</span><span class="p">(</span><span class="n">sqlite_fpath</span><span class="p">)</span>
        <span class="n">replay_llm</span> <span class="o">=</span> <span class="n">ReplayLLM</span><span class="p">(</span>
            <span class="n">llm_calls</span><span class="o">=</span><span class="n">llm_calls</span><span class="p">,</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">tokenizer_name</span> <span class="ow">or</span> <span class="n">llm</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">context_size</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">context_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">replay_llm</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span>
        <span class="n">replay_llm</span><span class="o">.</span><span class="n">count_tokens_fn</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">count_tokens</span>
        <span class="n">replay_llm</span><span class="o">.</span><span class="n">make_training_text_fn</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">make_training_text</span>
        <span class="k">return</span> <span class="n">replay_llm</span>

    <span class="k">def</span> <span class="nf">model_post_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">__context</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dups</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">llm_call</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm_calls</span><span class="p">:</span>
            <span class="n">prompt_key</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">llm_call</span><span class="o">.</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">llm_call</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
            <span class="k">if</span> <span class="n">prompt_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="ow">and</span> <span class="n">output</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output duplicate, using last value!</span><span class="se">\n</span><span class="s2">OLD:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">NEW:</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">dups</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">output</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span><span class="si">}</span><span class="s2"> outputs, </span><span class="si">{</span><span class="n">dups</span><span class="si">}</span><span class="s2"> duplicates&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">model_post_init</span><span class="p">(</span><span class="n">__context</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates an LLMStream based on the provided prompt.</span>

<span class="sd">        This method checks if the prompt has been previously processed and cached. If a cached output is found,</span>
<span class="sd">        it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no</span>
<span class="sd">        similar prompt is found, a FatalError is raised.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt object containing the messages to be processed.</span>
<span class="sd">            **kwargs (dict, optional): Additional keyword arguments.</span>

<span class="sd">        Returns:</span>
<span class="sd">            LLMStream: A stream of LLM events containing the generated output.</span>

<span class="sd">        Raises:</span>
<span class="sd">            FatalError: If the prompt is not found in the cache and no similar prompt is found.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">_implementation</span><span class="p">():</span>
            <span class="n">prompt_key</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">prompt_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s2">&quot;prompt cache hit&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">))</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;prompt of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">)</span><span class="si">}</span><span class="s2"> not found, checking similar ones..&quot;</span><span class="p">,</span> <span class="s2">&quot;yellow&quot;</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">known_prompts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                <span class="n">closest</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">closest_prompt</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">,</span> <span class="n">known_prompts</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;=</span> <span class="mf">0.7</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Closest prompt score </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zip_longest</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">closest</span><span class="p">),</span> <span class="n">fillvalue</span><span class="o">=</span><span class="p">{})):</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;STEP</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">diff_strings</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span><span class="w"> </span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">FatalError</span><span class="p">(</span><span class="s2">&quot;prompt not found&quot;</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">output</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">LLMStream</span><span class="p">(</span><span class="n">_implementation</span><span class="p">(),</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates training text based on the provided prompt and output.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The input prompt to generate training text from.</span>
<span class="sd">            output (LLMOutput): The output generated by the language model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            TrainingText: The generated training text.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_training_text_fn</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Counts the number of tokens in the given messages.</span>

<span class="sd">        Args:</span>
<span class="sd">            messages (Union[list[dict], str]): A list of message dictionaries or a single string message.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The total number of tokens in the messages.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_tokens_fn</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.ReplayLLM.count_tokens" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">count_tokens</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Counts the number of tokens in the given messages.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>messages</code></b>
              (<code>Union[list[dict], str]</code>)
          –
          <div class="doc-md-description">
            <p>A list of message dictionaries or a single string message.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>int</code></b> (              <code>int</code>
)          –
          <div class="doc-md-description">
            <p>The total number of tokens in the messages.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Counts the number of tokens in the given messages.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (Union[list[dict], str]): A list of message dictionaries or a single string message.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The total number of tokens in the messages.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_tokens_fn</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.ReplayLLM.from_llm" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">run_dir</span><span class="p">,</span> <span class="n">prompts_file</span><span class="o">=</span><span class="n">DB_DEFAULT_FILENAME</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

        <p>Create a ReplayLLM instance from an existing LLM and a SQLite database file.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>cls</code></b>
              (<code>Type</code>)
          –
          <div class="doc-md-description">
            <p>The class to instantiate.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>llm</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLM" href="#tapeagents.llms.LLM">LLM</a></code>)
          –
          <div class="doc-md-description">
            <p>The original LLM instance.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>run_dir</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The directory where the SQLite database file is located.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>prompts_file</code></b>
              (<code>str</code>, default:
                  <code><span title="tapeagents.config.DB_DEFAULT_FILENAME">DB_DEFAULT_FILENAME</span></code>
)
          –
          <div class="doc-md-description">
            <p>The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><a class="autorefs autorefs-internal" title="tapeagents.llms.ReplayLLM" href="#tapeagents.llms.ReplayLLM">ReplayLLM</a></code>
          –
          <div class="doc-md-description">
            <p>An instance of ReplayLLM initialized with the LLM calls from the SQLite database.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>AssertionError</code>
            –
          <div class="doc-md-description">
            <p>If the SQLite database file does not exist at the specified path.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">from_llm</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">llm</span><span class="p">:</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">run_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompts_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">DB_DEFAULT_FILENAME</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a ReplayLLM instance from an existing LLM and a SQLite database file.</span>

<span class="sd">    Args:</span>
<span class="sd">        cls (Type): The class to instantiate.</span>
<span class="sd">        llm (LLM): The original LLM instance.</span>
<span class="sd">        run_dir (str): The directory where the SQLite database file is located.</span>
<span class="sd">        prompts_file (str, optional): The name of the SQLite database file. Defaults to DB_DEFAULT_FILENAME.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (ReplayLLM): An instance of ReplayLLM initialized with the LLM calls from the SQLite database.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AssertionError: If the SQLite database file does not exist at the specified path.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sqlite_fpath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">run_dir</span><span class="p">,</span> <span class="n">prompts_file</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">sqlite_fpath</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Sqlite not found: </span><span class="si">{</span><span class="n">sqlite_fpath</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">llm_calls</span> <span class="o">=</span> <span class="n">retrieve_all_llm_calls</span><span class="p">(</span><span class="n">sqlite_fpath</span><span class="p">)</span>
    <span class="n">replay_llm</span> <span class="o">=</span> <span class="n">ReplayLLM</span><span class="p">(</span>
        <span class="n">llm_calls</span><span class="o">=</span><span class="n">llm_calls</span><span class="p">,</span>
        <span class="n">model_name</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">tokenizer_name</span> <span class="ow">or</span> <span class="n">llm</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">llm</span><span class="o">.</span><span class="n">context_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">replay_llm</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">tokenizer</span>
    <span class="n">replay_llm</span><span class="o">.</span><span class="n">count_tokens_fn</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">count_tokens</span>
    <span class="n">replay_llm</span><span class="o">.</span><span class="n">make_training_text_fn</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">make_training_text</span>
    <span class="k">return</span> <span class="n">replay_llm</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.ReplayLLM.generate" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Generates an LLMStream based on the provided prompt.</p>
<p>This method checks if the prompt has been previously processed and cached. If a cached output is found,
it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no
similar prompt is found, a FatalError is raised.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt object containing the messages to be processed.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>**kwargs</code></b>
              (<code>dict</code>, default:
                  <code>{}</code>
)
          –
          <div class="doc-md-description">
            <p>Additional keyword arguments.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>LLMStream</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.llms.LLMStream" href="#tapeagents.llms.LLMStream">LLMStream</a></code>
)          –
          <div class="doc-md-description">
            <p>A stream of LLM events containing the generated output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code><span title="tapeagents.utils.FatalError">FatalError</span></code>
            –
          <div class="doc-md-description">
            <p>If the prompt is not found in the cache and no similar prompt is found.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLMStream</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates an LLMStream based on the provided prompt.</span>

<span class="sd">    This method checks if the prompt has been previously processed and cached. If a cached output is found,</span>
<span class="sd">    it is returned. Otherwise, it attempts to find the closest known prompt and logs the differences. If no</span>
<span class="sd">    similar prompt is found, a FatalError is raised.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt object containing the messages to be processed.</span>
<span class="sd">        **kwargs (dict, optional): Additional keyword arguments.</span>

<span class="sd">    Returns:</span>
<span class="sd">        LLMStream: A stream of LLM events containing the generated output.</span>

<span class="sd">    Raises:</span>
<span class="sd">        FatalError: If the prompt is not found in the cache and no similar prompt is found.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_implementation</span><span class="p">():</span>
        <span class="n">prompt_key</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prompt_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s2">&quot;prompt cache hit&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">))</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="n">prompt_key</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;prompt of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">)</span><span class="si">}</span><span class="s2"> not found, checking similar ones..&quot;</span><span class="p">,</span> <span class="s2">&quot;yellow&quot;</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">known_prompts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">closest</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">closest_prompt</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">,</span> <span class="n">known_prompts</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;=</span> <span class="mf">0.7</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Closest prompt score </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zip_longest</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">closest</span><span class="p">),</span> <span class="n">fillvalue</span><span class="o">=</span><span class="p">{})):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;STEP</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">diff_strings</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">)),</span><span class="w"> </span><span class="n">b</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">)))</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">FatalError</span><span class="p">(</span><span class="s2">&quot;prompt not found&quot;</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">output</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">LLMStream</span><span class="p">(</span><span class="n">_implementation</span><span class="p">(),</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.ReplayLLM.make_training_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Generates training text based on the provided prompt and output.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The input prompt to generate training text from.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The output generated by the language model.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>TrainingText</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.core.TrainingText" href="../core/#tapeagents.core.TrainingText">TrainingText</a></code>
)          –
          <div class="doc-md-description">
            <p>The generated training text.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates training text based on the provided prompt and output.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The input prompt to generate training text from.</span>
<span class="sd">        output (LLMOutput): The output generated by the language model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        TrainingText: The generated training text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_training_text_fn</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="tapeagents.llms.TrainableLLM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <code>TrainableLLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="tapeagents.llms.CachedLLM" href="#tapeagents.llms.CachedLLM">CachedLLM</a></code></p>


        <p>Class for interacting with trainable language models through OpenAI-compatible API endpoints.</p>
<p>This class implements functionality for both inference and training-related operations with
language models served via Text Generation Inference (TGI) or vLLM endpoints that expose
an OpenAI-compatible API interface. It supports both streaming and non-streaming modes,
and includes methods for token counting and log probability calculations.</p>


<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.TrainableLLM.base_url">base_url</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>Base URL of the API endpoint</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><span title="tapeagents.llms.TrainableLLM.api_token">api_token</span></code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>Authentication token for API access</p>
          </div>
        </li>
    </ul>









<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.count_tokens" href="#tapeagents.llms.TrainableLLM.count_tokens">count_tokens</a></code></b>
            –
            <div class="doc-md-description">
              <p>Count the number of tokens in the given messages.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.get_log_probs" href="#tapeagents.llms.TrainableLLM.get_log_probs">get_log_probs</a></code></b>
            –
            <div class="doc-md-description">
              <p>Calculate the log probabilities of the given output based on the provided prompt.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.get_log_probs_chat_complete" href="#tapeagents.llms.TrainableLLM.get_log_probs_chat_complete">get_log_probs_chat_complete</a></code></b>
            –
            <div class="doc-md-description">
              <p>Calculate the log probabilities of the tokens in the completion generated by the language model.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.get_log_probs_complete" href="#tapeagents.llms.TrainableLLM.get_log_probs_complete">get_log_probs_complete</a></code></b>
            –
            <div class="doc-md-description">
              <p>Get the log probabilities of the tokens in the output given the prompt.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.load_tokenizer" href="#tapeagents.llms.TrainableLLM.load_tokenizer">load_tokenizer</a></code></b>
            –
            <div class="doc-md-description">
              <p>Loads the tokenizer for the model.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="tapeagents.llms.TrainableLLM.make_training_text" href="#tapeagents.llms.TrainableLLM.make_training_text">make_training_text</a></code></b>
            –
            <div class="doc-md-description">
              <p>Generates training text from a given prompt and LLM output.</p>
            </div>
          </li>
    </ul>



              <details class="quote">
                <summary>Source code in <code>tapeagents/llms.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">TrainableLLM</span><span class="p">(</span><span class="n">CachedLLM</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for interacting with trainable language models through OpenAI-compatible API endpoints.</span>

<span class="sd">    This class implements functionality for both inference and training-related operations with</span>
<span class="sd">    language models served via Text Generation Inference (TGI) or vLLM endpoints that expose</span>
<span class="sd">    an OpenAI-compatible API interface. It supports both streaming and non-streaming modes,</span>
<span class="sd">    and includes methods for token counting and log probability calculations.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        base_url (str): Base URL of the API endpoint</span>
<span class="sd">        api_token (str): Authentication token for API access</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># TODO: use OpenAI Python client when the certificate issue is resolved.</span>
    <span class="c1"># TODO: consider using litellm</span>

    <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">api_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">exclude</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">model_post_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">__context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">model_post_init</span><span class="p">(</span><span class="n">__context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="n">TAPEAGENTS_LLM_TOKEN</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="nd">@retry</span><span class="p">(</span><span class="n">stop</span><span class="o">=</span><span class="n">stop_after_attempt</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">wait</span><span class="o">=</span><span class="n">wait_exponential</span><span class="p">(</span><span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">_generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Generator</span><span class="p">[</span><span class="n">LLMEvent</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
        <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="p">:</span>
            <span class="n">headers</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span>
            <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="n">url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/v1/chat/completions&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="n">data</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
            <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stream</span><span class="p">,</span>
            <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">r</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to get completion: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stream</span><span class="p">:</span>
            <span class="n">response_buffer</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">byte_payload</span> <span class="ow">in</span> <span class="n">r</span><span class="o">.</span><span class="n">iter_lines</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">byte_payload</span> <span class="o">==</span> <span class="sa">b</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">payload</span> <span class="o">=</span> <span class="n">byte_payload</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">payload</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;data:&quot;</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">payload</span> <span class="o">==</span> <span class="s2">&quot;data: [DONE]&quot;</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">json_payload</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">payload</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;data:&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span>
                    <span class="n">response_delta</span> <span class="o">=</span> <span class="n">json_payload</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;delta&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">response_delta</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">response_buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response_delta</span><span class="p">)</span>
                    <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">chunk</span><span class="o">=</span><span class="n">response_delta</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">response_buffer</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;message&quot;</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">content</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empty completion </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">LLMOutput</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to parse llm response: </span><span class="si">{</span><span class="n">r</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_output</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">LLMEvent</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the tokenizer for the model.</span>

<span class="sd">        If the tokenizer is not already loaded, this method will import the</span>
<span class="sd">        `transformers` library and load the tokenizer using the model name or</span>
<span class="sd">        tokenizer name. If `_MOCK_TOKENIZER` is set, it will use that instead.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If neither `self.tokenizer_name` nor `self.model_name`</span>
<span class="sd">                        is provided and `_MOCK_TOKENIZER` is not set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">transformers</span>

            <span class="n">name</span> <span class="o">=</span> <span class="n">_MOCK_TOKENIZER</span> <span class="k">if</span> <span class="n">_MOCK_TOKENIZER</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generates training text from a given prompt and LLM output.</span>

<span class="sd">        This method loads the tokenizer and uses it to create training text</span>
<span class="sd">        suitable for training a language model.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The input prompt to generate training text from.</span>
<span class="sd">            output (LLMOutput): The output from the language model to be used in training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            TrainingText: The generated training text.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">trainable_llm_make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_log_probs_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the log probabilities of the tokens in the output given the prompt.</span>

<span class="sd">        This method sends a request to the language model API to generate the log probabilities</span>
<span class="sd">        for the tokens in the provided output, given the prompt. It uses the tokenizer to encode</span>
<span class="sd">        the prompt and output, and extracts the log probabilities from the API response.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (str): The input prompt text.</span>
<span class="sd">            output (str): The output text for which log probabilities are to be calculated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[float]: A list of log probabilities for each token in the output.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If the API response is not as expected or if there is a mismatch</span>
<span class="sd">                          between the tokens in the response and the provided output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>

        <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="p">:</span>
            <span class="n">headers</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span> <span class="ow">and</span> <span class="n">prompt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">):</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>

        <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span>
        <span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt_text</span><span class="p">,</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;echo&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;include_stop_str_in_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># self.include_stop_str_in_output,</span>
            <span class="s2">&quot;skip_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># number of completions to generate</span>
            <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># return a single completion and not a stream of lines</span>
        <span class="p">}</span>
        <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/v1/completions&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;POST request to </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">generation_args</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>  <span class="c1"># raise exception if status code is not in the 200s</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">][</span><span class="s2">&quot;token_logprobs&quot;</span><span class="p">]</span>
            <span class="n">prompt_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">prompt_completion_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">][</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
            <span class="k">assert</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="n">output</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Tokens do not match completion: </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation API wrong response: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_probs</span>

    <span class="k">def</span> <span class="nf">get_log_probs_chat_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the log probabilities of the tokens in the completion generated by the language model.</span>

<span class="sd">        This function sends a request to the language model API to generate completions and calculate log probabilities.</span>
<span class="sd">        The function uses the tokenizer to encode the prompt and completion texts.</span>
<span class="sd">        The log probabilities are extracted from the API response and validated against the original completion.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Prompt): The prompt containing the messages to be sent to the language model.</span>
<span class="sd">            output (LLMOutput): The output from the language model containing the generated completion.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[float]: A list of log probabilities for each token in the generated completion.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If the response from the generation API is incorrect or cannot be parsed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="p">:</span>
            <span class="n">headers</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

        <span class="n">time_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">prompt_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">messages</span> <span class="o">+</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()]</span>
        <span class="n">prompt_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">prompt_completion_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span> <span class="ow">and</span> <span class="n">prompt_text</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">):</span>
            <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>
            <span class="n">prompt_completion_text</span> <span class="o">=</span> <span class="n">prompt_completion_text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>

        <span class="n">prompt_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">prompt_completion_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt_completion_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;echo&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s2">&quot;include_stop_str_in_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># self.include_stop_str_in_output,</span>
            <span class="s2">&quot;skip_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># number of completions to generate</span>
            <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># return a single completion and not a stream of lines</span>
        <span class="p">}</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="n">url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/v1/chat/completions&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="n">generation_args</span><span class="p">,</span>
            <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
            <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">decoded_tokens</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">log_prob</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;prompt_logprobs&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">log_prob</span><span class="p">:</span>
                    <span class="n">token_key</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">log_prob</span><span class="p">))</span>
                    <span class="n">token_info</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">[</span><span class="n">token_key</span><span class="p">]</span>
                    <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_info</span><span class="p">[</span><span class="s2">&quot;logprob&quot;</span><span class="p">])</span>
                    <span class="n">decoded_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_info</span><span class="p">[</span><span class="s2">&quot;decoded_token&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
                    <span class="n">decoded_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
            <span class="n">decoded_tokens</span> <span class="o">=</span> <span class="n">decoded_tokens</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
            <span class="n">reconstructed_completion</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">decoded_tokens</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span> <span class="ow">in</span> <span class="n">reconstructed_completion</span><span class="p">:</span>
                <span class="n">reconstructed_completion</span> <span class="o">=</span> <span class="n">reconstructed_completion</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span><span class="p">)]</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">reconstructed_completion</span> <span class="o">==</span> <span class="n">completion</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Tokens do not match completion: </span><span class="si">{</span><span class="n">reconstructed_completion</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">completion</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation API wrong response: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log likelihood calculation took </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">time_t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens per second: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">time_t0</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_probs</span>

    <span class="k">def</span> <span class="nf">get_log_probs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the log probabilities of the given output based on the provided prompt.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt (Union[str, Prompt]): The input prompt, which can be either a string or a Prompt object.</span>
<span class="sd">            output (Union[str, LLMOutput]): The output to evaluate, which can be either a string or an LLMOutput object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[float]: A list of log probabilities corresponding to the given output.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the input types are not valid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_log_probs_complete</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">Prompt</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">LLMOutput</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_log_probs_chat_complete</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid input types&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count the number of tokens in the given messages.</span>

<span class="sd">        This method loads the tokenizer and then counts the number of tokens</span>
<span class="sd">        in the provided messages. The messages can be either a string or a list</span>
<span class="sd">        of dictionaries.</span>

<span class="sd">        Args:</span>
<span class="sd">            messages (Union[list[dict], str]): The messages to count tokens for. It can</span>
<span class="sd">                               be a single string or a list of dictionaries.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: The number of tokens in the provided messages.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.count_tokens" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">count_tokens</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Count the number of tokens in the given messages.</p>
<p>This method loads the tokenizer and then counts the number of tokens
in the provided messages. The messages can be either a string or a list
of dictionaries.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>messages</code></b>
              (<code>Union[list[dict], str]</code>)
          –
          <div class="doc-md-description">
            <p>The messages to count tokens for. It can
               be a single string or a list of dictionaries.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>int</code></b> (              <code>int</code>
)          –
          <div class="doc-md-description">
            <p>The number of tokens in the provided messages.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Count the number of tokens in the given messages.</span>

<span class="sd">    This method loads the tokenizer and then counts the number of tokens</span>
<span class="sd">    in the provided messages. The messages can be either a string or a list</span>
<span class="sd">    of dictionaries.</span>

<span class="sd">    Args:</span>
<span class="sd">        messages (Union[list[dict], str]): The messages to count tokens for. It can</span>
<span class="sd">                           be a single string or a list of dictionaries.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: The number of tokens in the provided messages.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.get_log_probs" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">get_log_probs</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Calculate the log probabilities of the given output based on the provided prompt.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code>Union[str, <a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a>]</code>)
          –
          <div class="doc-md-description">
            <p>The input prompt, which can be either a string or a Prompt object.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code>Union[str, <a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a>]</code>)
          –
          <div class="doc-md-description">
            <p>The output to evaluate, which can be either a string or an LLMOutput object.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>list[float]</code>
          –
          <div class="doc-md-description">
            <p>list[float]: A list of log probabilities corresponding to the given output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>ValueError</code>
            –
          <div class="doc-md-description">
            <p>If the input types are not valid.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_log_probs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the log probabilities of the given output based on the provided prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Union[str, Prompt]): The input prompt, which can be either a string or a Prompt object.</span>
<span class="sd">        output (Union[str, LLMOutput]): The output to evaluate, which can be either a string or an LLMOutput object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list[float]: A list of log probabilities corresponding to the given output.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the input types are not valid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_log_probs_complete</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">Prompt</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">LLMOutput</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_log_probs_chat_complete</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid input types&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.get_log_probs_chat_complete" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">get_log_probs_chat_complete</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Calculate the log probabilities of the tokens in the completion generated by the language model.</p>
<p>This function sends a request to the language model API to generate completions and calculate log probabilities.
The function uses the tokenizer to encode the prompt and completion texts.
The log probabilities are extracted from the API response and validated against the original completion.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The prompt containing the messages to be sent to the language model.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The output from the language model containing the generated completion.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>list[float]</code>
          –
          <div class="doc-md-description">
            <p>list[float]: A list of log probabilities for each token in the generated completion.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>RuntimeError</code>
            –
          <div class="doc-md-description">
            <p>If the response from the generation API is incorrect or cannot be parsed.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_log_probs_chat_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the log probabilities of the tokens in the completion generated by the language model.</span>

<span class="sd">    This function sends a request to the language model API to generate completions and calculate log probabilities.</span>
<span class="sd">    The function uses the tokenizer to encode the prompt and completion texts.</span>
<span class="sd">    The log probabilities are extracted from the API response and validated against the original completion.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The prompt containing the messages to be sent to the language model.</span>
<span class="sd">        output (LLMOutput): The output from the language model containing the generated completion.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list[float]: A list of log probabilities for each token in the generated completion.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If the response from the generation API is incorrect or cannot be parsed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="p">:</span>
        <span class="n">headers</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

    <span class="n">time_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">prompt_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">messages</span> <span class="o">+</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()]</span>
    <span class="n">prompt_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">prompt_completion_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span> <span class="ow">and</span> <span class="n">prompt_text</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">):</span>
        <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt_text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>
        <span class="n">prompt_completion_text</span> <span class="o">=</span> <span class="n">prompt_completion_text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>

    <span class="n">prompt_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">prompt_completion_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt_completion_text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;echo&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;include_stop_str_in_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># self.include_stop_str_in_output,</span>
        <span class="s2">&quot;skip_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># number of completions to generate</span>
        <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># return a single completion and not a stream of lines</span>
    <span class="p">}</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
        <span class="n">url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/v1/chat/completions&quot;</span><span class="p">,</span>
        <span class="n">json</span><span class="o">=</span><span class="n">generation_args</span><span class="p">,</span>
        <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
        <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">decoded_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">log_prob</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;prompt_logprobs&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">log_prob</span><span class="p">:</span>
                <span class="n">token_key</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">log_prob</span><span class="p">))</span>
                <span class="n">token_info</span> <span class="o">=</span> <span class="n">log_prob</span><span class="p">[</span><span class="n">token_key</span><span class="p">]</span>
                <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_info</span><span class="p">[</span><span class="s2">&quot;logprob&quot;</span><span class="p">])</span>
                <span class="n">decoded_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token_info</span><span class="p">[</span><span class="s2">&quot;decoded_token&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
                <span class="n">decoded_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
        <span class="n">decoded_tokens</span> <span class="o">=</span> <span class="n">decoded_tokens</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
        <span class="n">reconstructed_completion</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">decoded_tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span> <span class="ow">in</span> <span class="n">reconstructed_completion</span><span class="p">:</span>
            <span class="n">reconstructed_completion</span> <span class="o">=</span> <span class="n">reconstructed_completion</span><span class="p">[:</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span><span class="p">)]</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">reconstructed_completion</span> <span class="o">==</span> <span class="n">completion</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Tokens do not match completion: </span><span class="si">{</span><span class="n">reconstructed_completion</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">completion</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation API wrong response: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log likelihood calculation took </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">time_t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens per second: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">time_t0</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">log_probs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.get_log_probs_complete" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">get_log_probs_complete</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Get the log probabilities of the tokens in the output given the prompt.</p>
<p>This method sends a request to the language model API to generate the log probabilities
for the tokens in the provided output, given the prompt. It uses the tokenizer to encode
the prompt and output, and extracts the log probabilities from the API response.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The input prompt text.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The output text for which log probabilities are to be calculated.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>list[float]</code>
          –
          <div class="doc-md-description">
            <p>list[float]: A list of log probabilities for each token in the output.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>RuntimeError</code>
            –
          <div class="doc-md-description">
            <p>If the API response is not as expected or if there is a mismatch
          between the tokens in the response and the provided output.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_log_probs_complete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the log probabilities of the tokens in the output given the prompt.</span>

<span class="sd">    This method sends a request to the language model API to generate the log probabilities</span>
<span class="sd">    for the tokens in the provided output, given the prompt. It uses the tokenizer to encode</span>
<span class="sd">    the prompt and output, and extracts the log probabilities from the API response.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (str): The input prompt text.</span>
<span class="sd">        output (str): The output text for which log probabilities are to be calculated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list[float]: A list of log probabilities for each token in the output.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If the API response is not as expected or if there is a mismatch</span>
<span class="sd">                      between the tokens in the response and the provided output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>

    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="p">:</span>
        <span class="n">headers</span> <span class="o">|=</span> <span class="p">{</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">api_token</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span> <span class="ow">and</span> <span class="n">prompt</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">):</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>

    <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span>
    <span class="n">generation_args</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt_text</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;logprobs&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;echo&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;include_stop_str_in_output&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># self.include_stop_str_in_output,</span>
        <span class="s2">&quot;skip_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;n&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># number of completions to generate</span>
        <span class="s2">&quot;stream&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># return a single completion and not a stream of lines</span>
    <span class="p">}</span>
    <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/v1/completions&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;POST request to </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">generation_args</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">verify</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">r</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>  <span class="c1"># raise exception if status code is not in the 200s</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">][</span><span class="s2">&quot;token_logprobs&quot;</span><span class="p">]</span>
        <span class="n">prompt_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">prompt_completion_encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span> <span class="o">+</span> <span class="n">output</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">][</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_encoded</span><span class="p">)</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_completion_encoded</span><span class="p">)]</span>
        <span class="k">assert</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="n">output</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Tokens do not match completion: </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation API wrong response: </span><span class="si">{</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_probs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.load_tokenizer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">load_tokenizer</span><span class="p">()</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Loads the tokenizer for the model.</p>
<p>If the tokenizer is not already loaded, this method will import the
<code>transformers</code> library and load the tokenizer using the model name or
tokenizer name. If <code>_MOCK_TOKENIZER</code> is set, it will use that instead.</p>


<p><span class="doc-section-title">Raises:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>ValueError</code>
            –
          <div class="doc-md-description">
            <p>If neither <code>self.tokenizer_name</code> nor <code>self.model_name</code>
        is provided and <code>_MOCK_TOKENIZER</code> is not set.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads the tokenizer for the model.</span>

<span class="sd">    If the tokenizer is not already loaded, this method will import the</span>
<span class="sd">    `transformers` library and load the tokenizer using the model name or</span>
<span class="sd">    tokenizer name. If `_MOCK_TOKENIZER` is set, it will use that instead.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If neither `self.tokenizer_name` nor `self.model_name`</span>
<span class="sd">                    is provided and `_MOCK_TOKENIZER` is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">transformers</span>

        <span class="n">name</span> <span class="o">=</span> <span class="n">_MOCK_TOKENIZER</span> <span class="k">if</span> <span class="n">_MOCK_TOKENIZER</span> <span class="k">else</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_name</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="tapeagents.llms.TrainableLLM.make_training_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <code class="highlight language-python"><span class="n">make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Generates training text from a given prompt and LLM output.</p>
<p>This method loads the tokenizer and uses it to create training text
suitable for training a language model.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The input prompt to generate training text from.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The output from the language model to be used in training.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>TrainingText</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.core.TrainingText" href="../core/#tapeagents.core.TrainingText">TrainingText</a></code>
)          –
          <div class="doc-md-description">
            <p>The generated training text.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_training_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates training text from a given prompt and LLM output.</span>

<span class="sd">    This method loads the tokenizer and uses it to create training text</span>
<span class="sd">    suitable for training a language model.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The input prompt to generate training text from.</span>
<span class="sd">        output (LLMOutput): The output from the language model to be used in training.</span>

<span class="sd">    Returns:</span>
<span class="sd">        TrainingText: The generated training text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">load_tokenizer</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">trainable_llm_make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h2 id="tapeagents.llms.closest_prompt" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>            <code class="highlight language-python"><span class="n">closest_prompt</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">,</span> <span class="n">known_prompts</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt_key</code></b>
              (<code>str</code>)
          –
          <div class="doc-md-description">
            <p>The prompt to compare against the known prompts.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>known_prompts</code></b>
              (<code>list[str]</code>)
          –
          <div class="doc-md-description">
            <p>A list of known prompts to compare with the prompt_key.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>tuple[str, float]</code>
          –
          <div class="doc-md-description">
            <p>tuple[str, float]: A tuple containing the closest matching prompt and its similarity score.
               If no prompts are found, returns an empty string and a score of 0.0.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">closest_prompt</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">known_prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the closest matching prompt from a list of known prompts based on a Levenshtein similarity ratio.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_key (str): The prompt to compare against the known prompts.</span>
<span class="sd">        known_prompts (list[str]): A list of known prompts to compare with the prompt_key.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[str, float]: A tuple containing the closest matching prompt and its similarity score.</span>
<span class="sd">                           If no prompts are found, returns an empty string and a score of 0.0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ratios</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">ratio</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">score_cutoff</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">known_prompts</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="n">ratios</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mf">0.0</span>
    <span class="n">ratios</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">closest</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">ratios</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">closest</span><span class="p">,</span> <span class="n">score</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="tapeagents.llms.trainable_llm_make_training_text" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>            <code class="highlight language-python"><span class="n">trainable_llm_make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Generates training text for LLM fine-tuning by combining prompt and output using tokenizer's chat template.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
            <b><code>prompt</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.Prompt" href="../core/#tapeagents.core.Prompt">Prompt</a></code>)
          –
          <div class="doc-md-description">
            <p>The input prompt containing conversation messages.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>output</code></b>
              (<code><a class="autorefs autorefs-internal" title="tapeagents.core.LLMOutput" href="../core/#tapeagents.core.LLMOutput">LLMOutput</a></code>)
          –
          <div class="doc-md-description">
            <p>The model's output/response.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
            <b><code>tokenizer</code></b>
              (<code>PreTrainedTokenizer</code>)
          –
          <div class="doc-md-description">
            <p>The tokenizer used to format the conversation.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>TrainingText</code></b> (              <code><a class="autorefs autorefs-internal" title="tapeagents.core.TrainingText" href="../core/#tapeagents.core.TrainingText">TrainingText</a></code>
)          –
          <div class="doc-md-description">
            <p>A dataclass containing:</p>
<ul>
<li>text (str): The formatted conversation text</li>
<li>n_predicted (int): Length of the output text portion</li>
</ul>
          </div>
        </li>
    </ul>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>Uses tokenizer's chat template to format conversations</li>
<li>Removes BOS token if present in the beginning of the text</li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>tapeagents/llms.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span>
<span class="normal">952</span>
<span class="normal">953</span>
<span class="normal">954</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">trainable_llm_make_training_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="n">Prompt</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">LLMOutput</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainingText</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates training text for LLM fine-tuning by combining prompt and output using tokenizer&#39;s chat template.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt (Prompt): The input prompt containing conversation messages.</span>
<span class="sd">        output (LLMOutput): The model&#39;s output/response.</span>
<span class="sd">        tokenizer (PreTrainedTokenizer): The tokenizer used to format the conversation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        TrainingText: A dataclass containing:</span>

<span class="sd">            - text (str): The formatted conversation text</span>
<span class="sd">            - n_predicted (int): Length of the output text portion</span>

<span class="sd">    Note:</span>
<span class="sd">        - Uses tokenizer&#39;s chat template to format conversations</span>
<span class="sd">        - Removes BOS token if present in the beginning of the text</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompt_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">conversation</span><span class="o">=</span><span class="n">prompt</span><span class="o">.</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">.</span><span class="n">messages</span> <span class="o">+</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">output</span><span class="o">.</span><span class="n">content</span><span class="p">}],</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">output_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span> <span class="p">:]</span>

    <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span> <span class="ow">and</span> <span class="n">text</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">TrainingText</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">n_predicted</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">output_text</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>