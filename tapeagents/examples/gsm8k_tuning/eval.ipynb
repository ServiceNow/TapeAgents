{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tapeagents.llms import LLAMA\n",
    "from tapeagents.tapeagents.examples.gsm8k_tuning.math_agent import (\n",
    "    MathAgent,\n",
    "    MathEnvironment,\n",
    "    extract_result_value,\n",
    "    solve_task,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MathEnvironment()\n",
    "\n",
    "\n",
    "def eval(tested_agent, test_set) -> float:\n",
    "    test_solved = []\n",
    "    for i, sample in enumerate(tqdm(test_set)):\n",
    "        sample = extract_result_value(sample)\n",
    "        try:\n",
    "            tape = solve_task(tested_agent, env, sample)\n",
    "            test_solved.append(int(tape.metadata.result[\"solved\"]))\n",
    "        except Exception as e:\n",
    "            print(colored(\"Failed to solve task: {e}\", \"red\"))\n",
    "            test_solved.append(0)\n",
    "            raise e\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            print(f\"{i}: Current accuracy: {np.mean(test_solved):.3f}\")\n",
    "    acc = np.mean(test_solved).item()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "test_samples = [s for s in test_dataset]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(test_samples)  # type: ignore\n",
    "test_set = test_samples[:200]\n",
    "\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "val_samples = [s for s in dataset]\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(val_samples)  # type: ignore\n",
    "val_set = val_samples[:200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference: vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "untuned_agent = MathAgent(\n",
    "    llms={\n",
    "        \"default\": LLAMA(\n",
    "            base_url=\"http://localhost:8000\",\n",
    "            model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            tokenizer_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            parameters=dict(temperature=0.1),\n",
    "            use_cache=False,\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untuned model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = eval(untuned_agent, val_set)\n",
    "print(f\"Untuned on train {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = eval(untuned_agent, test_set)\n",
    "print(f\"Untuned on test {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"untuned\": {\"train\": val_acc, \"test\": acc}}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference: vllm serve gsm8k/tune1/intermediate/1000/\n",
    "tuned_agent = MathAgent(\n",
    "    llms={\n",
    "        \"default\": LLAMA(\n",
    "            base_url=\"http://localhost:8000\",\n",
    "            model_name=\"gsm8k/tune1/intermediate/1000/\",\n",
    "            tokenizer_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            parameters=dict(temperature=0.0),\n",
    "            use_cache=False,\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_val_acc = eval(tuned_agent, test_set)\n",
    "print(f\"Tuned on test {tuned_val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_acc = eval(tuned_agent, val_set)\n",
    "print(f\"Tuned on train {tuned_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\") as f:\n",
    "    f.write(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"untuned\": {\"train\": val_acc, \"test\": acc},\n",
    "                \"tuned\": {\"train\": tuned_acc, \"test\": tuned_val_acc},\n",
    "            }\n",
    "        )\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tapeagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
